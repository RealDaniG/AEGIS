{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03285v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks", "summary": "arXiv:2510.03285v1 Announce Type: new Abstract: Recent advances in browser-based LLM agents have shown promise for automating tasks ranging from simple form filling to hotel booking or online shopping. Current benchmarks measure agent performance in controlled environments, such as containers or stable networks, where websites behave deterministically. However, in the real world, users access websites over networks and HTTPS connections that introduce instability from multiple sources: client-side, server-side issues or broader system failures. Moreover, live websites are prone to web attacks such Cross-Site Scripting, as well as general site modifications which can cause unexpected or malicious pop-ups or improper functionality. To address this gap, we present WAREX: Web Agent Reliability Evaluation on Existing Benchmarks. We measure the impact of WAREX across three popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that introducing WAREX leads to significant drops in task success rates, highlighting the limited robustness of state-of-the-art agents.", "link": "https://arxiv.org/abs/2510.03285", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.651110Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03377v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints", "summary": "arXiv:2510.03377v1 Announce Type: new Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its supply, increasing prices, and the impact of climate change, force the global economy to develop more energy-efficient solutions for their operations. The Manufacturing sector is not excluded from this challenge as one of the largest consumers of energy. Energy-efficient scheduling is a method that attracts manufacturing companies to reduce their consumption as it can be quickly deployed and can show impact immediately. In this study, the hybrid flow shop scheduling problem with blocking constraint (BHFS) is investigated in which we seek to minimize the latest completion time (i.e. makespan) and overall energy consumption, a typical manufacturing setting across many industries from automotive to pharmaceutical. Energy consumption and the latest completion time of customer orders are usually conflicting objectives. Therefore, we first formulate the problem as a novel multi-objective mixed integer programming (MIP) model and propose an augmented epsilon-constraint method for finding the Pareto-optimal solutions. Also, an effective multi-objective metaheuristic algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large instances in reasonable time. Our proposed methods are benchmarked using small, medium, and large-size instances to evaluate their efficiency. Two well-known algorithms are adopted for comparing our novel approaches. The computational results show the effectiveness of our method.", "link": "https://arxiv.org/abs/2510.03377", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.651264Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03399v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition", "summary": "arXiv:2510.03399v1 Announce Type: new Abstract: Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.", "link": "https://arxiv.org/abs/2510.03399", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.651355Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03418v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection", "summary": "arXiv:2510.03418v1 Announce Type: new Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources, offering advanced capabilities for information access and decision-making. However, contradictions in retrieved evidence can result in inconsistent or untrustworthy outputs, which is especially problematic in enterprise settings where compliance, governance, and accountability are critical. Existing benchmarks for contradiction detection are limited to sentence-level analysis and do not capture the complexity of enterprise documents such as contracts, financial filings, compliance reports, or policy manuals. To address this limitation, we propose ContraGen, a contradiction-aware benchmark framework tailored to enterprise domain. The framework generates synthetic enterprise-style documents with embedded contradictions, enabling systematic evaluation of both intra-document and cross-document consistency. Automated contradiction mining is combined with human-in-the-loop validation to ensure high accuracy. Our contributions include generating realistic enterprise documents, modeling a taxonomy of contradiction types common in business processes, enabling controlled creation of self- and pairwise contradictions, developing a contradiction-aware retrieval evaluation pipeline and embedding human oversight to reflect domain-specific judgment complexity. This work establishes a foundation for more trustworthy and accountable RAG systems in enterprise information-seeking applications, where detecting and resolving contradictions is essential for reducing risk and ensuring compliance.", "link": "https://arxiv.org/abs/2510.03418", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.651442Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03453v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "A Qualitative Comparative Evaluation of Cognitive and Generative Theories", "summary": "arXiv:2510.03453v1 Announce Type: new Abstract: Evaluation is a critical activity associated with any theory. Yet this has proven to be an exceptionally challenging activity for theories based on cognitive architectures. For an overlapping set of reasons, evaluation can also be challenging for theories based on generative neural architectures. This dual challenge is approached here by leveraging a broad perspective on theory evaluation to yield a wide-ranging, albeit qualitative, comparison of whole-mind-oriented cognitive and generative architectures and the full systems that are based on these architectures.", "link": "https://arxiv.org/abs/2510.03453", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.651490Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03469v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification", "summary": "arXiv:2510.03469v1 Announce Type: new Abstract: We introduce a novel framework for evaluating the alignment between natural language plans and their expected behavior by converting them into Kripke structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs) and performing model checking. We systematically evaluate this framework on a simplified version of the PlanBench plan verification dataset and report on metrics like Accuracy, Precision, Recall and F1 scores. Our experiments demonstrate that GPT-5 achieves excellent classification performance (F1 score of 96.3%) while almost always producing syntactically perfect formal representations that can act as guarantees. However, the synthesis of semantically perfect formal models remains an area for future exploration.", "link": "https://arxiv.org/abs/2510.03469", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.651648Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03485v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection", "summary": "arXiv:2510.03485v1 Announce Type: new Abstract: Autonomous web agents need to operate under externally imposed or human-specified policies while generating long-horizon trajectories. However, little work has examined whether these trajectories comply with such policies, or whether policy violations persist across different contexts such as domains (e.g., shopping or coding websites) and subdomains (e.g., product search and order management in shopping). To address this gap, we introduce PolicyGuardBench, a benchmark of about 60k examples for detecting policy violations in agent trajectories. From diverse agent runs, we generate a broad set of policies and create both within subdomain and cross subdomain pairings with violation labels. In addition to full-trajectory evaluation, PolicyGuardBench also includes a prefix-based violation detection task where models must anticipate policy violations from truncated trajectory prefixes rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a lightweight guardrail model that delivers strong detection accuracy across all tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes across domains and preserves high accuracy on unseen settings. Together, PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework for studying policy compliance in web agent trajectories, and show that accurate and generalizable guardrails are feasible at small scales.", "link": "https://arxiv.org/abs/2510.03485", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.652005Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03506v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows", "summary": "arXiv:2510.03506v1 Announce Type: new Abstract: We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.", "link": "https://arxiv.org/abs/2510.03506", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.652281Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03605v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Understanding the Role of Training Data in Test-Time Scaling", "summary": "arXiv:2510.03605v1 Announce Type: new Abstract: Test-time scaling improves the reasoning capabilities of large language models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts (CoTs). This enables models to tackle more complex problem by breaking them down into additional steps, backtracking, and correcting mistakes. Despite its strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions in the training data under which long CoTs emerge, and when such long CoTs improve the performance, remain unclear. In this paper, we study the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. Our analysis provides a theoretical explanation for several intriguing observations: First, at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. Second, if the skills required to solve a downstream task are not sufficiently present in the training data, increasing test-time compute can harm performance. Finally, we characterize task hardness via the smallest eigenvalue of its feature covariance matrix and show that training on a diverse, relevant, and hard set of tasks results in best performance for test-time scaling. We confirm our findings with experiments on large, nonlinear transformer architectures.", "link": "https://arxiv.org/abs/2510.03605", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653185Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03612v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Cross-Modal Content Optimization for Steering Web Agent Preferences", "summary": "arXiv:2510.03612v1 Announce Type: new Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes selection tasks like content recommendation or product ranking by combining multimodal perception with preference reasoning. Recent studies reveal that these agents are vulnerable against attackers who can bias selection outcomes through preference manipulations using adversarial pop-ups, image perturbations, or content tweaks. Existing work, however, either assumes strong white-box access, with limited single-modal perturbations, or uses impractical settings. In this paper, we demonstrate, for the first time, that joint exploitation of visual and textual channels yields significantly more powerful preference manipulations under realistic attacker capabilities. We introduce Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible modifications to an item's visual and natural language descriptions, exploiting CLIP-transferable image perturbations and RLHF-induced linguistic biases to steer agent decisions. In contrast to prior studies that assume gradient access, or control over webpages, or agent memory, we adopt a realistic black-box threat setup: a non-privileged adversary can edit only their own listing's images and textual metadata, with no insight into the agent's model internals. We evaluate CPS on agents powered by state-of-the-art proprietary and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both movie selection and e-commerce tasks. Our results show that CPS is significantly more effective than leading baseline methods. For instance, our results show that CPS consistently outperforms baselines across all models while maintaining 70% lower detection rates, demonstrating both effectiveness and stealth. These findings highlight an urgent need for robust defenses as agentic systems play an increasingly consequential role in society.", "link": "https://arxiv.org/abs/2510.03612", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653283Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03632v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information", "summary": "arXiv:2510.03632v1 Announce Type: new Abstract: Tree search has become as a representative framework for test-time reasoning with large language models (LLMs), exemplified by methods such as Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning paths. However, it remains difficult to provide instant and reliable quantitative assessments of intermediate reasoning step quality, and extensive path exploration is computationally costly. To address this, we propose Mutual Information Tree Search (MITS), a novel framework that guides reasoning with information-theoretic principles. MITS introduces an effective scoring function based on pointwise mutual information (PMI), which enables step-wise evaluation of reasoning paths and search tree expansion via beam search without expensive look-ahead simulations, achieving superior reasoning performances while maintaining computational efficiency. The framework is complemented by an entropy-based dynamic sampling strategy that adaptively allocates computational resources to uncertain reasoning steps where exploration is most beneficial. For final prediction, MITS employs a weighted voting scheme that combines PMI scores with prediction consensus. Through comprehensive experiments on diverse reasoning benchmarks, MITS consistently surpasses baseline methods, establishing a principled and efficient framework for LLM reasoning.", "link": "https://arxiv.org/abs/2510.03632", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653355Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03680v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs", "summary": "arXiv:2510.03680v1 Announce Type: new Abstract: Diffusion large language models (dLLMs) have emerged as a promising alternative to autoregressive models, offering flexible generation orders and strong performance on complex reasoning tasks. However, instruction-tuned dLLMs exhibit a critical vulnerability we term \\texttt{} overflow: as allocated sequence length increases, responses paradoxically become shorter, collapsing into early termination or degenerating into streams of \\texttt{} tokens. Although noticed in practice, this issue has not been systematically analyzed. We trace its root cause to the dual role of \\texttt{} as both termination and padding, which concentrates probability mass on \\texttt{} at later positions and propagates backward to trigger early termination. To address this, we introduce Rainbow Padding, a simple remedy that replaces repeated \\texttt{} placeholders with a repeating cycle of distinct padding tokens, distributing probability mass and breaking \\texttt{} dominance. Experiments show that Rainbow Padding substantially improves length robustness and output quality, with as few as seven padding tokens sufficient to prevent early termination. Moreover, the method integrates efficiently into existing instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data yields significant improvements, making this solution highly practical. The code is publicly available at https://github.com/quasar529/rainbow-padding.", "link": "https://arxiv.org/abs/2510.03680", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653431Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03696v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models", "summary": "arXiv:2510.03696v1 Announce Type: new Abstract: Evaluating the quality of multi-turn chatbot interactions remains challenging, as most existing methods assess interactions at the turn level without addressing whether a user's overarching goal was fulfilled. A ``goal'' here refers to an information need or task, such as asking for policy information or applying for leave. We propose a comprehensive framework for goal-oriented evaluation of multi-agent systems (MAS), introducing the \\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals, and a \\textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for failure in multi-agent chatbots. Our method segments conversations by user goals and evaluates success using all relevant turns. We present a model-based evaluation system combining teacher LLMs, where domain experts define goals, set quality standards serving as a guidance for the LLMs. The LLMs use ``thinking tokens'' to produce interpretable rationales, enabling \\textit{explainable}, \\textit{data-efficient} evaluations. In an enterprise setting, we apply our framework to evaluate AIDA, a zero-to-one employee conversational agent system built as a ground-up multi-agent conversational agent, and observe GSR improvement from 63\\% to 79\\% over six months since its inception. Our framework is generic and offers actionable insights through a detailed defect taxonomy based on analysis of failure points in multi-agent chatbots, diagnosing overall success, identifying key failure modes, and informing system improvements.", "link": "https://arxiv.org/abs/2510.03696", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653513Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03700v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis", "summary": "arXiv:2510.03700v1 Announce Type: new Abstract: An accurate differential diagnosis (DDx) is essential for patient care, shaping therapeutic decisions and influencing outcomes. Recently, Large Language Models (LLMs) have emerged as promising tools to support this process by generating a DDx list from patient narratives. However, existing evaluations of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy, which fail to distinguish between clinically relevant near-misses and diagnostically distant errors. To mitigate this limitation, we introduce H-DDx, a hierarchical evaluation framework that better reflects clinical relevance. H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses to ICD-10 codes and applies a hierarchical metric that credits predictions closely related to the ground-truth diagnosis. In benchmarking 22 leading models, we show that conventional flat metrics underestimate performance by overlooking clinically meaningful outputs, with our results highlighting the strengths of domain-specialized open-source models. Furthermore, our framework enhances interpretability by revealing hierarchical error patterns, demonstrating that LLMs often correctly identify the broader clinical context even when the precise diagnosis is missed.", "link": "https://arxiv.org/abs/2510.03700", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653580Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03727v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Bridging the Gap Between Multimodal Foundation Models and World Models", "summary": "arXiv:2510.03727v1 Announce Type: new Abstract: Humans understand the world through the integration of multiple sensory modalities, enabling them to perceive, reason about, and imagine dynamic physical processes. Inspired by this capability, multimodal foundation models (MFMs) have emerged as powerful tools for multimodal understanding and generation. However, today's MFMs fall short of serving as effective world models. They lack the essential ability such as perform counterfactual reasoning, simulate dynamics, understand the spatiotemporal information, control generated visual outcomes, and perform multifaceted reasoning. We investigates what it takes to bridge the gap between multimodal foundation models and world models. We begin by improving the reasoning capabilities of MFMs through discriminative tasks and equipping MFMs with structured reasoning skills, such as causal inference, counterfactual thinking, and spatiotemporal reasoning, enabling them to go beyond surface correlations and understand deeper relationships within visual and textual data. Next, we explore generative capabilities of multimodal foundation models across both image and video modalities, introducing new frameworks for structured and controllable generation. Our approaches incorporate scene graphs, multimodal conditioning, and multimodal alignment strategies to guide the generation process, ensuring consistency with high-level semantics and fine-grained user intent. We further extend these techniques to controllable 4D generation, enabling interactive, editable, and morphable object synthesis over time and space.", "link": "https://arxiv.org/abs/2510.03727", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653662Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03771v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation", "summary": "arXiv:2510.03771v1 Announce Type: new Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable evaluation. While LLMs excel in verifiable tasks like coding and mathematics, where gold-standard solutions are available, adoption remains challenging for subjective tasks that lack a single correct answer. E-commerce Query Rewriting (QR) is one such problem where determining whether a rewritten query properly captures the user intent is extremely difficult to figure out algorithmically. In this work, we introduce OptAgent, a novel framework that combines multi-agent simulations with genetic algorithms to verify and optimize queries for QR. Instead of relying on a static reward model or a single LLM judge, our approach uses multiple LLM-based agents, each acting as a simulated shopping customer, as a dynamic reward signal. The average of these agent-derived scores serves as an effective fitness function for an evolutionary algorithm that iteratively refines the user's initial query. We evaluate OptAgent on a dataset of 1000 real-world e-commerce queries in five different categories, and we observe an average improvement of 21.98% over the original user query and 3.36% over a Best-of-N LLM rewriting baseline.", "link": "https://arxiv.org/abs/2510.03771", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653762Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03777v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time", "summary": "arXiv:2510.03777v1 Announce Type: new Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been shown to improve model performance on complex tasks. Although it is an effective way of scaling inference time, it often struggles to generate diverse solution candidates, frequently relying on the same underlying approach to solve the problem and thus producing redundant samples. To address this limitation, we propose a new inference algorithm, GuidedSampling, which decouples the exploration and generation phases during inference, increasing diversity of generated candidate solutions. The exploration phase identifies multiple concepts that can be utilized to solve the problem, while the generation phase applies a specific concept to provide final solution candidates. We first define the theoretical bounds of GuidedSampling and then empirically demonstrate that it improves the performance of base model at pass@50 by on an average ~21.6% across various benchmarks compared to RS. Furthermore, models trained on trajectories of GuidedSampling exhibit substantial performance improvements at pass@5 by on an average ~9.7%, compared to models trained on traditional RS. Additionally, models trained with GuidedSampling increases the average number of concepts per instance (1.67 -> 3.03), yielding a diverse set of candidates than traditional RS.", "link": "https://arxiv.org/abs/2510.03777", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653848Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03845v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "The Hidden Game Problem", "summary": "arXiv:2510.03845v1 Announce Type: new Abstract: This paper investigates a class of games with large strategy spaces, motivated by challenges in AI alignment and language games. We introduce the hidden game problem, where for each player, an unknown subset of strategies consistently yields higher rewards compared to the rest. The central question is whether efficient regret minimization algorithms can be designed to discover and exploit such hidden structures, leading to equilibrium in these subgames while maintaining rationality in general. We answer this question affirmatively by developing a composition of regret minimization techniques that achieve optimal external and swap regret bounds. Our approach ensures rapid convergence to correlated equilibria in hidden subgames, leveraging the hidden game structure for improved computational efficiency.", "link": "https://arxiv.org/abs/2510.03845", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653896Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03847v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs", "summary": "arXiv:2510.03847v1 Announce Type: new Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are sufficient and often superior for agentic workloads where the objective is schema- and API-constrained accuracy rather than open-ended generation. We synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini, Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B, DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4, StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with guided decoding libraries (XGrammar, Outlines). We formalize SLM-default, LLM-fallback systems with uncertainty-aware routing and verifier cascades, and propose engineering metrics that reflect real production goals: cost per successful task (CPS), schema validity rate, executable call rate, p50/p95 latency, and energy per request. Guided decoding, strict JSON Schema outputs, and validator-first tool execution close much of the capability gap with larger models and often let SLMs match or surpass LLMs on tool use, function calling, and RAG at 10x-100x lower token cost with materially better latency and energy. We provide design patterns for agent stacks that prioritize SLMs: schema-first prompting, type-safe function registries, confidence scoring with verifier rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits where fallback remains valuable (open-domain reasoning and some long-horizon planning). The result is a practical blueprint for building fast, inexpensive, and reliable agents that default to SLMs while preserving headroom with targeted LLM assistance. Keywords: small language models, agents, function calling, structured outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency, edge inference", "link": "https://arxiv.org/abs/2510.03847", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653951Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03851v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Algorithm Generation via Creative Ideation", "summary": "arXiv:2510.03851v1 Announce Type: new Abstract: Designing system algorithms remains challenging, where the discontinuous nature of the solution space often forces system engineers to rely on generic heuristics at the expense of performance. We study whether LLMs can practically drive algorithm generation, and find that they are biased towards well-known generic designs, rather than making the creative leaps needed to navigate the discontinuous solution space. To address this limitation, we introduce MetaMuse, a framework for creative ideation built on three self-reflection principles: (1) quantifying solution diversity and usefulness in measurable performance space, rather than abstract idea space, (2) steering ideation through external stimuli, rather than internal randomness, and (3) constructing executable solutions using waypoint reasoning, rather than free-form chain-of-thought. Extensive evaluation shows that MetaMuse can generate high-performing solutions for two critical problems at a global cloud provider: cache replacement (reducing cache misses by up to 35.76%) and online bin packing (reducing bin usage by up to 30.93%).", "link": "https://arxiv.org/abs/2510.03851", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.653986Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03859v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning", "summary": "arXiv:2510.03859v1 Announce Type: new Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot on finding anomalies quickly. As more complex systems, like smart healthcare, energy grids and industrial automation, appear, it is easier to see the shortcomings of older methods of detection. Monitoring failures usually happen in dynamic, high dimensional situations, especially when data is incomplete, messy or always evolving. Such limits point out the requirement for adaptive, intelligent systems that always improve and think. LLMs are now capable of significantly changing how context is understood and semantic inference is done across all types of data. This proposal suggests using an LLM supported contextual reasoning method along with XAI agents to improve how anomalies are found in significant IoT environments. To discover hidden patterns and notice inconsistencies in data streams, it uses attention methods, avoids dealing with details from every time step and uses memory buffers with meaning. Because no code AI stresses transparency and interpretability, people can check and accept the AI's decisions, helping ensure AI follows company policies. The two architectures are put together in a test that compares the results of the traditional model with those of the suggested LLM enhanced model. Important measures to check are the accuracy of detection, how much inaccurate information is included in the results, how clearly the findings can be read and how fast the system responds under different test situations. The metaheuristic is tested in simulations of real world smart grid and healthcare contexts to check its adaptability and reliability. From the study, we see that the new approach performs much better than most existing models in both accuracy and interpretation, so it could be a good fit for future anomaly detection tasks in IoT", "link": "https://arxiv.org/abs/2510.03859", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654040Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03863v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation", "summary": "arXiv:2510.03863v1 Announce Type: new Abstract: Online services rely on CAPTCHAs as a first line of defense against automated abuse, yet recent advances in multi-modal large language models (MLLMs) have eroded the effectiveness of conventional designs that focus on text recognition or 2D image understanding. To address this challenge, we present Spatial CAPTCHA, a novel human-verification framework that leverages fundamental differences in spatial reasoning between humans and MLLMs. Unlike existing CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning, perspective-taking, occlusion handling, and mental rotation. These skills are intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The system employs a procedural generation pipeline with constraint-based difficulty control, automated correctness verification, and human-in-the-loop validation to ensure scalability, robustness, and adaptability. Evaluation on a corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0% Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA, which confirms its effectiveness as both a security mechanism and a diagnostic tool for spatial reasoning in AI.", "link": "https://arxiv.org/abs/2510.03863", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654079Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03886v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Rare Text Semantics Were Always There in Your Diffusion Transformer", "summary": "arXiv:2510.03886v1 Announce Type: new Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim for exceptional visual fidelity. As these models advance, users continually push the boundary with imaginative or rare prompts, which advanced models still falter in generating, since their concepts are often too scarce to leave a strong imprint during pre-training. In this paper, we propose a simple yet effective intervention that surfaces rare semantics inside MM-DiTs without additional training steps, data, denoising-time optimization, or reliance on external modules (e.g., large language models). In particular, the joint-attention mechanism intrinsic to MM-DiT sequentially updates text embeddings alongside image embeddings throughout transformer blocks. We find that by mathematically expanding representational basins around text token embeddings via variance scale-up before the joint-attention blocks, rare semantics clearly emerge in MM-DiT's outputs. Furthermore, our results generalize effectively across text-to-vision tasks, including text-to-image, text-to-video, and text-driven image editing. Our work invites generative models to reveal the semantics that users intend, once hidden yet ready to surface.", "link": "https://arxiv.org/abs/2510.03886", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654128Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03892v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Kantian-Utilitarian XAI: Meta-Explained", "summary": "arXiv:2510.03892v1 Announce Type: new Abstract: We present a gamified explainable AI (XAI) system for ethically aware consumer decision-making in the coffee domain. Each session comprises six rounds with three options per round. Two symbolic engines provide real-time reasons: a Kantian module flags rule violations (e.g., child labor, deforestation risk without shade certification, opaque supply chains, unsafe decaf), and a utilitarian module scores options via multi-criteria aggregation over normalized attributes (price, carbon, water, transparency, farmer income share, taste/freshness, packaging, convenience). A meta-explainer with a regret bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a deontically clean, near-parity option when welfare loss is small. We release a structured configuration (attribute schema, certification map, weights, rule set), a policy trace for auditability, and an interactive UI.", "link": "https://arxiv.org/abs/2510.03892", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654159Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.03969v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Quantifying Risks in Multi-turn Conversation with Large Language Models", "summary": "arXiv:2510.03969v1 Announce Type: new Abstract: Large Language Models (LLMs) can produce catastrophic responses in conversational settings that pose serious risks to public safety and security. Existing evaluations often fail to fully reveal these vulnerabilities because they rely on fixed attack prompt sequences, lack statistical guarantees, and do not scale to the vast space of multi-turn conversations. In this work, we propose QRLLM, a novel, principled Certification framework for Catastrophic risks in multi-turn Conversation for LLMs that bounds the probability of an LLM generating catastrophic responses under multi-turn conversation distributions with statistical guarantees. We model multi-turn conversations as probability distributions over query sequences, represented by a Markov process on a query graph whose edges encode semantic similarity to capture realistic conversational flow, and quantify catastrophic risks using confidence intervals. We define several inexpensive and practical distributions: random node, graph path, adaptive with rejection. Our results demonstrate that these distributions can reveal substantial catastrophic risks in frontier models, with certified lower bounds as high as 70\\% for the worst model, highlighting the urgent need for improved safety training strategies in frontier LLMs.", "link": "https://arxiv.org/abs/2510.03969", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654200Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04009v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models", "summary": "arXiv:2510.04009v1 Announce Type: new Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities far beyond conventional tasks. Creativity, long regarded as a hallmark of human intelligence and a driver of innovation, is now increasingly recognized as a critical dimension of machine intelligence in the era of generative FMs, complementing traditional measures of accuracy. However, existing evaluation frameworks for creativity remain fragmented, relying on ad hoc metrics not firmly grounded in established theories. To address this gap, we introduce C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs. C^2-Eval distinguishes between two complementary forms of creativity: convergent creativity, where tasks admit constrained solutions (e.g., code generation), and divergent creativity, where tasks are open-ended (e.g., storytelling). It evaluates both dimensions using fine-grained criteria derived from social-science theory, focusing on Usefulness, Originality, and Surprise (U-O-S). Through extensive experiments on leading proprietary and open-source models, we analyze trade-offs in their creative capabilities. Our results highlight both the strengths and challenges of current FMs in pursuing a creative machine mind, showing that C^2-Eval is an effective lens for examining the evolving landscape of creative AI.", "link": "https://arxiv.org/abs/2510.04009", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654249Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04017v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Zephyrus: An Agentic Framework for Weather Science", "summary": "arXiv:2510.04017v1 Announce Type: new Abstract: Foundation models for weather science are pre-trained on vast amounts of structured numerical data and outperform traditional weather forecasting systems. However, these models lack language-based reasoning capabilities, limiting their utility in interactive scientific workflows. Large language models (LLMs) excel at understanding and generating text but cannot reason about high-dimensional meteorological datasets. We bridge this gap by building a novel agentic framework for weather science. Our framework includes a Python code-based environment for agents (ZephyrusWorld) to interact with weather data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying for geographical masks from natural language, weather forecasting, and climate simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather agent that iteratively analyzes weather datasets, observes results, and refines its approach through conversational feedback loops. We accompany the agent with a new benchmark, ZephyrusBench, with a scalable data generation pipeline that constructs diverse question-answer pairs across weather-related tasks, from basic lookups to advanced forecasting, extreme event detection, and counterfactual reasoning. Experiments on this benchmark demonstrate the strong performance of Zephyrus agents over text-only baselines, outperforming them by up to 35 percentage points in correctness. However, on harder tasks, Zephyrus performs similarly to text-only baselines, highlighting the challenging nature of our benchmark and suggesting promising directions for future work.", "link": "https://arxiv.org/abs/2510.04017", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654296Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04023v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions", "summary": "arXiv:2510.04023v1 Announce Type: new Abstract: Recent advances in large language models (LLMs) have enabled a new class of AI agents that automate multiple stages of the data science workflow by integrating planning, tool use, and multimodal reasoning across text, code, tables, and visuals. This survey presents the first comprehensive, lifecycle-aligned taxonomy of data science agents, systematically analyzing and mapping forty-five systems onto the six stages of the end-to-end data science process: business understanding and data acquisition, exploratory analysis and visualization, feature engineering, model building and selection, interpretation and explanation, and deployment and monitoring. In addition to lifecycle coverage, we annotate each agent along five cross-cutting design dimensions: reasoning and planning style, modality integration, tool orchestration depth, learning and alignment methods, and trust, safety, and governance mechanisms. Beyond classification, we provide a critical synthesis of agent capabilities, highlight strengths and limitations at each stage, and review emerging benchmarks and evaluation practices. Our analysis identifies three key trends: most systems emphasize exploratory analysis, visualization, and modeling while neglecting business understanding, deployment, and monitoring; multimodal reasoning and tool orchestration remain unresolved challenges; and over 90% lack explicit trust and safety mechanisms. We conclude by outlining open challenges in alignment stability, explainability, governance, and robust evaluation frameworks, and propose future research directions to guide the development of robust, trustworthy, low-latency, transparent, and broadly accessible data science agents.", "link": "https://arxiv.org/abs/2510.04023", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654345Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04033v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "A global log for medical AI", "summary": "arXiv:2510.04033v1 Announce Type: new Abstract: Modern computer systems often rely on syslog, a simple, universal protocol that records every critical event across heterogeneous infrastructure. However, healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals rush to pilot large language models and other AI-based clinical decision support tools, we still lack a standard way to record how, when, by whom, and for whom these AI models are used. Without that transparency and visibility, it is challenging to measure real-world performance and outcomes, detect adverse events, or correct bias or dataset drift. In the spirit of syslog, we introduce MedLog, a protocol for event-level logging of clinical AI. Any time an AI model is invoked to interact with a human, interface with another algorithm, or act independently, a MedLog record is created. This record consists of nine core fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and feedback, providing a structured and consistent record of model activity. To encourage early adoption, especially in low-resource settings, and minimize the data footprint, MedLog supports risk-based sampling, lifecycle-aware retention policies, and write-behind caching; detailed traces for complex, agentic, or multi-stage workflows can also be captured under MedLog. MedLog can catalyze the development of new databases and software to store and analyze MedLog records. Realizing this vision would enable continuous surveillance, auditing, and iterative improvement of medical AI, laying the foundation for a new form of digital epidemiology.", "link": "https://arxiv.org/abs/2510.04033", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654390Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04040v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning", "summary": "arXiv:2510.04040v1 Announce Type: new Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models. To the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.", "link": "https://arxiv.org/abs/2510.04040", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654435Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04048v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Increasing LLM response trustworthiness using voting ensembles", "summary": "arXiv:2510.04048v1 Announce Type: new Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to quantify the uncertainty in their responses, making them difficult to trust in high-stakes applications. One of the simplest approaches to eliciting more accurate answers is to select the mode of many responses, a technique known as ensembling. In this work, we expand on typical ensembling approaches by looking at ensembles with a variable voting threshold. We introduce a theoretical framework for question answering and show that, by permitting ensembles to \"abstain\" from providing an answer when the dominant response falls short of the threshold, it is possible to dramatically increase the trustworthiness of the remaining answers. From this framework, we derive theoretical results as well as report experimental results on two problem domains: arithmetic problem solving and clinical-note question-answering. In both domains, we observe that large gains in answer trustworthiness can be achieved using highly restrictive voting ensembles, while incurring relatively modest reductions in response yield and accuracy. Due to this quality, voting ensembles may be particularly useful in applications - such as healthcare and data annotation - that require a high degree of certainty but which may not require that every question receive an automated answer.", "link": "https://arxiv.org/abs/2510.04048", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654477Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04051v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Toward a unified framework for data-efficient evaluation of large language models", "summary": "arXiv:2510.04051v1 Announce Type: new Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a cornerstone of their development, yet it's often computationally and financially prohibitive. While Item Response Theory (IRT) offers a promising path toward data-efficient evaluation by disentangling model capability from item difficulty, existing IRT-based methods are hampered by significant limitations. They are typically restricted to binary correctness metrics, failing to natively handle the continuous scores used in generative tasks, and they operate on single benchmarks, ignoring valuable structural knowledge like correlations across different metrics or benchmarks. To overcome these challenges, we introduce LEGO-IRT, a unified and flexible framework for data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both binary and continuous evaluation metrics. Moreover, it introduces a factorized architecture to explicitly model and leverage structural knowledge, decomposing model ability estimates into a general component and structure-specific (e.g., per-metric or per-benchmark) components. Through extensive experiments involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves stable capability estimates using just $3\\%$ of the total evaluation items. We demonstrate that incorporating structural knowledge reduces estimation error by up to $10\\%$ and reveal that the latent abilities estimated by our framework may align more closely with human preferences.", "link": "https://arxiv.org/abs/2510.04051", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654520Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04064v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion", "summary": "arXiv:2510.04064v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly expected to navigate the nuances of human emotion. While research confirms that LLMs can simulate emotional intelligence, their internal emotional mechanisms remain largely unexplored. This paper investigates the latent emotional representations within modern LLMs by asking: how, where, and for how long is emotion encoded in their neural architecture? To address this, we introduce a novel, large-scale Reddit corpus of approximately 400,000 utterances, balanced across seven basic emotions through a multi-stage process of classification, rewriting, and synthetic generation. Using this dataset, we employ lightweight \"probes\" to read out information from the hidden layers of various Qwen3 and LLaMA models without altering their parameters. Our findings reveal that LLMs develop a surprisingly well-defined internal geometry of emotion, which sharpens with model scale and significantly outperforms zero-shot prompting. We demonstrate that this emotional signal is not a final-layer phenomenon but emerges early and peaks mid-network. Furthermore, the internal states are both malleable (they can be influenced by simple system prompts) and persistent, as the initial emotional tone remains detectable for hundreds of subsequent tokens. We contribute our dataset, an open-source probing toolkit, and a detailed map of the emotional landscape within LLMs, offering crucial insights for developing more transparent and aligned AI systems. The code and dataset are open-sourced.", "link": "https://arxiv.org/abs/2510.04064", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654567Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04073v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention", "summary": "arXiv:2510.04073v1 Announce Type: new Abstract: The rise of artificial intelligence (AI) as super-capable assistants has transformed productivity and decision-making across domains. Yet, this integration raises critical concerns about value alignment - ensuring AI behaviors remain consistent with human ethics and intentions. A key risk is value drift, where AI systems deviate from aligned values due to evolving contexts, learning dynamics, or unintended optimizations, potentially leading to inefficiencies or ethical breaches. We propose the Moral Anchor System (MAS), a novel framework to detect, predict, and mitigate value drift in AI agents. MAS combines real-time Bayesian inference for monitoring value states, LSTM networks for forecasting drift, and a human-centric governance layer for adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent breaches, while reducing false positives and alert fatigue via supervised fine-tuning with human feedback. Our hypothesis: integrating probabilistic drift detection, predictive analytics, and adaptive governance can reduce value drift incidents by 80 percent or more in simulations, maintaining high detection accuracy (85 percent) and low false positive rates (0.08 post-adaptation). Rigorous experiments with goal-misaligned agents validate MAS's scalability and responsiveness. MAS's originality lies in its predictive and adaptive nature, contrasting static alignment methods. Contributions include: (1) MAS architecture for AI integration; (2) empirical results prioritizing speed and usability; (3) cross-domain applicability insights; and (4) open-source code for replication.", "link": "https://arxiv.org/abs/2510.04073", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654612Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04089v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows", "summary": "arXiv:2510.04089v1 Announce Type: new Abstract: Large language models (LLMs) have exhibited significant capabilities in addressing challenging problems throughout various fields, often through the use of agentic workflows that adhere to structured instructions and multi-step procedures. However, designing such workflows demands substantial manual effort, posing challenges to scalability and generalizability. Recent studies have aimed to minimize the human intervention needed for their construction, leading to advances in automated techniques for optimizing agentic workflows. However, current approaches are often constrained by their limited representational capacity, insufficient adaptability, weak scalability, and pairwise comparison paradigm -- issues that stem primarily from a dependence on discrete optimization techniques. To overcome these limitations, we introduce a new score-based preference approach, refereed as SPOGW, which operates directly on cardinal reward signals through group-wise comparison and enables more efficient and stable optimization in a continuous space. SPOGW incorporates Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL), which regulates training update by placing greater emphasis on the advantageous regions of the policy response. In five benchmark datasets covering mathematical reasoning, coding, and question answering, SPOGW matches or exceeds the performance of current state-of-the-art approaches, presenting a viable and forward-looking methodology for automated generation and optimization of agentic workflows.", "link": "https://arxiv.org/abs/2510.04089", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654655Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04093v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems", "summary": "arXiv:2510.04093v1 Announce Type: new Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES) aims to assess students' mastery of knowledge concepts from heterogeneous, noisy interactions. Recent work has tried to utilize Large Language Models (LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are prone to noise-induced misjudgments. Specially, WIES's open environment continuously attracts new students and produces vast amounts of response logs, exacerbating the data imbalance and noise issues inherent in traditional educational systems. To address these challenges, we propose DLLM, a Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first constructs independent subgraphs based on response correctness, then applies relation augmentation alignment module to mitigate data imbalance. The two subgraph representations are then fused and aligned with LLM-derived, semantically augmented representations. Importantly, before each alignment step, DLLM employs a two-stage denoising diffusion module to eliminate intrinsic noise while assisting structural representation alignment. Specifically, unconditional denoising diffusion first removes erroneous information, followed by conditional denoising diffusion based on graph-guided to eliminate misleading information. Finally, the noise-robust representation that integrates semantic knowledge and structural information is fed into existing cognitive diagnosis models for prediction. Experimental results on three publicly available web-based educational platform datasets demonstrate that our DLLM achieves optimal predictive performance across varying noise levels, which demonstrates that DLLM achieves noise robustness while effectively leveraging semantic knowledge from LLM.", "link": "https://arxiv.org/abs/2510.04093", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654708Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04097v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning", "summary": "arXiv:2510.04097v1 Announce Type: new Abstract: Automating the conversion of UI images into web code is a critical task for front-end development and rapid prototyping. Advances in multimodal large language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet existing benchmarks remain limited in data diversity and evaluation reliability. To address these issues, we present WebRenderBench, a large-scale benchmark of 22.5k webpages collected from real-world portal sites, offering greater diversity, complexity, and realism than prior benchmarks. We further propose a novel evaluation metric that measures layout and style consistency from the final rendered pages. Unlike vision-based methods that rely on costly LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry, our approach enables more efficient, objective, and reliable UI quality assessment. Finally, we introduce the Automated Layout and Style Inspection Agent (ALISA), which integrates this metric into reinforcement learning as a reward signal to enhance training on crawled asymmetric webpages. Experiments show that ALISA significantly boosts generation performance, achieving state-of-the-art results across multiple metrics.", "link": "https://arxiv.org/abs/2510.04097", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654748Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04116v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning", "summary": "arXiv:2510.04116v1 Announce Type: new Abstract: Meta reasoning behaviors work as a skeleton to guide large language model (LLM) reasoning, thus help to improve reasoning performance. However, prior researches implement meta reasoning skeleton with manually designed structure, limiting ability to adapt to query-specific requirement and capture intricate logical dependency among reasoning steps. To deal with the challenges, we represent meta reasoning skeleton with directed acyclic graph (DAG) to unify skeletons proposed in prior works and model intricate logical dependency. Then we propose AutoMR, a framework that searches for query-aware meta reasoning skeleton automatically inspired by automated machine learning (AutoML). Specifically, we construct search space based on DAG representation of skeleton and then formulate the search problem. We design a dynamic skeleton sampling algorithm by expanding meta reasoning skeleton along with reasoning context at inference time. This algorithm can derive any meta reasoning skeleton in search space efficiently and adapt skeleton to evolving base reasoning context, thus enable efficient query-aware skeleton search. We conduct experiments on extensive benchmark datasets. Experimental results show that AutoMR achieves better reasoning performance than previous works broadly.", "link": "https://arxiv.org/abs/2510.04116", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654812Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04128v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Internal states before wait modulate reasoning patterns", "summary": "arXiv:2510.04128v1 Announce Type: new Abstract: Prior work has shown that a significant driver of performance in reasoning models is their ability to reason and self-correct. A distinctive marker in these reasoning traces is the token wait, which often signals reasoning behavior such as backtracking. Despite being such a complex behavior, little is understood of exactly why models do or do not decide to reason in this particular manner, which limits our understanding of what makes a reasoning model so effective. In this work, we address the question whether model's latents preceding wait tokens contain relevant information for modulating the subsequent reasoning process. We train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent attribution technique in the crosscoder setting. We locate a small set of features relevant for promoting/suppressing wait tokens' probabilities. Finally, through a targeted series of experiments analyzing max activating examples and causal interventions, we show that many of our identified features indeed are relevant for the reasoning process and give rise to different types of reasoning patterns such as restarting from the beginning, recalling prior knowledge, expressing uncertainty, and double-checking.", "link": "https://arxiv.org/abs/2510.04128", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654852Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04140v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs", "summary": "arXiv:2510.04140v1 Announce Type: new Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely adopted technique for enhancing the reasoning ability of Large Language Models (LLMs). However, the effectiveness of RLVR strongly depends on the capability of base models. This issue arises because it requires the model to have sufficient capability to perform high-quality exploration, which involves both effectiveness and diversity. Unfortunately, existing methods address this issue by imitating expert trajectories, which improve effectiveness but neglect diversity. To address this, we argue that the expert only needs to provide guidance only at critical decision points rather than the entire reasoning path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation for Token-level Optimization of Reasoning, a framework that provides expert guidance only at critical decision points to perform effective and diverse exploration in RLVR. Extensive experiments show that MENTOR enables models capture the essence of expert strategies rather than surface imitation, thereby performing high-quality exploration and achieving superior overall performance. Our code is available online.", "link": "https://arxiv.org/abs/2510.04140", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654892Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04141v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning", "summary": "arXiv:2510.04141v1 Announce Type: new Abstract: This survey paper chronicles the evolution of evaluation in multimodal artificial intelligence (AI), framing it as a progression of increasingly sophisticated \"cognitive examinations.\" We argue that the field is undergoing a paradigm shift, moving from simple recognition tasks that test \"what\" a model sees, to complex reasoning benchmarks that probe \"why\" and \"how\" it understands. This evolution is driven by the saturation of older benchmarks, where high performance often masks fundamental weaknesses. We chart the journey from the foundational \"knowledge tests\" of the ImageNet era to the \"applied logic and comprehension\" exams such as GQA and Visual Commonsense Reasoning (VCR), which were designed specifically to diagnose systemic flaws such as shortcut learning and failures in compositional generalization. We then survey the current frontier of \"expert-level integration\" benchmarks (e.g., MMBench, SEED-Bench, MMMU) designed for today's powerful multimodal large language models (MLLMs), which increasingly evaluate the reasoning process itself. Finally, we explore the uncharted territories of evaluating abstract, creative, and social intelligence. We conclude that the narrative of AI evaluation is not merely a history of datasets, but a continuous, adversarial process of designing better examinations that, in turn, redefine our goals for creating truly intelligent systems.", "link": "https://arxiv.org/abs/2510.04141", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654935Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04173v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Open Agent Specification (Agent Spec) Technical Report", "summary": "arXiv:2510.04173v1 Announce Type: new Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows AI agents and their workflows to be defined in a way that is compatible across different AI frameworks, promoting portability and interoperability within AI Agent frameworks. Agent Spec aims to resolve the challenges of fragmented agent development by providing a common unified specification that allows AI agents to be designed once and deployed across various frameworks, improving interoperability and reusability, and reducing redundant development efforts. Additionally, Agent Spec facilitates development tools and portability, allowing AI agents to be defined independently of their execution environment and enabling teams to exchange solutions without implementation-specific limitations. Agent Spec benefits four key groups: (i) Agent developers, who gain access to a superset of reusable components and design patterns, enabling them to leverage a broader range of functionalities; (ii) Agent framework and tool developers, who can use Agent Spec as an interchange format and therefore benefit from the support of other frameworks as well as other tools; (iii) Researchers, who can achieve reproducible results and comparability, facilitating more reliable and consistent outcomes; (iv) Enterprises, which benefit from faster prototype-to-deployment, increased productivity, as well as greater scalability and maintainability for their AI agent solutions. This technical report provides an overview of the technical foundations of Agent Spec, including motivation, benefits, and future developments.", "link": "https://arxiv.org/abs/2510.04173", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.654982Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04195v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Constructing coherent spatial memory in LLM agents through graph rectification", "summary": "arXiv:2510.04195v1 Announce Type: new Abstract: Given a map description through global traversal navigation instructions (e.g., visiting each room sequentially with action signals such as north, west, etc.), an LLM can often infer the implicit spatial layout of the environment and answer user queries by providing a shortest path from a start to a destination (for instance, navigating from the lobby to a meeting room via the hall and elevator). However, such context-dependent querying becomes incapable as the environment grows much longer, motivating the need for incremental map construction that builds a complete topological graph from stepwise observations. We propose a framework for LLM-driven construction and map repair, designed to detect, localize, and correct structural inconsistencies in incrementally constructed navigation graphs. Central to our method is the Version Control, which records the full history of graph edits and their source observations, enabling fine-grained rollback, conflict tracing, and repair evaluation. We further introduce an Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation. To properly evaluate our approach, we create a refined version of the MANGO benchmark dataset by systematically removing non-topological actions and inherent structural conflicts, providing a cleaner testbed for LLM-driven construction and map repair. Our approach significantly improves map correctness and robustness, especially in scenarios with entangled or chained inconsistencies. Our results highlight the importance of introspective, history-aware repair mechanisms for maintaining coherent spatial memory in LLM agents.", "link": "https://arxiv.org/abs/2510.04195", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.655030Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04196v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability", "summary": "arXiv:2510.04196v1 Announce Type: new Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications, where they must be both useful and safe. Safety is especially challenging in multimodal settings: images and text can be combined to bypass guardrails, and single objective training can cause policy drift that yields over-refusal on benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed reinforcement learning framework that trains reasoning oriented LMRMs under multimodal, multitask, and multiobjective signals, and we release the resulting model, COSMO-R1. Our approach aims to let safety and capability grow together in one stable pipeline rather than competing during alignment. In experiments, COSMO-R1 improves safety while maintaining-and often improving multimodal reasoning and instruction following, shows stronger robustness to multimodal jailbreaks, and reduces unnecessary refusals. The framework also transfers across backbones with consistent gains. Ablations support the design choices, indicating a simple path to advancing safety and general capability together in LMRMs.", "link": "https://arxiv.org/abs/2510.04196", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.655067Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04206v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework", "summary": "arXiv:2510.04206v1 Announce Type: new Abstract: Recent advances in large language models (LLMs) have sparked growing interest in building generalist agents that can learn through online interactions. However, applying reinforcement learning (RL) to train LLM agents in multi-turn, multi-task settings remains challenging due to lack of scalable infrastructure and stable training algorithms. In this work, we present the AgentRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AgentRL features a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, we design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. Experiments show that AgentRL, trained on open LLMs across five agentic tasks, significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents. Multi-task training with AgentRL matches the best results among all task-specific models. AgentRL is open-sourced at https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in building \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}.", "link": "https://arxiv.org/abs/2510.04206", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.655109Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04265v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Don't Pass$\\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation", "summary": "arXiv:2510.04265v1 Announce Type: new Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often yields unstable, misleading rankings, especially when the number of trials (samples) is limited and compute is constrained. We present a principled Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over $N$ trials (avg$@N$) with posterior estimates of a model's underlying success probability and credible intervals, yielding stable rankings and a transparent decision rule for differences. Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty of any weighted rubric and enabling the use of prior evidence when appropriate. Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$), explaining its empirical robustness while adding principled uncertainty. Empirically, in simulations with known ground-truth success rates and on AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster convergence and greater rank stability than Pass$@k$ and recent variants, enabling reliable comparisons at far smaller sample counts. The framework clarifies when observed gaps are statistically meaningful (non-overlapping credible intervals) versus noise, and it naturally extends to graded, rubric-based evaluations. Together, these results recommend replacing Pass$@k$ for LLM evaluation and ranking with a posterior-based, compute-efficient protocol that unifies binary and non-binary evaluation while making uncertainty explicit. Code is available at https://mohsenhariri.github.io/bayes-kit", "link": "https://arxiv.org/abs/2510.04265", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.655157Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04272v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales", "summary": "arXiv:2510.04272v1 Announce Type: new Abstract: Effective cross-functional coordination is essential for enhancing firm-wide profitability, particularly in the face of growing organizational complexity and scale. Recent advances in artificial intelligence, especially in reinforcement learning (RL), offer promising avenues to address this fundamental challenge. This paper proposes a unified multi-agent RL framework tailored for joint optimization across distinct functional modules, exemplified via coordinating inventory replenishment and personalized product recommendation. We first develop an integrated theoretical model to capture the intricate interplay between these functions and derive analytical benchmarks that characterize optimal coordination. The analysis reveals synchronized adjustment patterns across products and over time, highlighting the importance of coordinated decision-making. Leveraging these insights, we design a novel multi-timescale multi-agent RL architecture that decomposes policy components according to departmental functions and assigns distinct learning speeds based on task complexity and responsiveness. Our model-free multi-agent design improves scalability and deployment flexibility, while multi-timescale updates enhance convergence stability and adaptability across heterogeneous decisions. We further establish the asymptotic convergence of the proposed algorithm. Extensive simulation experiments demonstrate that the proposed approach significantly improves profitability relative to siloed decision-making frameworks, while the behaviors of the trained RL agents align closely with the managerial insights from our theoretical model. Taken together, this work provides a scalable, interpretable RL-based solution to enable effective cross-functional coordination in complex business settings.", "link": "https://arxiv.org/abs/2510.04272", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.655209Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04281v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction", "summary": "arXiv:2510.04281v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse data modalities, but current medical adaptations such as LLaVA-Med often fail to fully exploit the synergy between color fundus photography (CFP) and optical coherence tomography (OCT), and offer limited interpretability of quantitative biomarkers. We introduce GROK, a grounded multimodal large language model that jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of ocular and systemic disease. GROK comprises three core modules: Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment, and Supervised Instruction Fine-Tuning, which together establish a quantitative-to-qualitative diagnostic chain of thought, mirroring real clinical reasoning when producing detailed lesion annotations. To evaluate our approach, we introduce the Grounded Ophthalmic Understanding benchmark, which covers six disease categories and three tasks: macro-level diagnostic classification, report generation quality, and fine-grained clinical assessment of the generated chain of thought. Experiments show that, with only LoRA (Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK outperforms comparable 7B and 32B baselines on both report quality and fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are publicly available in the GROK repository.", "link": "https://arxiv.org/abs/2510.04281", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.655251Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04284v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning", "summary": "arXiv:2510.04284v1 Announce Type: new Abstract: The professionalism of a human doctor in outpatient service depends on two core abilities: the ability to make accurate medical decisions and the medical consultation skill to conduct strategic, empathetic patient inquiry. Existing Large Language Models (LLMs) have achieved remarkable accuracy on medical decision-making benchmarks. However, they often lack the ability to conduct the strategic and empathetic consultation, which is essential for real-world clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor agent trained to master both of the capabilities by ask high-yield questions and conduct strategic multi-turn inquiry to guide decision-making. Our framework introduces three key components: a multi-agent interactive environment, a two-tiered reward architecture that separately optimizes clinical decision-making and communicative inquiry skills, and an experience repository to ground policy learning in high-quality prior trajectories. We evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across multi-facet metrics, such as communication quality, user experience, and task accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source specialized LLMs by a substantial margin with higher parameter efficiency and outperforms powerful proprietary models. Furthermore, the human evaluations show a strong preference for Doctor-R1 to generate human-preferred clinical dialogue, demonstrating the effectiveness of the framework.", "link": "https://arxiv.org/abs/2510.04284", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.655294Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04311v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems", "summary": "arXiv:2510.04311v1 Announce Type: new Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm for harnessing collective intelligence to achieve more advanced forms of AI behaviour. While recent studies suggest that LLM-MAS can outperform LLM single-agent systems (LLM-SAS) on certain tasks, the lack of systematic experimental designs limits the strength and generality of these conclusions. We argue that a principled understanding of task complexity, such as the degree of sequential reasoning required and the breadth of capabilities involved, is essential for assessing the effectiveness of LLM-MAS in task solving. To this end, we propose a theoretical framework characterising tasks along two dimensions: depth, representing reasoning length, and width, representing capability diversity. We theoretically examine a representative class of LLM-MAS, namely the multi-agent debate system, and empirically evaluate its performance in both discriminative and generative tasks with varying depth and width. Theoretical and empirical results show that the benefit of LLM-MAS over LLM-SAS increases with both task depth and width, and the effect is more pronounced with respect to depth. This clarifies when LLM-MAS are beneficial and provides a principled foundation for designing future LLM-MAS methods and benchmarks.", "link": "https://arxiv.org/abs/2510.04311", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-08T01:36:14.655337Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03243v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank", "summary": "arXiv:2510.03243v1 Announce Type: new Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low latency and high throughput, particularly with the growing use of reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve (FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks delay shorter ones queued behind them. In this paper, we introduce PARS, a prompt-aware LLM task scheduler that improves serving efficiency by approximating shortest-job-first (SJF) scheduling through pairwise ranking with margin ranking loss. PARS focuses on impactful scheduling decisions and is seamlessly integrated into the state-of-the-art LLM serving system vLLM. It effectively predicts response-length-based task ordering, reducing latency with minimal overhead. Extensive experiments across multiple LLMs and real-world inference datasets show that PARS significantly improves performance, including for reasoning workloads. Furthermore, our cross-model evaluations demonstrate that the design generalizes well, enabling effective scheduling even when predictors are trained on different LLMs.", "link": "https://arxiv.org/abs/2510.03243", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.427786Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03244v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion", "summary": "arXiv:2510.03244v1 Announce Type: new Abstract: Large time series foundation models often adopt channel-independent architectures to handle varying data dimensions, but this design ignores crucial cross-channel dependencies. Concurrently, existing multimodal approaches have not fully exploited the power of large vision models (LVMs) to interpret spatiotemporal data. Additionally, there remains significant unexplored potential in leveraging the advantages of information extraction from different modalities to enhance time series forecasting performance. To address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO uniquely renders multivariate time series into image, enabling pre-trained LVM to extract complex cross-channel patterns that are invisible to channel-independent models. These visual features are then aligned and fused with representations from the time series modality. By freezing the LVM and training only 7.45% of its parameters, VIFO achieves competitive performance on multiple benchmarks, offering an efficient and effective solution for capturing cross-variable relationships in", "link": "https://arxiv.org/abs/2510.03244", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.427829Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03245v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability", "summary": "arXiv:2510.03245v1 Announce Type: new Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of real world noise and intentional perturbations remains a significant challenge. To address this, attribution methods have been proposed, though their efficacy remains suboptimal and necessitates further refinement. In this paper, we propose a novel category of transferable adversarial attacks, called transferable frequency-aware attacks, enabling frequency-aware exploration via both high-and low-frequency components. Based on this type of attacks, we also propose a novel attribution method, named Frequency-Aware Model Parameter Explorer (FAMPE), which improves the explainability for DNNs. Relative to the current state-of-the-art method AttEXplore, our FAMPE attains an average gain of 13.02% in Insertion Score, thereby outperforming existing approaches. Through detailed ablation studies, we also investigate the role of both high- and low-frequency components in explainability.", "link": "https://arxiv.org/abs/2510.03245", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.427867Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03247v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data", "summary": "arXiv:2510.03247v1 Announce Type: new Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in data-hungry deep learning. However, existing AL algorithms focus almost exclusively on unimodal data, overlooking the substantial annotation burden in multimodal learning. We introduce the first framework for multimodal active learning with unaligned data, where the learner must actively acquire cross-modal alignments rather than labels on pre-aligned pairs. This setting captures the practical bottleneck in modern multimodal pipelines such as CLIP and SigLIP, where unimodal features are easy to obtain but high-quality alignment is costly. We develop a new algorithm that combines uncertainty and diversity principles in a modality-aware design, achieves linear-time acquisition, and applies seamlessly to both pool-based and streaming-based settings. Extensive experiments on benchmark datasets demonstrate that our approach consistently reduces multimodal annotation cost while preserving performance; for instance, on the ColorSwap dataset it cuts annotation requirements by up to $40\\%$ without loss in accuracy.", "link": "https://arxiv.org/abs/2510.03247", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.427979Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03248v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models", "summary": "arXiv:2510.03248v1 Announce Type: new Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over 69 million cases annually worldwide. Finite element (FE) models offer high-fidelity predictions of brain deformation but are computationally expensive, requiring hours per simulation and limiting their clinical utility for rapid decision-making. This study benchmarks state-of-the-art neural operator (NO) architectures for rapid, patient-specific prediction of brain displacement fields, aiming to enable real-time TBI modeling in clinical and translational settings. We formulated TBI modeling as an operator learning problem, mapping subject-specific anatomical MRI, magnetic resonance elastography (MRE) stiffness maps, and demographic features to full-field 3D brain displacement predictions. Four architectures - Fourier Neural Operator (FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator Network (DeepONet) were trained and evaluated on 249 MRE datasets across physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest accuracy (MSE = 0.0023, 94.3\\% spatial fidelity) and preserved fine-scale features, while F-FNO converged 2$\\times$ faster than standard FNO. DeepONet offered the fastest inference (14.5 iterations/s) with a 7$\\times$ computational speed-up over MG-FNO, suggesting utility for embedded or edge computing applications. All NOs reduced computation time from hours to milliseconds without sacrificing anatomical realism. NOs provide an efficient, resolution-invariant approach for predicting brain deformation, opening the door to real-time, patient-specific TBI risk assessment, clinical triage support, and optimization of protective equipment. These results highlight the potential for NO-based digital twins of the human brain, enabling scalable, on-demand biomechanical modeling in both clinical and population health contexts.", "link": "https://arxiv.org/abs/2510.03248", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428043Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03250v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Light Differentiable Logic Gate Networks", "summary": "arXiv:2510.03250v1 Announce Type: new Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency at inference while sustaining competitive accuracy. But vanishing gradients, discretization errors, and high training cost impede scaling these networks. Even with dedicated parameter initialization schemes from subsequent works, increasing depth still harms accuracy. We show that the root cause of these issues lies in the underlying parametrization of logic gate neurons themselves. To overcome this issue, we propose a reparametrization that also shrinks the parameter size logarithmically in the number of inputs per gate. For binary inputs, this already reduces the model size by 4x, speeds up the backward pass by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we show that the accuracy on CIFAR-100 remains stable and sometimes superior to the original parametrization.", "link": "https://arxiv.org/abs/2510.03250", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428077Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03251v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Numerion: A Multi-Hypercomplex Model for Time Series Forecasting", "summary": "arXiv:2510.03251v1 Announce Type: new Abstract: Many methods aim to enhance time series forecasting by decomposing the series through intricate model structures and prior knowledge, yet they are inevitably limited by computational complexity and the robustness of the assumptions. Our research uncovers that in the complex domain and higher-order hypercomplex spaces, the characteristic frequencies of time series naturally decrease. Leveraging this insight, we propose Numerion, a time series forecasting model based on multiple hypercomplex spaces. Specifically, grounded in theoretical support, we generalize linear layers and activation functions to hypercomplex spaces of arbitrary power-of-two dimensions and introduce a novel Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture. Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces of varying dimensions, naturally decomposing and independently modeling the series, and adaptively fuses the latent patterns exhibited in different spaces through a dynamic fusion mechanism. Experiments validate the model`s performance, achieving state-of-the-art results on multiple public datasets. Visualizations and quantitative analyses comprehensively demonstrate the ability of multi-dimensional RHR-MLPs to naturally decompose time series and reveal the tendency of higher dimensional hypercomplex spaces to capture lower frequency features.", "link": "https://arxiv.org/abs/2510.03251", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428126Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03252v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Universal Multi-Domain Translation via Diffusion Routers", "summary": "arXiv:2510.03252v1 Announce Type: new Abstract: Multi-domain translation (MDT) aims to learn translations between multiple domains, yet existing approaches either require fully aligned tuples or can only handle domain pairs seen in training, limiting their practicality and excluding many cross-domain mappings. We introduce universal MDT (UMDT), a generalization of MDT that seeks to translate between any pair of $K$ domains using only $K-1$ paired datasets with a central domain. To tackle this problem, we propose Diffusion Router (DR), a unified diffusion-based framework that models all central$\\leftrightarrow$non-central translations with a single noise predictor conditioned on the source and target domain labels. DR enables indirect non-central translations by routing through the central domain. We further introduce a novel scalable learning strategy with a variational-bound objective and an efficient Tweedie refinement procedure to support direct non-central mappings. Through evaluation on three large-scale UMDT benchmarks, DR achieves state-of-the-art results for both indirect and direct translations, while lowering sampling cost and unlocking novel tasks such as sketch$\\leftrightarrow$segmentation. These results establish DR as a scalable and versatile framework for universal translation across multiple domains.", "link": "https://arxiv.org/abs/2510.03252", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428166Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03253v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents", "summary": "arXiv:2510.03253v1 Announce Type: new Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked with solving complex, long-horizon problems. Aligning these agents via preference-based offline methods like Direct Preference Optimization (DPO) is a promising direction, yet it faces a critical granularity mismatch. Trajectory-level DPO provides a signal that is too coarse for precise credit assignment, while step-level DPO is often too myopic to capture the value of multi-step behaviors. To resolve this challenge, we introduce Hierarchical Preference Learning (HPL), a hierarchical framework that optimizes LLM agents by leveraging preference signals at multiple, synergistic granularities. While HPL incorporates trajectory- and step-level DPO for global and local policy stability, its core innovation lies in group-level preference optimization guided by a dual-layer curriculum. Our approach first decomposes expert trajectories into semantically coherent action groups and then generates contrasting suboptimal groups to enable preference learning at a fine-grained, sub-task level. Then, instead of treating all preference pairs equally, HPL introduces a curriculum scheduler that organizes the learning process from simple to complex. This curriculum is structured along two axes: the group length, representing sub-task complexity, and the sample difficulty, defined by the reward gap between preferred and dispreferred action groups. Experiments on three challenging agent benchmarks show that HPL outperforms existing state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO loss effectively integrates preference signals across multiple granularities, while the dual-layer curriculum is crucial for enabling the agent to solve a wide range of tasks, from simple behaviors to complex multi-step sequences.", "link": "https://arxiv.org/abs/2510.03253", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428219Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03254v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Adversarial training with restricted data manipulation", "summary": "arXiv:2510.03254v1 Announce Type: new Abstract: Adversarial machine learning concerns situations in which learners face attacks from active adversaries. Such scenarios arise in applications such as spam email filtering, malware detection and fake image generation, where security methods must be actively updated to keep up with the everimproving generation of malicious data. Pessimistic Bilevel optimisation has been shown to be an effective method of training resilient classifiers against such adversaries. By modelling these scenarios as a game between the learner and the adversary, we anticipate how the adversary will modify their data and then train a resilient classifier accordingly. However, since existing pessimistic bilevel approaches feature an unrestricted adversary, the model is vulnerable to becoming overly pessimistic and unrealistic. When finding the optimal solution that defeats the classifier, it is possible that the adversary's data becomes nonsensical and loses its intended nature. Such an adversary will not properly reflect reality, and consequently, will lead to poor classifier performance when implemented on real-world data. By constructing a constrained pessimistic bilevel optimisation model, we restrict the adversary's movements and identify a solution that better reflects reality. We demonstrate through experiments that this model performs, on average, better than the existing approach.", "link": "https://arxiv.org/abs/2510.03254", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428267Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03255v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "SciTS: Scientific Time Series Understanding and Generation with LLMs", "summary": "arXiv:2510.03255v1 Announce Type: new Abstract: The scientific reasoning ability of large language models (LLMs) has recently attracted significant attention. Time series, as a fundamental modality in scientific data, presents unique challenges that are often overlooked in current multimodal LLMs, which either encode numerical sequences as text or convert them into images. Such approaches may be insufficient for comprehensive scientific time series understanding and generation. Existing unified time series models typically specialise in either forecasting or analysis, and their effectiveness on non-periodic, heterogeneous scientific signals remains unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12 scientific domains and 43 tasks, with over 50k+ instances, both univariate and multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz in frequency. We benchmark 17 models, including text-only LLMs, multimodal LLMs, and unified time series models, and find that general-purpose LLMs exhibit stronger generalisability than specialised time series models, while representing time series as text or images limits their performance due to excessively long sequences and loss of numerical precision, respectively. We then introduce TimeOmni, a framework that equips LLMs with the ability to understand and generate time series while remaining compatible with general-purpose LLM training. This work fills a gap in both dedicated benchmarks and modelling frameworks for scientific time series, paving the way for LLMs to understand and generate complex temporal scientific data.", "link": "https://arxiv.org/abs/2510.03255", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428319Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03257v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?", "summary": "arXiv:2510.03257v1 Announce Type: new Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate real-time challenge of bundling and matching passengers-each with distinct origins and destinations-to available vehicles, all while navigating significant system uncertainties. Due to the extensive observation space arising from the large number of drivers and orders, order dispatching, though fundamentally a centralized task, is often addressed using Multi-Agent Reinforcement Learning (MARL). However, independent MARL methods fail to capture global information and exhibit poor cooperation among workers, while Centralized Training Decentralized Execution (CTDE) MARL methods suffer from the curse of dimensionality. To overcome these challenges, we propose Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method designed specifically for large-scale order dispatching on ride-sharing platforms. Built on a variant TD3, our approach addresses the vast action space through an action decomposition strategy that breaks down the joint action probability into individual driver action probabilities. To handle the extensive observation space, we introduce a novel BERT-based network, where parameter reuse mitigates parameter growth as the number of drivers and orders increases, and the attention mechanism effectively captures the complex relationships among the large pool of driver and orders. We validate our method using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves approximately an 11.95% improvement over current state-of-the-art methods, with a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our code, trained model parameters, and processed data are publicly available at the repository https://github.com/RS2002/Triple-BERT .", "link": "https://arxiv.org/abs/2510.03257", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428383Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03258v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation", "summary": "arXiv:2510.03258v1 Announce Type: new Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to unknown test data with potential distribution shifts in an online manner. Many existing TTA methods rely on entropy as a confidence metric to optimize the model. However, these approaches are sensitive to the predefined entropy threshold, influencing which samples are chosen for model adaptation. Consequently, potentially reliable target samples are often overlooked and underutilized. For instance, a sample's entropy might slightly exceed the threshold initially, but fall below it after the model is updated. Such samples can provide stable supervised information and offer a normal range of gradients to guide model adaptation. In this paper, we propose a general approach, \\underline{POEM}, to promote TTA via ex\\underline{\\textbf{p}}loring the previously unexpl\\underline{\\textbf{o}}red reliabl\\underline{\\textbf{e}} sa\\underline{\\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch network to strike a balance between extracting domain-agnostic representations and achieving high performance on target data. Comprehensive experiments across multiple architectures demonstrate that POEM consistently outperforms existing TTA methods in both challenging scenarios and real-world domain shifts, while remaining computationally efficient. The effectiveness of POEM is evaluated through extensive analyses and thorough ablation studies. Moreover, the core idea behind POEM can be employed as an augmentation strategy to boost the performance of existing TTA approaches. The source code is publicly available at \\emph{https://github.com/ycarobot/POEM}", "link": "https://arxiv.org/abs/2510.03258", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428430Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03259v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning", "summary": "arXiv:2510.03259v1 Announce Type: new Abstract: Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by itself. We argue that large reasoning models lack this meta-awareness property by proving severe misalignment between true rollouts and predicted meta information. We posit that aligning meta-prediction with true rollouts will lead to significant performance gains. To verify this hypothesis, we design a training pipeline that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced meta-awareness directly translates to improved accuracy. Unlike existing meta-cognitive reasoning models, our method does not require external training sources but leverages self-generated signals to train meta-awareness. Moreover, our method enables efficient training by i) filtering out zero-variance prompts that are either trivial or unsolvable and ii) cutting off lengthy rollouts when they are unlikely to lead to correct answers. The results are inspiring: our strategy yields significant improvements in both accuracy and training efficiency on in-domain tasks and shows strong generalization to out-of-domain benchmarks. More specifically, our method can speed up GRPO training by over 1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 % boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks spanning logical, scientific, and coding domains.", "link": "https://arxiv.org/abs/2510.03259", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428481Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03260v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Semantic-Inductive Attribute Selection for Zero-Shot Learning", "summary": "arXiv:2510.03260v1 Announce Type: new Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial Intelligence Systems, particularly in those that operate in open-world scenarios where systems must adapt to new tasks dynamically. Semantic spaces play a pivotal role as they bridge seen and unseen classes, but whether human-annotated or generated by a machine learning model, they often contain noisy, redundant, or irrelevant attributes that hinder performance. To address this, we introduce a partitioning scheme that simulates unseen conditions in an inductive setting (which is the most challenging), allowing attribute relevance to be assessed without access to semantic information from unseen classes. Within this framework, we study two complementary feature-selection strategies and assess their generalisation. The first adapts embedded feature selection to the particular demands of ZSL, turning model-driven rankings into meaningful semantic pruning; the second leverages evolutionary computation to directly explore the space of attribute subsets more broadly. Experiments on five benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods consistently improve accuracy on unseen classes by reducing redundancy, but in complementary ways: RFS is efficient and competitive though dependent on critical hyperparameters, whereas GA is more costly yet explores the search space more broadly and avoids such dependence. These results confirm that semantic spaces are inherently redundant and highlight the proposed partitioning scheme as an effective tool to refine them under inductive conditions.", "link": "https://arxiv.org/abs/2510.03260", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428527Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03261v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark", "summary": "arXiv:2510.03261v1 Announce Type: new Abstract: Thermal errors in machine tools significantly impact machining precision and productivity. Traditional thermal error correction/compensation methods rely on measured temperature-deformation fields or on transfer functions. Most existing data-driven compensation strategies employ neural networks (NNs) to directly predict thermal errors or specific compensation values. While effective, these approaches are tightly bound to particular error types, spatial locations, or machine configurations, limiting their generality and adaptability. In this work, we introduce a novel paradigm in which NNs are trained to predict high-fidelity temperature and heat flux fields within the machine tool. The proposed framework enables subsequent computation and correction of a wide range of error types using modular, swappable downstream components. The NN is trained using data obtained with the finite element method under varying initial conditions and incorporates a correlation-based selection strategy that identifies the most informative measurement points, minimising hardware requirements during inference. We further benchmark state-of-the-art time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit, Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal Convolutional Network, by training both specialised models, tailored for specific initial conditions, and general models, capable of extrapolating to unseen scenarios. The results show accurate and low-cost prediction of temperature and heat flux fields, laying the basis for enabling flexible and generalisable thermal error correction in machine tool environments.", "link": "https://arxiv.org/abs/2510.03261", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428578Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03263v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models", "summary": "arXiv:2510.03263v1 Announce Type: new Abstract: The impressive capability of modern text-to-image models to generate realistic visuals has come with a serious drawback: they can be misused to create harmful, deceptive or unlawful content. This has accelerated the push for machine unlearning. This new field seeks to selectively remove specific knowledge from a model's training data without causing a drop in its overall performance. However, it turns out that actually forgetting a given concept is an extremely difficult task. Models exposed to attacks using adversarial prompts show the ability to generate so-called unlearned concepts, which can be not only harmful but also illegal. In this paper, we present considerations regarding the ability of models to forget and recall knowledge, introducing the Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which we consider to be a regenerative approach supporting the effective recovery of previously lost knowledge. Moreover, we propose that robustness in knowledge retrieval is a crucial yet underexplored evaluation measure for developing more robust and effective unlearning techniques. Finally, we demonstrate that forgetting occurs in two distinct ways: short-term, where concepts can be quickly recalled, and long-term, where recovery is more challenging.", "link": "https://arxiv.org/abs/2510.03263", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428668Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03265v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "MindCraft: How Concept Trees Take Shape In Deep Models", "summary": "arXiv:2510.03265v1 Announce Type: new Abstract: Large-scale foundation models demonstrate strong performance across language, vision, and reasoning tasks. However, how they internally structure and stabilize concepts remains elusive. Inspired by causal inference, we introduce the MindCraft framework built upon Concept Trees. By applying spectral decomposition at each layer and linking principal directions into branching Concept Paths, Concept Trees reconstruct the hierarchical emergence of concepts, revealing exactly when they diverge from shared representations into linearly separable subspaces. Empirical evaluations across diverse scenarios across disciplines, including medical diagnosis, physics reasoning, and political decision-making, show that Concept Trees recover semantic hierarchies, disentangle latent concepts, and can be widely applied across multiple domains. The Concept Tree establishes a widely applicable and powerful framework that enables in-depth analysis of conceptual representations in deep models, marking a significant step forward in the foundation of interpretable AI.", "link": "https://arxiv.org/abs/2510.03265", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428783Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03266v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model", "summary": "arXiv:2510.03266v1 Announce Type: new Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics, necessitating robust methods for detecting and analyzing anomalous behavior in plant productivity. This study presents a novel application of variational autoencoders (VAE) for identifying extreme events in gross primary productivity (GPP) from Community Earth System Model version 2 simulations across four AR6 regions in the Continental United States. We compare VAE-based anomaly detection with traditional singular spectral analysis (SSA) methods across three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario. The VAE architecture employs three dense layers and a latent space with an input sequence length of 12 months, trained on a normalized GPP time series to reconstruct the GPP and identifying anomalies based on reconstruction errors. Extreme events are defined using 5th percentile thresholds applied to both VAE and SSA anomalies. Results demonstrate strong regional agreement between VAE and SSA methods in spatial patterns of extreme event frequencies, despite VAE producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA across regions and periods). Both methods reveal increasing magnitudes and frequencies of negative carbon cycle extremes toward 2050-80, particularly in Western and Central North America. The VAE approach shows comparable performance to established SSA techniques, while offering computational advantages and enhanced capability for capturing non-linear temporal dependencies in carbon cycle variability. Unlike SSA, the VAE method does not require one to define the periodicity of the signals in the data; it discovers them from the data.", "link": "https://arxiv.org/abs/2510.03266", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428831Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03267v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "PT$^2$-LLM: Post-Training Ternarization for Large Language Models", "summary": "arXiv:2510.03267v1 Announce Type: new Abstract: Large Language Models (LLMs) have shown impressive capabilities across diverse tasks, but their large memory and compute demands hinder deployment. Ternarization has gained attention as a promising compression technique, delivering substantial size reduction and high computational efficiency. However, its potential in the post-training quantization (PTQ) setting remains underexplored, due to the challenge of training-free parameter optimization and the quantization difficulty posed by outliers and dispersed weights. To address these issues, we propose PT$^2$-LLM, a post-training ternarization framework tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which alternates between optimal ternary grid construction and flexible rounding to minimize quantization error, and (2) Activation-aware Grid Alignment (AGA), which further refines the ternary grid to better match full-precision outputs. In addition, we propose a plug-and-play Structural Similarity-based Reordering (SSR) strategy that leverages inter-column structural similarity to ease quantization and mitigate outlier effects, further enhancing overall performance. Extensive experiments demonstrate that PT$^2$-LLM delivers competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with lower memory cost, while also accelerating both prefill and decoding to achieve end-to-end speedup. The code and models will be available at https://github.com/XIANGLONGYAN/PT2-LLM.", "link": "https://arxiv.org/abs/2510.03267", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428879Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03268v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment", "summary": "arXiv:2510.03268v1 Announce Type: new Abstract: Multimodal contrastive learning (MCL) aims to embed data from different modalities in a shared embedding space. However, empirical evidence shows that representations from different modalities occupy completely separate regions of embedding space, a phenomenon referred to as the modality gap. Moreover, experimental findings on how the size of the modality gap influences downstream performance are inconsistent. These observations raise two key questions: (1) What causes the modality gap? (2) How does it affect downstream tasks? To address these questions, this paper introduces the first theoretical framework for analyzing the convergent optimal representations of MCL and the modality alignment when training is optimized. Specifically, we prove that without any constraint or under the cone constraint, the modality gap converges to zero. Under the subspace constraint (i.e., representations of two modalities fall into two distinct hyperplanes due to dimension collapse), the modality gap converges to the smallest angle between the two hyperplanes. This result identifies \\emph{dimension collapse} as the fundamental origin of the modality gap. Furthermore, our theorems demonstrate that paired samples cannot be perfectly aligned under the subspace constraint. The modality gap influences downstream performance by affecting the alignment between sample pairs. We prove that, in this case, perfect alignment between two modalities can still be achieved via two ways: hyperplane rotation and shared space projection.", "link": "https://arxiv.org/abs/2510.03268", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428938Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03269v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "General Exploratory Bonus for Optimistic Exploration in RLHF", "summary": "arXiv:2510.03269v1 Announce Type: new Abstract: Optimistic exploration is central to improving sample efficiency in reinforcement learning with human feedback, yet existing exploratory bonus methods to incentivize exploration often fail to realize optimism. We provide a theoretical analysis showing that current formulations, under KL or $\\alpha$-divergence regularization, unintentionally bias exploration toward high-probability regions of the reference model, thereby reinforcing conservative behavior instead of promoting discovery of uncertain regions. To address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle. GEB counteracts divergence-induced bias via reference-dependent reward regulation and unifies prior heuristic bonuses as special cases, while extending naturally across the full $\\alpha$-divergence family. Empirically, GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones. These results demonstrate that GEB offers both a principled and practical solution for optimistic exploration in RLHF.", "link": "https://arxiv.org/abs/2510.03269", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.428977Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03270v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "CoDA: Coding LM via Diffusion Adaptation", "summary": "arXiv:2510.03270v1 Announce Type: new Abstract: Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.", "link": "https://arxiv.org/abs/2510.03270", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429005Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03271v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary", "summary": "arXiv:2510.03271v1 Announce Type: new Abstract: Decision boundary, the subspace of inputs where a machine learning model assigns equal classification probabilities to two classes, is pivotal in revealing core model properties and interpreting behaviors. While analyzing the decision boundary of large language models (LLMs) has raised increasing attention recently, constructing it for mainstream LLMs remains computationally infeasible due to the enormous vocabulary-sequence sizes and the auto-regressive nature of LLMs. To address this issue, in this paper we propose Decision Potential Surface (DPS), a new notion for analyzing LLM decision boundary. DPS is defined on the confidences in distinguishing different sampling sequences for each input, which naturally captures the potential of decision boundary. We prove that the zero-height isohypse in DPS is equivalent to the decision boundary of an LLM, with enclosed regions representing decision regions. By leveraging DPS, for the first time in the literature, we propose an approximate decision boundary construction algorithm, namely $K$-DPS, which only requires K-finite times of sequence sampling to approximate an LLM's decision boundary with negligible error. We theoretically derive the upper bounds for the absolute error, expected error, and the error concentration between K-DPS and the ideal DPS, demonstrating that such errors can be trade-off with sampling times. Our results are empirically validated by extensive experiments across various LLMs and corpora.", "link": "https://arxiv.org/abs/2510.03271", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429056Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03272v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling", "summary": "arXiv:2510.03272v1 Announce Type: new Abstract: The Transformer architecture has revolutionized artificial intelligence, yet a principled theoretical understanding of its internal mechanisms remains elusive. This paper introduces a novel analytical framework that reconceptualizes the Transformer's discrete, layered structure as a continuous spatiotemporal dynamical system governed by a master Partial Differential Equation (PDE). Within this paradigm, we map core architectural components to distinct mathematical operators: self-attention as a non-local interaction, the feed-forward network as a local reaction, and, critically, residual connections and layer normalization as indispensable stabilization mechanisms. We do not propose a new model, but rather employ the PDE system as a theoretical probe to analyze the mathematical necessity of these components. By comparing a standard Transformer with a PDE simulator that lacks explicit stabilizers, our experiments provide compelling empirical evidence for our central thesis. We demonstrate that without residual connections, the system suffers from catastrophic representational drift, while the absence of layer normalization leads to unstable, explosive training dynamics. Our findings reveal that these seemingly heuristic \"tricks\" are, in fact, fundamental mathematical stabilizers required to tame an otherwise powerful but inherently unstable continuous system. This work offers a first-principles explanation for the Transformer's design and establishes a new paradigm for analyzing deep neural networks through the lens of continuous dynamics.", "link": "https://arxiv.org/abs/2510.03272", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429102Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03273v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Learning without Global Backpropagation via Synergistic Information Distillation", "summary": "arXiv:2510.03273v1 Announce Type: new Abstract: Backpropagation (BP), while foundational to deep learning, imposes two critical scalability bottlenecks: update locking, where network modules remain idle until the entire backward pass completes, and high memory consumption due to storing activations for gradient computation. To address these limitations, we introduce Synergistic Information Distillation (SID), a novel training framework that reframes deep learning as a cascade of local cooperative refinement problems. In SID, a deep network is structured as a pipeline of modules, each imposed with a local objective to refine a probabilistic belief about the ground-truth target. This objective balances fidelity to the target with consistency to the belief from its preceding module. By decoupling the backward dependencies between modules, SID enables parallel training and hence eliminates update locking and drastically reduces memory requirements. Meanwhile, this design preserves the standard feed-forward inference pass, making SID a versatile drop-in replacement for BP. We provide a theoretical foundation, proving that SID guarantees monotonic performance improvement with network depth. Empirically, SID consistently matches or surpasses the classification accuracy of BP, exhibiting superior scalability and pronounced robustness to label noise.Code is available at: https://github.com/ychAlbert/sid-bp", "link": "https://arxiv.org/abs/2510.03273", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429142Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03274v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models", "summary": "arXiv:2510.03274v1 Announce Type: new Abstract: Diffusion large language models (dLLMs), which offer bidirectional context and flexible masked-denoising generation, are emerging as a compelling alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model sizes continue to grow, motivating weight compression for deployment. Although post-training quantization (PTQ) is effective for AR LLMs, directly transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the fully visible signals assumed by standard PTQ methods, we introduce Masked Calibration Simulation (MCS) to align calibration with the timestep-dependent masking, which yields more reliable calibrations. Moreover, we propose a Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight representations via an optimization algorithm. It performs iterative approximation guided by our simulated calibration data. In addition, under a strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a sensitivity-based precision allocation scheme that adaptively assigns bit width across channel groups. When restricted to 2-bit precision, Quant-dLLM consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer PTQ methods on dLLMs. The code and models will be available at: https://github.com/ZTA2785/Quant-dLLM.", "link": "https://arxiv.org/abs/2510.03274", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429192Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03275v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size", "summary": "arXiv:2510.03275v1 Announce Type: new Abstract: Large language models (LLMs) face significant computational and memory challenges, making extremely low-bit quantization crucial for their efficient deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size, a novel framework that enables extremely low-bit quantization of LLMs while preserving their linguistic reasoning capabilities. A distinctive feature of SDQ-LLM is the continuous adjustability of the Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal trade-off between model size and accuracy. SDQ-LLM uses upsampling combined with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding high-precision parameters into 1-bit or 1.58-bit representations, replacing the multiplication operations within linear layers with addition. This approach significantly enhances inference efficiency under extremely low-bit quantization. To further reduce the loss of quantization precision, we incorporate Hadamard-based weight smoothing prior to quantization, improving the stability and robustness of the weight representations. Furthermore, to fully leverage the continuity of the OSR and reduce precision loss, recognizing the correlation between quantization sensitivity and weight variance, we propose a fine-grained, layer- and linear-wise OSR allocation strategy, MultiOSR. This strategy distributes OSR both across layers and within each layer, based on weight variance and parameter scale. Finally, extensive experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a more efficient and high-precision performance even under highly aggressive low-OSR settings. Our code is available at https://github.com/Dreamlittlecat/LLM-Quant-Factory.", "link": "https://arxiv.org/abs/2510.03275", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429244Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03276v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks", "summary": "arXiv:2510.03276v1 Announce Type: new Abstract: The combination of linear transformations and non-linear activation functions forms the foundation of most modern deep neural networks, enabling them to approximate highly complex functions. This paper explores the introduction of quadratic transformations to further increase nonlinearity in neural networks, with the aim of enhancing the performance of existing architectures. To reduce parameter complexity and computational complexity, we propose a lightweight quadratic enhancer that uses low-rankness, weight sharing, and sparsification techniques. For a fixed architecture, the proposed approach introduces quadratic interactions between features at every layer, while only adding negligible amounts of additional model parameters and forward computations. We conduct a set of proof-of-concept experiments for the proposed method across three tasks: image classification, text classification, and fine-tuning large-language models. In all tasks, the proposed approach demonstrates clear and substantial performance gains.", "link": "https://arxiv.org/abs/2510.03276", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429289Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03278v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition", "summary": "arXiv:2510.03278v1 Announce Type: new Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing equations to solve differential equations under uncertainty. However, interpreting uncertainty and overconfidence in B-PINNs requires care due to the poorly understood effects the physical constraints have on the network; overconfidence could reflect warranted precision, enforced by the constraints, rather than miscalibration. Motivated by the need to further clarify how individual physical constraints shape these networks, we introduce a scalable, matrix-free Laplace framework that decomposes the posterior Hessian into contributions from each constraint and provides metrics to quantify their relative influence on the loss landscape. Applied to the Van der Pol equation, our method tracks how constraints sculpt the network's geometry and shows, directly through the Hessian, how changing a single loss weight non-trivially redistributes curvature and effective dominance across the others.", "link": "https://arxiv.org/abs/2510.03278", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429322Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03279v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "MemMamba: Rethinking Memory Patterns in State Space Model", "summary": "arXiv:2510.03279v1 Announce Type: new Abstract: With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling.", "link": "https://arxiv.org/abs/2510.03279", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429378Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03282v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework", "summary": "arXiv:2510.03282v1 Announce Type: new Abstract: Interpreting language models often involves circuit analysis, which aims to identify sparse subnetworks, or circuits, that accomplish specific tasks. Existing circuit discovery algorithms face a fundamental trade-off: attribution patching is fast but unfaithful to the full model, while edge pruning is faithful but computationally expensive. This research proposes a hybrid attribution and pruning (HAP) framework that uses attribution patching to identify a high-potential subgraph, then applies edge pruning to extract a faithful circuit from it. We show that HAP is 46\\% faster than baseline algorithms without sacrificing circuit faithfulness. Furthermore, we present a case study on the Indirect Object Identification task, showing that our method preserves cooperative circuit components (e.g. S-inhibition heads) that attribution patching methods prune at high sparsity. Our results show that HAP could be an effective approach for improving the scalability of mechanistic interpretability research to larger models. Our code is available at https://anonymous.4open.science/r/HAP-circuit-discovery.", "link": "https://arxiv.org/abs/2510.03282", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429439Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03283v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment", "summary": "arXiv:2510.03283v1 Announce Type: new Abstract: Large language models (LLMs) deployed on edge servers are increasingly used in latency-sensitive applications such as personalized assistants, recommendation, and content moderation. However, the non-stationary nature of user data necessitates frequent retraining, which introduces a fundamental tension between inference latency and model accuracy under constrained GPU resources. Existing retraining strategies either delay model updates, over-commit resources to retraining, or overlook iteration-level retraining granularity. In this paper, we identify that iteration-level scheduling is crucial for adapting retraining frequency to model drift without violating service-level objectives (SLOs). We propose MACE, a hybrid LLM system that colocates concurrent inference (prefill, decode) and fine-tuning, with intelligent memory management to maximize task performance while promising inference throughput. MACE leverages the insight that not all model updates equally affect output alignment and allocates GPU cycles accordingly to balance throughput, latency, and update freshness. Our trace-driven evaluation shows that MACE matches or exceeds continuous retraining while reducing inference latency by up to 63% and maintaining throughput under resource constraints. Compared to periodic retraining, MACE improves latency breakdown across prefill, decode, and finetune stages, and sustains GPU utilization above 85% in NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid scheduling is a promising direction for deploying LLMs with continual learning capabilities on edge platforms.", "link": "https://arxiv.org/abs/2510.03283", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429491Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03284v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments", "summary": "arXiv:2510.03284v1 Announce Type: new Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a scalable framework for Federated Instruction Tuning (FIT) of Large Language Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT framework combines federated learning with 4-bit Quantized Low-Rank Adaptation (QLORA), mitigating the core issues of communication and computational overhead. We demonstrate this by filtering the general-purpose Databricks Dolly 15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable framework for decentralized LLM deployment on home compute gateways.", "link": "https://arxiv.org/abs/2510.03284", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429531Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03288v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain", "summary": "arXiv:2510.03288v1 Announce Type: new Abstract: Log-based anomaly detection is a essential task for ensuring the reliability and performance of software systems. However, the performance of existing anomaly detection methods heavily relies on labeling, while labeling a large volume of logs is highly challenging. To address this issue, many approaches based on transfer learning and active learning have been proposed. Nevertheless, their effectiveness is hindered by issues such as the gap between source and target system data distributions and cold-start problems. In this paper, we propose LogAction, a novel log-based anomaly detection model based on active domain adaptation. LogAction integrates transfer learning and active learning techniques. On one hand, it uses labeled data from a mature system to train a base model, mitigating the cold-start issue in active learning. On the other hand, LogAction utilize free energy-based sampling and uncertainty-based sampling to select logs located at the distribution boundaries for manual labeling, thus addresses the data distribution gap in transfer learning with minimal human labeling efforts. Experimental results on six different combinations of datasets demonstrate that LogAction achieves an average 93.01% F1 score with only 2% of manual labels, outperforming some state-of-the-art methods by 26.28%. Website: https://logaction.github.io", "link": "https://arxiv.org/abs/2510.03288", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429572Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03289v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Why mask diffusion does not work", "summary": "arXiv:2510.03289v1 Announce Type: new Abstract: The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion.", "link": "https://arxiv.org/abs/2510.03289", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429603Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03290v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Single-Core Superscalar Optimization of Clifford Neural Layers", "summary": "arXiv:2510.03290v1 Announce Type: new Abstract: Within the growing interest in the physical sciences in developing networks with equivariance properties, Clifford neural layers shine as one approach that delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this paper, we analyze the inner structure of the computation within Clifford convolutional layers and propose and implement several optimizations to speed up the inference process while maintaining correctness. In particular, we begin by analyzing the theoretical foundations of Clifford algebras to eliminate redundant matrix allocations and computations, then systematically apply established optimization techniques to enhance performance further. We report a final average speedup of 21.35x over the baseline implementation of eleven functions and runtimes comparable to and faster than the original PyTorch implementation in six cases. In the remaining cases, we achieve performance in the same order of magnitude as the original library.", "link": "https://arxiv.org/abs/2510.03290", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429638Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03291v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs", "summary": "arXiv:2510.03291v1 Announce Type: new Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning.", "link": "https://arxiv.org/abs/2510.03291", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429694Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03293v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing", "summary": "arXiv:2510.03293v1 Announce Type: new Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each token to a subset of experts through a learned gate function. While conditional routing reduces training costs, it shifts the burden on inference memory: expert parameters and activations consume memory, limiting the number of experts per device. As tokens are routed, some experts become overloaded while others are underutilized. Because experts are mapped to GPUs, this imbalance translates directly into degraded system performance in terms of latency, throughput, and cost. We present LASER, a plug-and-play, inference-time routing algorithm that balances load while preserving accuracy. LASER adapts to the shape of the gate's score distribution. When scores provide a clear preference, it routes to the strongest experts; when scores are more uniform, it broadens the set of viable experts and routes to the least-loaded among them. Because LASER relies only on gate scores from a trained model, it integrates directly into existing MoE inference pipelines without retraining or finetuning. We evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets (ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing, translating into lower latency and higher throughput, while keeping the accuracy changes negligible.", "link": "https://arxiv.org/abs/2510.03293", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429742Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03298v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models", "summary": "arXiv:2510.03298v1 Announce Type: new Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual Optimization (CAFL-L), a principled extension of FedAvg that explicitly incorporates device-level resource constraints including energy, communication, memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to dynamically adapt training hyperparameters -- freezing depth, local steps, batch size, and communication compression -- while preserving training stability through token-budget preservation via gradient accumulation. Experiments on a character-level language model demonstrate that CAFL-L achieves superior constraint satisfaction compared to standard FedAvg (reducing memory usage by 20% and communication by 95%) while maintaining competitive validation performance, making it practical for deployment on resource-constrained edge devices.", "link": "https://arxiv.org/abs/2510.03298", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429773Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03301v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles", "summary": "arXiv:2510.03301v1 Announce Type: new Abstract: This paper introduces a novel adaptive ensemble framework that synergistically combines XGBoost and neural networks through sophisticated meta-learning. The proposed method leverages advanced uncertainty quantification techniques and feature importance integration to dynamically orchestrate model selection and combination. Experimental results demonstrate superior predictive performance and enhanced interpretability across diverse datasets, contributing to the development of more intelligent and flexible machine learning systems.", "link": "https://arxiv.org/abs/2510.03301", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429803Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03302v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models", "summary": "arXiv:2510.03302v1 Announce Type: new Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models to prevent inappropriate content generation for safety and copyright considerations. However, as models evolve to next-generation architectures like Flux, established erasure methods (\\textit{e.g.}, ESD, UCE, AC) exhibit degraded effectiveness, raising questions about their true mechanisms. Through systematic analysis, we reveal that concept erasure creates only an illusion of ``amnesia\": rather than genuine forgetting, these methods bias sampling trajectories away from target concepts, making the erasure fundamentally reversible. This insight motivates the need to distinguish superficial safety from genuine concept removal. In this work, we propose \\textbf{RevAm} (\\underline{Rev}oking \\underline{Am}nesia), an RL-based trajectory optimization framework that resurrects erased concepts by dynamically steering the denoising process without modifying model weights. By adapting Group Relative Policy Optimization (GRPO) to diffusion models, RevAm explores diverse recovery trajectories through trajectory-level rewards, overcoming local optima that limit existing methods. Extensive experiments demonstrate that RevAm achieves superior concept resurrection fidelity while reducing computational time by 10$\\times$, exposing critical vulnerabilities in current safety mechanisms and underscoring the need for more robust erasure techniques beyond trajectory manipulation.", "link": "https://arxiv.org/abs/2510.03302", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429844Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03305v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies", "summary": "arXiv:2510.03305v1 Announce Type: new Abstract: Machine learning has been increasingly applied in climate modeling on system emulation acceleration, data-driven parameter inference, forecasting, and knowledge discovery, addressing challenges such as physical consistency, multi-scale coupling, data sparsity, robust generalization, and integration with scientific workflows. This paper analyzes a series of case studies from applied machine learning research in climate modeling, with a focus on design choices and workflow structure. Rather than reviewing technical details, we aim to synthesize workflow design patterns across diverse projects in ML-enabled climate modeling: from surrogate modeling, ML parameterization, probabilistic programming, to simulation-based inference, and physics-informed transfer learning. We unpack how these workflows are grounded in physical knowledge, informed by simulation data, and designed to integrate observations. We aim to offer a framework for ensuring rigor in scientific machine learning through more transparent model development, critical evaluation, informed adaptation, and reproducibility, and to contribute to lowering the barrier for interdisciplinary collaboration at the interface of data science and climate modeling.", "link": "https://arxiv.org/abs/2510.03305", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429885Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03309v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval", "summary": "arXiv:2510.03309v1 Announce Type: new Abstract: Multimodal foundation models hold promise for drug discovery and biomedical applications, but most existing approaches rely on heavy pretraining or large scale multimodal corpora. We investigate whether thin contrastive bridges, lightweight projection heads over frozen unimodal encoders can align chemical and textual representations without training a full multimodal model. Using paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with biomedical sentence embeddings through dual linear projections trained with a contrastive objective. To better handle drugs sharing the same therapeutic target, we incorporate hard negative weighting and a margin loss. Evaluation under scaffold based splits, which require generalization across disjoint chemical cores, demonstrates that our approach achieves non-trivial cross modal alignment and substantially improves within target discrimination compared to frozen baselines. These results suggest that thin bridges offer a compute efficient alternative to large scale multimodal pretraining, enabling scaffold aware drug text alignment and target specific retrieval in precision medicine.", "link": "https://arxiv.org/abs/2510.03309", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429931Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03310v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management", "summary": "arXiv:2510.03310v1 Announce Type: new Abstract: LLMs are emerging tools for simulating human behavior in business, economics, and social science, offering a lower-cost complement to laboratory experiments, field studies, and surveys. This paper evaluates how well LLMs replicate human behavior in operations management. Using nine published experiments in behavioral operations, we assess two criteria: replication of hypothesis-test outcomes and distributional alignment via Wasserstein distance. LLMs reproduce most hypothesis-level effects, capturing key decision biases, but their response distributions diverge from human data, including for strong commercial models. We also test two lightweight interventions -- chain-of-thought prompting and hyperparameter tuning -- which reduce misalignment and can sometimes let smaller or open-source models match or surpass larger systems.", "link": "https://arxiv.org/abs/2510.03310", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.429964Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.03313v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining", "summary": "arXiv:2510.03313v1 Announce Type: new Abstract: Scaling laws for language model training traditionally characterize how performance scales with model size and dataset volume. Prior work has explored architecture variants and data treatments such as dataset filtering and noise injection in language model pretraining; however, these studies have not formalized data quality within a principled scaling law. We introduce a dimensionless data-quality parameter Q, and propose a quality-aware scaling law extending the Chinchilla framework to predict loss as a joint function of model size, data volume, and data quality. The law is motivated by an effective-sample-size and information-theoretic view of noisy or redundant corpora, and it admits two practical estimators for Q: (i) a corruption rate proxy and (ii) a deficiency measure. Through synthetic experiments in neural machine translation and autoregressive modeling -- where we systematically control data quality via multiple levels of noise injection and coverage variation -- we show that loss scales predictably with data quality and that higher-quality data can substantially reduce model size and hence compute requirements. Our results demonstrate a sublinear decay of effective data with quality and robustness to moderate data corruption; out-of-sample evaluations further validate the predictive form of the law. Unlike prior empirical analyses, our work establishes an explicit, generalizable law for data quality, offering concrete guidance for balancing data curation effort and model scale in large-scale pretraining.", "link": "https://arxiv.org/abs/2510.03313", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-08T01:36:15.430012Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03277v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Quantile-Scaled Bayesian Optimization Using Rank-Only Feedback", "summary": "arXiv:2510.03277v1 Announce Type: new Abstract: Bayesian Optimization (BO) is widely used for optimizing expensive black-box functions, particularly in hyperparameter tuning. However, standard BO assumes access to precise objective values, which may be unavailable, noisy, or unreliable in real-world settings where only relative or rank-based feedback can be obtained. In this study, we propose Quantile-Scaled Bayesian Optimization (QS-BO), a principled rank-based optimization framework. QS-BO converts ranks into heteroscedastic Gaussian targets through a quantile-scaling pipeline, enabling the use of Gaussian process surrogates and standard acquisition functions without requiring explicit metric scores. We evaluate QS-BO on synthetic benchmark functions, including one- and two-dimensional nonlinear functions and the Branin function, and compare its performance against Random Search. Results demonstrate that QS-BO consistently achieves lower objective values and exhibits greater stability across runs. Statistical tests further confirm that QS-BO significantly outperforms Random Search at the 1\\% significance level. These findings establish QS-BO as a practical and effective extension of Bayesian Optimization for rank-only feedback, with promising applications in preference learning, recommendation, and human-in-the-loop optimization where absolute metric values are unavailable or unreliable.", "link": "https://arxiv.org/abs/2510.03277", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.669632Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03281v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Mathematically rigorous proofs for Shapley explanations", "summary": "arXiv:2510.03281v1 Announce Type: new Abstract: Machine Learning is becoming increasingly more important in today's world. It is therefore very important to provide understanding of the decision-making process of machine-learning models. A popular way to do this is by looking at the Shapley-Values of these models as introduced by Lundberg and Lee. In this thesis, we discuss the two main results by Lundberg and Lee from a mathematically rigorous standpoint and provide full proofs, which are not available from the original material. The first result of this thesis is an axiomatic characterization of the Shapley values in machine learning based on axioms by Young. We show that the Shapley values are the unique explanation to satisfy local accuracy, missingness, symmetry and consistency. Lundberg and Lee claim that the symmetry axiom is redundant for explanations. However, we provide a counterexample that shows the symmetry axiom is in fact essential. The second result shows that we can write the Shapley values as the unique solution to a weighted linear regression problem. This result is proven with the use of dimensionality reduction.", "link": "https://arxiv.org/abs/2510.03281", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.669676Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03624v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Transformed $\\ell_1$ Regularizations for Robust Principal Component Analysis: Toward a Fine-Grained Understanding", "summary": "arXiv:2510.03624v1 Announce Type: new Abstract: Robust Principal Component Analysis (RPCA) aims to recover a low-rank structure from noisy, partially observed data that is also corrupted by sparse, potentially large-magnitude outliers. Traditional RPCA models rely on convex relaxations, such as nuclear norm and $\\ell_1$ norm, to approximate the rank of a matrix and the $\\ell_0$ functional (the number of non-zero elements) of another. In this work, we advocate a nonconvex regularization method, referred to as transformed $\\ell_1$ (TL1), to improve both approximations. The rationale is that by varying the internal parameter of TL1, its behavior asymptotically approaches either $\\ell_0$ or $\\ell_1$. Since the rank is equal to the number of non-zero singular values and the nuclear norm is defined as their sum, applying TL1 to the singular values can approximate either the rank or the nuclear norm, depending on its internal parameter. We conduct a fine-grained theoretical analysis of statistical convergence rates, measured in the Frobenius norm, for both the low-rank and sparse components under general sampling schemes. These rates are comparable to those of the classical RPCA model based on the nuclear norm and $\\ell_1$ norm. Moreover, we establish constant-order upper bounds on the estimated rank of the low-rank component and the cardinality of the sparse component in the regime where TL1 behaves like $\\ell_0$, assuming that the respective matrices are exactly low-rank and exactly sparse. Extensive numerical experiments on synthetic data and real-world applications demonstrate that the proposed approach achieves higher accuracy than the classic convex model, especially under non-uniform sampling schemes.", "link": "https://arxiv.org/abs/2510.03624", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.669731Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03685v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "The analogy theorem in Hoare logic", "summary": "arXiv:2510.03685v1 Announce Type: new Abstract: The introduction of machine learning methods has led to significant advances in automation, optimization, and discoveries in various fields of science and technology. However, their widespread application faces a fundamental limitation: the transfer of models between data domains generally lacks a rigorous mathematical justification. The key problem is the lack of formal criteria to guarantee that a model trained on one type of data will retain its properties on another.This paper proposes a solution to this problem by formalizing the concept of analogy between data sets and models using first-order logic and Hoare logic.We formulate and rigorously prove a theorem that sets out the necessary and sufficient conditions for analogy in the task of knowledge transfer between machine learning models. Practical verification of the analogy theorem on model data obtained using the Monte Carlo method, as well as on MNIST and USPS data, allows us to achieving F1 scores of 0.84 and 0.88 for convolutional neural networks and random forests, respectively.The proposed approach not only allows us to justify the correctness of transfer between domains but also provides tools for comparing the applicability of models to different types of data.The main contribution of the work is a rigorous formalization of analogy at the level of program logic, providing verifiable guarantees of the correctness of knowledge transfer, which opens new opportunities for both theoretical research and the practical use of machine learning models in previously inaccessible areas.", "link": "https://arxiv.org/abs/2510.03685", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.669778Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03809v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Spectral Thresholds for Identifiability and Stability:Finite-Sample Phase Transitions in High-Dimensional Learning", "summary": "arXiv:2510.03809v1 Announce Type: new Abstract: In high-dimensional learning, models remain stable until they collapse abruptly once the sample size falls below a critical level. This instability is not algorithm-specific but a geometric mechanism: when the weakest Fisher eigendirection falls beneath sample-level fluctuations, identifiability fails. Our Fisher Threshold Theorem formalizes this by proving that stability requires the minimal Fisher eigenvalue to exceed an explicit $O(\\sqrt{d/n})$ bound. Unlike prior asymptotic or model-specific criteria, this threshold is finite-sample and necessary, marking a sharp phase transition between reliable concentration and inevitable failure. To make the principle constructive, we introduce the Fisher floor, a verifiable spectral regularization robust to smoothing and preconditioning. Synthetic experiments on Gaussian mixtures and logistic models confirm the predicted transition, consistent with $d/n$ scaling. Statistically, the threshold sharpens classical eigenvalue conditions into a non-asymptotic law; learning-theoretically, it defines a spectral sample-complexity frontier, bridging theory with diagnostics for robust high-dimensional inference.", "link": "https://arxiv.org/abs/2510.03809", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.669814Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03929v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Self-Speculative Masked Diffusions", "summary": "arXiv:2510.03929v1 Announce Type: new Abstract: We present self-speculative masked diffusions, a new class of masked diffusion generative models for discrete data that require significantly fewer function evaluations to generate samples. Standard masked diffusion models predict factorized logits over currently masked positions. A number of masked positions are then sampled, however, the factorization approximation means that sampling too many positions in one go leads to poor sample quality. As a result, many simulation steps and therefore neural network function evaluations are required to generate high-quality data. We reduce the computational burden by generating non-factorized predictions over masked positions. This is achieved by modifying the final transformer attention mask from non-causal to causal, enabling draft token generation and parallel validation via a novel, model-integrated speculative sampling mechanism. This results in a non-factorized predictive distribution over masked positions in a single forward pass. We apply our method to GPT2 scale text modelling and protein sequences generation, finding that we can achieve a ~2x reduction in the required number of network forward passes relative to standard masked diffusion models.", "link": "https://arxiv.org/abs/2510.03929", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.669859Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04042v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Simulation-based inference via telescoping ratio estimation for trawl processes", "summary": "arXiv:2510.04042v1 Announce Type: new Abstract: The growing availability of large and complex datasets has increased interest in temporal stochastic processes that can capture stylized facts such as marginal skewness, non-Gaussian tails, long memory, and even non-Markovian dynamics. While such models are often easy to simulate from, parameter estimation remains challenging. Simulation-based inference (SBI) offers a promising way forward, but existing methods typically require large training datasets or complex architectures and frequently yield confidence (credible) regions that fail to attain their nominal values, raising doubts on the reliability of estimates for the very features that motivate the use of these models. To address these challenges, we propose a fast and accurate, sample-efficient SBI framework for amortized posterior inference applicable to intractable stochastic processes. The proposed approach relies on two main steps: first, we learn the posterior density by decomposing it sequentially across parameter dimensions. Then, we use Chebyshev polynomial approximations to efficiently generate independent posterior samples, enabling accurate inference even when Markov chain Monte Carlo methods mix poorly. We further develop novel diagnostic tools for SBI in this context, as well as post-hoc calibration techniques; the latter not only lead to performance improvements of the learned inferential tool, but also to the ability to reuse it directly with new time series of varying lengths, thus amortizing the training cost. We demonstrate the method's effectiveness on trawl processes, a class of flexible infinitely divisible models that generalize univariate Gaussian processes, applied to energy demand data.", "link": "https://arxiv.org/abs/2510.04042", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.669904Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04276v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Scalable Causal Discovery from Recursive Nonlinear Data via Truncated Basis Function Scores and Tests", "summary": "arXiv:2510.04276v1 Announce Type: new Abstract: Learning graphical conditional independence structures from nonlinear, continuous or mixed data is a central challenge in machine learning and the sciences, and many existing methods struggle to scale to thousands of samples or hundreds of variables. We introduce two basis-expansion tools for scalable causal discovery. First, the Basis Function BIC (BF-BIC) score uses truncated additive expansions to approximate nonlinear dependencies. BF-BIC is theoretically consistent under additive models and extends to post-nonlinear (PNL) models via an invertible reparameterization. It remains robust under moderate interactions and supports mixed data through a degenerate-Gaussian embedding for discrete variables. In simulations with fully nonlinear neural causal models (NCMs), BF-BIC outperforms kernel- and constraint-based methods (e.g., KCI, RFCI) in both accuracy and runtime. Second, the Basis Function Likelihood Ratio Test (BF-LRT) provides an approximate conditional independence test that is substantially faster than kernel tests while retaining competitive accuracy. Extensive simulations and a real-data application to Canadian wildfire risk show that, when integrated into hybrid searches, BF-based methods enable interpretable and scalable causal discovery. Implementations are available in Python, R, and Java.", "link": "https://arxiv.org/abs/2510.04276", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.669943Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04277v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Relative Information Gain and Gaussian Process Regression", "summary": "arXiv:2510.04277v1 Announce Type: new Abstract: The sample complexity of estimating or maximising an unknown function in a reproducing kernel Hilbert space is known to be linked to both the effective dimension and the information gain associated with the kernel. While the information gain has an attractive information-theoretic interpretation, the effective dimension typically results in better rates. We introduce a new quantity called the relative information gain, which measures the sensitivity of the information gain with respect to the observation noise. We show that the relative information gain smoothly interpolates between the effective dimension and the information gain, and that the relative information gain has the same growth rate as the effective dimension. In the second half of the paper, we prove a new PAC-Bayesian excess risk bound for Gaussian process regression. The relative information gain arises naturally from the complexity term in this PAC-Bayesian bound. We prove bounds on the relative information gain that depend on the spectral properties of the kernel. When these upper bounds are combined with our excess risk bound, we obtain minimax-optimal rates of convergence.", "link": "https://arxiv.org/abs/2510.04277", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.669978Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04406v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Modular and Adaptive Conformal Prediction for Sequential Models via Residual Decomposition", "summary": "arXiv:2510.04406v1 Announce Type: new Abstract: Conformal prediction offers finite-sample coverage guarantees under minimal assumptions. However, existing methods treat the entire modeling process as a black box, overlooking opportunities to exploit modular structure. We introduce a conformal prediction framework for two-stage sequential models, where an upstream predictor generates intermediate representations for a downstream model. By decomposing the overall prediction residual into stage-specific components, our method enables practitioners to attribute uncertainty to specific pipeline stages. We develop a risk-controlled parameter selection procedure using family-wise error rate (FWER) control to calibrate stage-wise scaling parameters, and propose an adaptive extension for non-stationary settings that preserves long-run coverage guarantees. Experiments on synthetic distribution shifts, as well as real-world supply chain and stock market data, demonstrate that our approach maintains coverage under conditions that degrade standard conformal methods, while providing interpretable stage-wise uncertainty attribution. This framework offers diagnostic advantages and robust coverage that standard conformal methods lack.", "link": "https://arxiv.org/abs/2510.04406", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670057Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04426v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Divergence Phase Index: A Riesz-Transform Framework for Multidimensional Phase Difference Analysis", "summary": "arXiv:2510.04426v1 Announce Type: new Abstract: We introduce the Divergence Phase Index (DPI), a novel framework for quantifying phase differences in one and multidimensional signals, grounded in harmonic analysis via the Riesz transform. Based on classical Hilbert Transform phase measures, the DPI extends these principles to higher dimensions, offering a geometry-aware metric that is invariant to intensity scaling and sensitive to structural changes. We applied this method on both synthetic and real-world datasets, including intracranial EEG (iEEG) recordings during epileptic seizures, high-resolution microscopy images, and paintings. In the 1D case, the DPI robustly detects hypersynchronization associated with generalized epilepsy, while in 2D, it reveals subtle, imperceptible changes in images and artworks. Additionally, it can detect rotational variations in highly isotropic microscopy images. The DPI's robustness to amplitude variations and its adaptability across domains enable its use in diverse applications from nonlinear dynamics, complex systems analysis, to multidimensional signal processing.", "link": "https://arxiv.org/abs/2510.04426", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670140Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04556v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Gini-based Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing", "summary": "arXiv:2510.04556v1 Announce Type: new Abstract: In a dynamic landscape where portfolios and environments evolve, maintaining the accuracy of pricing models is critical. To the best of our knowledge, this is the first study to systematically examine concept drift in non-life insurance pricing. We (i) provide an overview of the relevant literature and commonly used methodologies, clarify the distinction between virtual drift and concept drift, and explain their implications for long-run model performance; (ii) review and formalize common performance measures, including the Gini index and deviance loss, and articulate their interpretation; (iii) derive the asymptotic distribution of the Gini index, enabling valid inference and hypothesis testing; and (iv) present a standardized monitoring procedure that indicates when refitting is warranted. We illustrate the framework using a modified real-world portfolio with induced concept drift and discuss practical considerations and pitfalls.", "link": "https://arxiv.org/abs/2510.04556", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670179Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04602v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Computing Wasserstein Barycenters through Gradient Flows", "summary": "arXiv:2510.04602v1 Announce Type: new Abstract: Wasserstein barycenters provide a powerful tool for aggregating probability measures, while leveraging the geometry of their ambient space. Existing discrete methods suffer from poor scalability, as they require access to the complete set of samples from input measures. We address this issue by recasting the original barycenter problem as a gradient flow in the Wasserstein space. Our approach offers two advantages. First, we achieve scalability by sampling mini-batches from the input measures. Second, we incorporate functionals over probability measures, which regularize the barycenter problem through internal, potential, and interaction energies. We present two algorithms for empirical and Gaussian mixture measures, providing convergence guarantees under the Polyak-{\\L}ojasiewicz inequality. Experimental validation on toy datasets and domain adaptation benchmarks show that our methods outperform previous discrete and neural net-based methods for computing Wasserstein barycenters.", "link": "https://arxiv.org/abs/2510.04602", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670216Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04762v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Fisher-Bingham-like normalizing flows on the sphere", "summary": "arXiv:2510.04762v1 Announce Type: new Abstract: A generic D-dimensional Gaussian can be conditioned or projected onto the D-1 unit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular Gaussian (AG) distribution families, respectively. These are some of the most fundamental distributions on the sphere, yet cannot straightforwardly be written as a normalizing flow except in two special cases: the von-Mises Fisher in D=3 and the central angular Gaussian in any D. In this paper, we describe how to generalize these special cases to a family of normalizing flows that behave similarly to the full FB or AG family in any D. We call them \"zoom-linear-project\" (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham distribution, their composition allows to gradually add complexity as needed. Furthermore, they can naturally handle conditional density estimation with target distributions that vary by orders of magnitude in scale - a setting that is important in astronomical applications but that existing flows often struggle with. A particularly useful member of the new family is the Kent analogue that can cheaply upgrade any flow in this situation to yield better performance.", "link": "https://arxiv.org/abs/2510.04762", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670252Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04780v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Kernel ridge regression under power-law data: spectrum and generalization", "summary": "arXiv:2510.04780v1 Announce Type: new Abstract: In this work, we investigate high-dimensional kernel ridge regression (KRR) on i.i.d. Gaussian data with anisotropic power-law covariance. This setting differs fundamentally from the classical source & capacity conditions for KRR, where power-law assumptions are typically imposed on the kernel eigen-spectrum itself. Our contributions are twofold. First, we derive an explicit characterization of the kernel spectrum for polynomial inner-product kernels, giving a precise description of how the kernel eigen-spectrum inherits the data decay. Second, we provide an asymptotic analysis of the excess risk in the high-dimensional regime for a particular kernel with this spectral behavior, showing that the sample complexity is governed by the effective dimension of the data rather than the ambient dimension. These results establish a fundamental advantage of learning with power-law anisotropic data over isotropic data. To our knowledge, this is the first rigorous treatment of non-linear KRR under power-law data.", "link": "https://arxiv.org/abs/2510.04780", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670286Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04811v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "A Noise Resilient Approach for Robust Hurst Exponent Estimation", "summary": "arXiv:2510.04811v1 Announce Type: new Abstract: Understanding signal behavior across scales is vital in areas such as natural phenomena analysis and financial modeling. A key property is self-similarity, quantified by the Hurst exponent (H), which reveals long-term dependencies. Wavelet-based methods are effective for estimating H due to their multi-scale analysis capability, but additive noise in real-world measurements often degrades accuracy. We propose Noise-Controlled ALPHEE (NC-ALPHEE), an enhancement of the Average Level-Pairwise Hurst Exponent Estimator (ALPHEE), incorporating noise mitigation and generating multiple level-pairwise estimates from signal energy pairs. A neural network (NN) combines these estimates, replacing traditional averaging. This adaptive learning maintains ALPHEE's behavior in noise-free cases while improving performance in noisy conditions. Extensive simulations show that in noise-free data, NC-ALPHEE matches ALPHEE's accuracy using both averaging and NN-based methods. Under noise, however, traditional averaging deteriorates and requires impractical level restrictions, while NC-ALPHEE consistently outperforms existing techniques without such constraints. NC-ALPHEE offers a robust, adaptive approach for H estimation, significantly enhancing the reliability of wavelet-based methods in noisy environments.", "link": "https://arxiv.org/abs/2510.04811", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670324Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04926v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Set to Be Fair: Demographic Parity Constraints for Set-Valued Classification", "summary": "arXiv:2510.04926v1 Announce Type: new Abstract: Set-valued classification is used in multiclass settings where confusion between classes can occur and lead to misleading predictions. However, its application may amplify discriminatory bias motivating the development of set-valued approaches under fairness constraints. In this paper, we address the problem of set-valued classification under demographic parity and expected size constraints. We propose two complementary strategies: an oracle-based method that minimizes classification risk while satisfying both constraints, and a computationally efficient proxy that prioritizes constraint satisfaction. For both strategies, we derive closed-form expressions for the (optimal) fair set-valued classifiers and use these to build plug-in, data-driven procedures for empirical predictions. We establish distribution-free convergence rates for violations of the size and fairness constraints for both methods, and under mild assumptions we also provide excess-risk bounds for the oracle-based approach. Empirical results demonstrate the effectiveness of both strategies and highlight the efficiency of our proxy method.", "link": "https://arxiv.org/abs/2510.04926", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670366Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04970v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning", "summary": "arXiv:2510.04970v1 Announce Type: new Abstract: We present FLOP (Fast Learning of Order and Parents), a score-based causal discovery algorithm for linear models. It pairs fast parent selection with iterative Cholesky-based score updates, cutting run-times over prior algorithms. This makes it feasible to fully embrace discrete search, enabling iterated local search with principled order initialization to find graphs with scores at or close to the global optimum. The resulting structures are highly accurate across benchmarks, with near-perfect recovery in standard settings. This performance calls for revisiting discrete search over graphs as a reasonable approach to causal discovery.", "link": "https://arxiv.org/abs/2510.04970", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670396Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05013v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Curiosity-Driven Co-Development of Action and Language in Robots Through Self-Exploration", "summary": "arXiv:2510.05013v1 Announce Type: new Abstract: Human infants acquire language and action co-developmentally, achieving remarkable generalization capabilities from only a minimal number of learning examples. In contrast, recent large language models require exposure to billions of training tokens to achieve such generalization. What mechanisms underlie such efficient developmental learning in humans? This study addresses this question through simulation experiments in which robots learn to perform various actions corresponding to imperative sentences (e.g., \\textit{push red cube}) via trials of self-guided exploration. Our approach integrates the active inference framework with reinforcement learning, enabling curiosity-driven developmental learning. The simulations yielded several nontrivial findings: i) Curiosity-driven exploration combined with motor noise substantially outperforms learning without curiosity. ii) Simpler, prerequisite-like actions emerge earlier in development, while more complex actions involving these prerequisites develop later. iii) Rote pairing of sentences and actions occurs before the emergence of compositional generalization. iv) Generalization is drastically improved as the number of compositional elements increases. These results shed light into possible mechanisms underlying efficient co-developmental learning in infants and provide computational parallels to findings in developmental psychology.", "link": "https://arxiv.org/abs/2510.05013", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670440Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05033v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Causal Abstractions, Categorically Unified", "summary": "arXiv:2510.05033v1 Announce Type: new Abstract: We present a categorical framework for relating causal models that represent the same system at different levels of abstraction. We define a causal abstraction as natural transformations between appropriate Markov functors, which concisely consolidate desirable properties a causal abstraction should exhibit. Our approach unifies and generalizes previously considered causal abstractions, and we obtain categorical proofs and generalizations of existing results on causal abstractions. Using string diagrammatical tools, we can explicitly describe the graphs that serve as consistent abstractions of a low-level graph under interventions. We discuss how methods from mechanistic interpretability, such as circuit analysis and sparse autoencoders, fit within our categorical framework. We also show how applying do-calculus on a high-level graphical abstraction of an acyclic-directed mixed graph (ADMG), when unobserved confounders are present, gives valid results on the low-level graph, thus generalizing an earlier statement by Anand et al. (2023). We argue that our framework is more suitable for modeling causal abstractions compared to existing categorical frameworks. Finally, we discuss how notions such as $\\tau$-consistency and constructive $\\tau$-abstractions can be recovered with our framework.", "link": "https://arxiv.org/abs/2510.05033", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670479Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03305v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies", "summary": "arXiv:2510.03305v1 Announce Type: cross Abstract: Machine learning has been increasingly applied in climate modeling on system emulation acceleration, data-driven parameter inference, forecasting, and knowledge discovery, addressing challenges such as physical consistency, multi-scale coupling, data sparsity, robust generalization, and integration with scientific workflows. This paper analyzes a series of case studies from applied machine learning research in climate modeling, with a focus on design choices and workflow structure. Rather than reviewing technical details, we aim to synthesize workflow design patterns across diverse projects in ML-enabled climate modeling: from surrogate modeling, ML parameterization, probabilistic programming, to simulation-based inference, and physics-informed transfer learning. We unpack how these workflows are grounded in physical knowledge, informed by simulation data, and designed to integrate observations. We aim to offer a framework for ensuring rigor in scientific machine learning through more transparent model development, critical evaluation, informed adaptation, and reproducibility, and to contribute to lowering the barrier for interdisciplinary collaboration at the interface of data science and climate modeling.", "link": "https://arxiv.org/abs/2510.03305", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670514Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03365v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Bias and Coverage Properties of the WENDy-IRLS Algorithm", "summary": "arXiv:2510.03365v1 Announce Type: cross Abstract: The Weak form Estimation of Nonlinear Dynamics (WENDy) method is a recently proposed class of parameter estimation algorithms that exhibits notable noise robustness and computational efficiency. This work examines the coverage and bias properties of the original WENDy-IRLS algorithm's parameter and state estimators in the context of the following differential equations: Logistic, Lotka-Volterra, FitzHugh-Nagumo, Hindmarsh-Rose, and a Protein Transduction Benchmark. The estimators' performance was studied in simulated data examples, under four different noise distributions (normal, log-normal, additive censored normal, and additive truncated normal), and a wide range of noise, reaching levels much higher than previously tested for this algorithm.", "link": "https://arxiv.org/abs/2510.03365", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670584Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03419v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Multi-task neural diffusion processes for uncertainty-quantified wind power prediction", "summary": "arXiv:2510.03419v1 Announce Type: cross Abstract: Uncertainty-aware wind power prediction is essential for grid integration and reliable wind farm operation. We apply neural diffusion processes (NDPs)-a recent class of models that learn distributions over functions-and extend them to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide the first empirical evaluation of NDPs in real supervisory control and data acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture cross-turbine correlations and enable few-shot adaptation to unseen turbines. The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of point accuracy and calibration, particularly for wind turbines whose behaviour deviates from the fleet average. In general, NDP-based models deliver calibrated and scalable predictions suitable for operational deployment, offering sharper, yet trustworthy, predictive intervals that can support dispatch and maintenance decisions in modern wind farms.", "link": "https://arxiv.org/abs/2510.03419", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670616Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03437v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation", "summary": "arXiv:2510.03437v1 Announce Type: cross Abstract: Kernel change-point detection (KCPD) has become a widely used tool for identifying structural changes in complex data. While existing theory establishes consistency under independence assumptions, real-world sequential data such as text exhibits strong dependencies. We establish new guarantees for KCPD under $m$-dependent data: specifically, we prove consistency in the number of detected change points and weak consistency in their locations under mild additional assumptions. We perform an LLM-based simulation that generates synthetic $m$-dependent text to validate the asymptotics. To complement these results, we present the first comprehensive empirical study of KCPD for text segmentation with modern embeddings. Across diverse text datasets, KCPD with text embeddings outperforms baselines in standard text segmentation metrics. We demonstrate through a case study on Taylor Swift's tweets that KCPD not only provides strong theoretical and simulated reliability but also practical effectiveness for text segmentation tasks.", "link": "https://arxiv.org/abs/2510.03437", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670652Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03464v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Optimal Regularization Under Uncertainty: Distributional Robustness and Convexity Constraints", "summary": "arXiv:2510.03464v1 Announce Type: cross Abstract: Regularization is a central tool for addressing ill-posedness in inverse problems and statistical estimation, with the choice of a suitable penalty often determining the reliability and interpretability of downstream solutions. While recent work has characterized optimal regularizers for well-specified data distributions, practical deployments are often complicated by distributional uncertainty and the need to enforce structural constraints such as convexity. In this paper, we introduce a framework for distributionally robust optimal regularization, which identifies regularizers that remain effective under perturbations of the data distribution. Our approach leverages convex duality to reformulate the underlying distributionally robust optimization problem, eliminating the inner maximization and yielding formulations that are amenable to numerical computation. We show how the resulting robust regularizers interpolate between memorization of the training distribution and uniform priors, providing insights into their behavior as robustness parameters vary. For example, we show how certain ambiguity sets, such as those based on the Wasserstein-1 distance, naturally induce regularity in the optimal regularizer by promoting regularizers with smaller Lipschitz constants. We further investigate the setting where regularizers are required to be convex, formulating a convex program for their computation and illustrating their stability with respect to distributional shifts. Taken together, our results provide both theoretical and computational foundations for designing regularizers that are reliable under model uncertainty and structurally constrained for robust deployment.", "link": "https://arxiv.org/abs/2510.03464", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670703Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03470v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "On residual network depth", "summary": "arXiv:2510.03470v1 Announce Type: cross Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled models of unprecedented depth, yet a formal understanding of why depth is so effective remains an open question. A popular intuition, following Veit et al. (2016), is that these residual networks behave like ensembles of many shallower models. Our key finding is an explicit analytical formula that verifies this ensemble perspective, proving that increasing network depth is mathematically equivalent to expanding the size of this implicit ensemble. Furthermore, our expansion reveals a hierarchical ensemble structure in which the combinatorial growth of computation paths leads to an explosion in the output signal, explaining the historical necessity of normalization layers in training deep models. This insight offers a first principles explanation for the historical dependence on normalization layers and sheds new light on a family of successful normalization-free techniques like SkipInit and Fixup. However, while these previous approaches infer scaling factors through optimizer analysis or a heuristic analogy to Batch Normalization, our work offers the first explanation derived directly from the network's inherent functional structure. Specifically, our Residual Expansion Theorem reveals that scaling each residual module provides a principled solution to taming the combinatorial explosion inherent to these architectures. We further show that this scaling acts as a capacity controls that also implicitly regularizes the model's complexity.", "link": "https://arxiv.org/abs/2510.03470", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670752Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03507v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Composite Optimization with Error Feedback: the Dual Averaging Approach", "summary": "arXiv:2510.03507v1 Announce Type: cross Abstract: Communication efficiency is a central challenge in distributed machine learning training, and message compression is a widely used solution. However, standard Error Feedback (EF) methods (Seide et al., 2014), though effective for smooth unconstrained optimization with compression (Karimireddy et al., 2019), fail in the broader and practically important setting of composite optimization, which captures, e.g., objectives consisting of a smooth loss combined with a non-smooth regularizer or constraints. The theoretical foundation and behavior of EF in the context of the general composite setting remain largely unexplored. In this work, we consider composite optimization with EF. We point out that the basic EF mechanism and its analysis no longer stand when a composite part is involved. We argue that this is because of a fundamental limitation in the method and its analysis technique. We propose a novel method that combines Dual Averaging with EControl (Gao et al., 2024), a state-of-the-art variant of the EF mechanism, and achieves for the first time a strong convergence analysis for composite optimization with error feedback. Along with our new algorithm, we also provide a new and novel analysis template for inexact dual averaging method, which might be of independent interest. We also provide experimental results to complement our theoretical findings.", "link": "https://arxiv.org/abs/2510.03507", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670832Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03534v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning", "summary": "arXiv:2510.03534v1 Announce Type: cross Abstract: We study the problem of long-term (multiple days) mapping of a river plume using multiple autonomous underwater vehicles (AUVs), focusing on the Douro river representative use-case. We propose an energy - and communication - efficient multi-agent reinforcement learning approach in which a central coordinator intermittently communicates with the AUVs, collecting measurements and issuing commands. Our approach integrates spatiotemporal Gaussian process regression (GPR) with a multi-head Q-network controller that regulates direction and speed for each AUV. Simulations using the Delft3D ocean model demonstrate that our method consistently outperforms both single- and multi-agent benchmarks, with scaling the number of agents both improving mean squared error (MSE) and operational endurance. In some instances, our algorithm demonstrates that doubling the number of AUVs can more than double endurance while maintaining or improving accuracy, underscoring the benefits of multi-agent coordination. Our learned policies generalize across unseen seasonal regimes over different months and years, demonstrating promise for future developments of data-driven long-term monitoring of dynamic plume environments.", "link": "https://arxiv.org/abs/2510.03534", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670873Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03535v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Sequential decoder training for improved latent space dynamics identification", "summary": "arXiv:2510.03535v1 Announce Type: cross Abstract: Accurate numerical solutions of partial differential equations are essential in many scientific fields but often require computationally expensive solvers, motivating reduced-order models (ROMs). Latent Space Dynamics Identification (LaSDI) is a data-driven ROM framework that combines autoencoders with equation discovery to learn interpretable latent dynamics. However, enforcing latent dynamics during training can compromise reconstruction accuracy of the model for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that improves reconstruction and prediction accuracy by sequentially learning additional decoders to correct residual errors from previous stages. Applied to the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI, achieving lower prediction errors and reduced training time across a wide range of architectures.", "link": "https://arxiv.org/abs/2510.03535", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670903Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03569v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Longitudinal Flow Matching for Trajectory Modeling", "summary": "arXiv:2510.03569v1 Announce Type: cross Abstract: Generative models for sequential data often struggle with sparsely sampled and high-dimensional trajectories, typically reducing the learning of dynamics to pairwise transitions. We propose \\textit{Interpolative Multi-Marginal Flow Matching} (IMMFM), a framework that learns continuous stochastic dynamics jointly consistent with multiple observed time points. IMMFM employs a piecewise-quadratic interpolation path as a smooth target for flow matching and jointly optimizes drift and a data-driven diffusion coefficient, supported by a theoretical condition for stable learning. This design captures intrinsic stochasticity, handles irregular sparse sampling, and yields subject-specific trajectories. Experiments on synthetic benchmarks and real-world longitudinal neuroimaging datasets show that IMMFM outperforms existing methods in both forecasting accuracy and further downstream tasks.", "link": "https://arxiv.org/abs/2510.03569", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670933Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03576v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems", "summary": "arXiv:2510.03576v1 Announce Type: cross Abstract: Deep learning has gained attention for solving PDEs, but the black-box nature of neural networks hinders precise enforcement of boundary conditions. To address this, we propose a boundary condition-guaranteed evolutionary Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN, we propose three distinct and combinable approaches for incorporating Dirichlet, periodic, and Neumann boundary conditions into the network. For Dirichlet problem, we use smooth and global Gaussian RBFs to construct univariate basis functions for approximating the solution and to encode boundary information at the activation level of the network. To handle periodic problems, we employ a periodic layer constructed from a set of sinusoidal functions to enforce the boundary conditions exactly. For a Neumann problem, we devise a least-squares formulation to guide the parameter evolution toward satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the periodic layer, and the evolutionary framework, we can perform accurate PDE simulations while rigorously enforcing boundary conditions. For demonstration, we conducted extensive numerical experiments on Dirichlet, Neumann, periodic, and mixed boundary value problems. The results indicate that BEKAN outperforms both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In conclusion, the proposed approach enhances the capability of KANs in solving PDE problems while satisfying boundary conditions, thereby facilitating advancements in scientific computing and engineering applications.", "link": "https://arxiv.org/abs/2510.03576", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.670976Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03578v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning", "summary": "arXiv:2510.03578v1 Announce Type: cross Abstract: Learning dynamics is essential for model-based control and Reinforcement Learning in engineering systems, such as robotics and power systems. However, limited system measurements, such as those from low-resolution sensors, demand sample-efficient learning. Symmetry provides a powerful inductive bias by characterizing equivariant relations in system states to improve sample efficiency. While recent methods attempt to discover symmetries from data, they typically assume a single global symmetry group and treat symmetry discovery and dynamic learning as separate tasks, leading to limited expressiveness and error accumulation. In this paper, we propose the Latent Mixture of Symmetries (Latent MoS), an expressive model that captures a mixture of symmetry-governed latent factors from complex dynamical measurements. Latent MoS focuses on dynamic learning while locally and provably preserving the underlying symmetric transformations. To further capture long-term equivariance, we introduce a hierarchical architecture that stacks MoS blocks. Numerical experiments in diverse physical systems demonstrate that Latent MoS outperforms state-of-the-art baselines in interpolation and extrapolation tasks while offering interpretable latent representations suitable for future geometric and safety-critical analyses.", "link": "https://arxiv.org/abs/2510.03578", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.671014Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03587v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Exact and Approximate MCMC for Doubly-intractable Probabilistic Graphical Models Leveraging the Underlying Independence Model", "summary": "arXiv:2510.03587v1 Announce Type: cross Abstract: Bayesian inference for doubly-intractable probabilistic graphical models typically involves variations of the exchange algorithm or approximate Markov chain Monte Carlo (MCMC) samplers. However, existing methods for both classes of algorithms require either perfect samplers or sequential samplers for complex models, which are often either not available, or suffer from poor mixing, especially in high dimensions. We develop a method that does not require perfect or sequential sampling, and can be applied to both classes of methods: exact and approximate MCMC. The key to our approach is to utilize the tractable independence model underlying an intractable probabilistic graphical model for the purpose of constructing a finite sample unbiased Monte Carlo (and not MCMC) estimate of the Metropolis--Hastings ratio. This innovation turns out to be crucial for scalability in high dimensions. The method is demonstrated on the Ising model. Gradient-based alternatives to construct a proposal, such as Langevin and Hamiltonian Monte Carlo approaches, also arise as a natural corollary to our general procedure, and are demonstrated as well.", "link": "https://arxiv.org/abs/2510.03587", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.671051Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03605v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Understanding the Role of Training Data in Test-Time Scaling", "summary": "arXiv:2510.03605v1 Announce Type: cross Abstract: Test-time scaling improves the reasoning capabilities of large language models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts (CoTs). This enables models to tackle more complex problem by breaking them down into additional steps, backtracking, and correcting mistakes. Despite its strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions in the training data under which long CoTs emerge, and when such long CoTs improve the performance, remain unclear. In this paper, we study the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. Our analysis provides a theoretical explanation for several intriguing observations: First, at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. Second, if the skills required to solve a downstream task are not sufficiently present in the training data, increasing test-time compute can harm performance. Finally, we characterize task hardness via the smallest eigenvalue of its feature covariance matrix and show that training on a diverse, relevant, and hard set of tasks results in best performance for test-time scaling. We confirm our findings with experiments on large, nonlinear transformer architectures.", "link": "https://arxiv.org/abs/2510.03605", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.671099Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03614v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Neural Bayesian Filtering", "summary": "arXiv:2510.03614v1 Announce Type: cross Abstract: We present Neural Bayesian Filtering (NBF), an algorithm for maintaining distributions over hidden states, called beliefs, in partially observable systems. NBF is trained to find a good latent representation of the beliefs induced by a task. It maps beliefs to fixed-length embedding vectors, which condition generative models for sampling. During filtering, particle-style updates compute posteriors in this embedding space using incoming observations and the environment's dynamics. NBF combines the computational efficiency of classical filters with the expressiveness of deep generative models - tracking rapidly shifting, multimodal beliefs while mitigating the risk of particle impoverishment. We validate NBF in state estimation tasks in three partially observable environments.", "link": "https://arxiv.org/abs/2510.03614", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.671166Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03634v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Handling Missing Data in Probabilistic Regression Trees: Methods and Implementation in R", "summary": "arXiv:2510.03634v1 Announce Type: cross Abstract: Probabilistic Regression Trees (PRTrees) generalize traditional decision trees by incorporating probability functions that associate each data point with different regions of the tree, providing smooth decisions and continuous responses. This paper introduces an adaptation of PRTrees capable of handling missing values in covariates through three distinct approaches: (i) a uniform probability method, (ii) a partial observation approach, and (iii) a dimension-reduced smoothing technique. The proposed methods preserve the interpretability properties of PRTrees while extending their applicability to incomplete datasets. Simulation studies under MCAR conditions demonstrate the relative performance of each approach, including comparisons with traditional regression trees on smooth function estimation tasks. The proposed methods, together with the original version, have been developed in R with highly optimized routines and are distributed in the PRTree package, publicly available on CRAN. In this paper we also present and discuss the main functionalities of the PRTree package, providing researchers and practitioners with new tools for incomplete data analysis.", "link": "https://arxiv.org/abs/2510.03634", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.671204Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03659v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders", "summary": "arXiv:2510.03659v1 Announce Type: cross Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models (LLMs), based on the assumption that their interpretable features naturally enable effective model behavior steering. Yet, a fundamental question remains unanswered: does higher interpretability indeed imply better steering utility? To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B, Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels, and evaluate their interpretability and steering utility based on SAEBench (arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis reveals only a relatively weak positive association (tau b approx 0.298), indicating that interpretability is an insufficient proxy for steering performance. We conjecture the interpretability utility gap may stem from the selection of SAE features, as not all of them are equally effective for steering. To further find features that truly steer the behavior of LLMs, we propose a novel selection criterion called Delta Token Confidence, which measures how much amplifying a feature changes the next token distribution. We show that our method improves the steering performance of three LLMs by 52.52 percent compared to the current best output score based criterion (arXiv:2503.34567). Strikingly, after selecting features with high Delta Token Confidence, the correlation between interpretability and utility vanishes (tau b approx 0), and can even become negative. This further highlights the divergence between interpretability and utility for the most effective steering features.", "link": "https://arxiv.org/abs/2510.03659", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.671302Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03678v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Towards Sampling Data Structures for Tensor Products in Turnstile Streams", "summary": "arXiv:2510.03678v1 Announce Type: cross Abstract: This paper studies the computational challenges of large-scale attention-based models in artificial intelligence by utilizing importance sampling methods in the streaming setting. Inspired by the classical definition of the $\\ell_2$ sampler and the recent progress of the attention scheme in Large Language Models (LLMs), we propose the definition of the attention sampler. Our approach significantly reduces the computational burden of traditional attention mechanisms. We analyze the effectiveness of the attention sampler from a theoretical perspective, including space and update time. Additionally, our framework exhibits scalability and broad applicability across various model architectures and domains.", "link": "https://arxiv.org/abs/2510.03678", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.671330Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03679v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Group Policy Gradient", "summary": "arXiv:2510.03679v1 Announce Type: cross Abstract: We introduce Group Policy Gradient (GPG), a family of critic-free policy-gradient estimators for general MDPs. Inspired by the success of GRPO's approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a learned value function with a group-based Monte Carlo advantage estimator, removing the memory, compute, and hyperparameter costs of training a critic while preserving PPO's clipped-objective structure. We prove the consistency of the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate empirically that GPG matches or outperforms PPO on standard benchmarks. GPG makes better use of parallel simulations, which, together with its critic-free design, results in more efficient use of computational resources than PPO.", "link": "https://arxiv.org/abs/2510.03679", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.671362Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03722v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach", "summary": "arXiv:2510.03722v1 Announce Type: cross Abstract: Reinforcement learning (RL) has been widely applied to sequential decision making, where interpretability and performance are both critical for practical adoption. Current approaches typically focus on performance and rely on post hoc explanations to account for interpretability. Different from these approaches, we focus on designing an interpretability-oriented yet performance-enhanced RL approach. Specifically, we propose a spectral based linear RL method that extends the ridge regression-based approach through a spectral filter function. The proposed method clarifies the role of regularization in controlling estimation error and further enables the design of an adaptive regularization parameter selection strategy guided by the bias-variance trade-off principle. Theoretical analysis establishes near-optimal bounds for both parameter estimation and generalization error. Extensive experiments on simulated environments and real-world datasets from Kuaishou and Taobao demonstrate that our method either outperforms or matches existing baselines in decision quality. We also conduct interpretability analyses to illustrate how the learned policies make decisions, thereby enhancing user trust. These results highlight the potential of our approach to bridge the gap between RL theory and practical decision making, providing interpretability, accuracy, and adaptability in management contexts.", "link": "https://arxiv.org/abs/2510.03722", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.671464Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03734v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Cost Efficient Fairness Audit Under Partial Feedback", "summary": "arXiv:2510.03734v1 Announce Type: cross Abstract: We study the problem of auditing the fairness of a given classifier under partial feedback, where true labels are available only for positively classified individuals, (e.g., loan repayment outcomes are observed only for approved applicants). We introduce a novel cost model for acquiring additional labeled data, designed to more accurately reflect real-world costs such as credit assessment, loan processing, and potential defaults. Our goal is to find optimal fairness audit algorithms that are more cost-effective than random exploration and natural baselines. In our work, we consider two audit settings: a black-box model with no assumptions on the data distribution, and a mixture model, where features and true labels follow a mixture of exponential family distributions. In the black-box setting, we propose a near-optimal auditing algorithm under mild assumptions and show that a natural baseline can be strictly suboptimal. In the mixture model setting, we design a novel algorithm that achieves significantly lower audit cost than the black-box case. Our approach leverages prior work on learning from truncated samples and maximum-a-posteriori oracles, and extends known results on spherical Gaussian mixtures to handle exponential family mixtures, which may be of independent interest. Moreover, our algorithms apply to popular fairness metrics including demographic parity, equal opportunity, and equalized odds. Empirically, we demonstrate strong performance of our algorithms on real-world fair classification datasets like Adult Income and Law School, consistently outperforming natural baselines by around 50% in terms of audit cost.", "link": "https://arxiv.org/abs/2510.03734", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.671509Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03798v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Robust Batched Bandits", "summary": "arXiv:2510.03798v1 Announce Type: cross Abstract: The batched multi-armed bandit (MAB) problem, in which rewards are collected in batches, is crucial for applications such as clinical trials. Existing research predominantly assumes light-tailed reward distributions, yet many real-world scenarios, including clinical outcomes, exhibit heavy-tailed characteristics. This paper bridges this gap by proposing robust batched bandit algorithms designed for heavy-tailed rewards, within both finite-arm and Lipschitz-continuous settings. We reveal a surprising phenomenon: in the instance-independent regime, as well as in the Lipschitz setting, heavier-tailed rewards necessitate a smaller number of batches to achieve near-optimal regret. In stark contrast, for the instance-dependent setting, the required number of batches to attain near-optimal regret remains invariant with respect to tail heaviness.", "link": "https://arxiv.org/abs/2510.03798", "published": "2025-10-07T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-08T01:36:16.671584Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05106v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis", "summary": "arXiv:2510.05106v1 Announce Type: new Abstract: The design of safety-critical agents based on large language models (LLMs) requires more than simple prompt engineering. This paper presents a comprehensive information-theoretic analysis of how rule encodings in system prompts influence attention mechanisms and compliance behaviour. We demonstrate that rule formats with low syntactic entropy and highly concentrated anchors reduce attention entropy and improve pointer fidelity, but reveal a fundamental trade-off between anchor redundancy and attention entropy that previous work failed to recognize. Through formal analysis of multiple attention architectures including causal, bidirectional, local sparse, kernelized, and cross-attention mechanisms, we establish bounds on pointer fidelity and show how anchor placement strategies must account for competing fidelity and entropy objectives. Combining these insights with a dynamic rule verification architecture, we provide a formal proof that hot reloading of verified rule sets increases the asymptotic probability of compliant outputs. These findings underscore the necessity of principled anchor design and dual enforcement mechanisms to protect LLM-based agents against prompt injection attacks while maintaining compliance in evolving domains.", "link": "https://arxiv.org/abs/2510.05106", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.709312Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05107v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Structured Cognition for Behavioral Intelligence in Large Language Model Agents: Preliminary Study", "summary": "arXiv:2510.05107v1 Announce Type: new Abstract: Large language models have advanced natural language understanding and generation, yet their use as autonomous agents raises architectural challenges for multi-step tasks. Existing frameworks often intertwine inference, memory, and control in a single prompt, which can reduce coherence and predictability. The Structured Cognitive Loop (SCL) is introduced as an alternative architecture that separates these functions. In SCL, the language model is dedicated to inference, memory is maintained externally, and execution is guided by a lightweight controller within a goal-directed loop. This design offloads cognitive load from the model and allows intermediate results to be stored, revisited, and checked before actions are taken, providing a clearer basis for traceability and evaluation. We evaluate SCL against prompt-based baselines including ReAct and common LangChain agents across three scenarios: temperature-based travel planning, email drafting with conditional send, and constraint-guided image generation. All systems share the same base model and tools under matched decoding settings. Across 360 episodes, SCL shows modest but consistent improvements. Task success averages 86.3 percent compared with 70-77 percent for baselines. Goal fidelity is higher, redundant calls are fewer, intermediate states are reused more reliably, and unsupported assertions per 100 tool calls are reduced. Ablations show that external memory and control each contribute independently, and decoding sweeps confirm stability of the effects. These results suggest that architectural separation can improve reliability and traceability without relying on larger models or heavier prompts. The findings are preliminary and intended to guide extended studies with additional models, longer horizons, multimodal tasks, and collaborative settings.", "link": "https://arxiv.org/abs/2510.05107", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.709459Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05115v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Optimization Modeling via Semantic Anchored Alignment", "summary": "arXiv:2510.05115v1 Announce Type: new Abstract: Large language models (LLMs) have opened new paradigms in optimization modeling by enabling the generation of executable solver code from natural language descriptions. Despite this promise, existing approaches typically remain solver-driven: they rely on single-pass forward generation and apply limited post-hoc fixes based on solver error messages, leaving undetected semantic errors that silently produce syntactically correct but logically flawed models. To address this challenge, we propose SAC-Opt, a backward-guided correction framework that grounds optimization modeling in problem semantics rather than solver feedback. At each step, SAC-Opt aligns the original semantic anchors with those reconstructed from the generated code and selectively corrects only the mismatched components, driving convergence toward a semantically faithful model. This anchor-driven correction enables fine-grained refinement of constraint and objective logic, enhancing both fidelity and robustness without requiring additional training or supervision. Empirical results on seven public datasets demonstrate that SAC-Opt improves average modeling accuracy by 7.8\\%, with gains of up to 21.9\\% on the ComplexLP dataset. These findings highlight the importance of semantic-anchored correction in LLM-based optimization workflows to ensure faithful translation from problem intent to solver-executable code.", "link": "https://arxiv.org/abs/2510.05115", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.709542Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05134v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Structuring Reasoning for Complex Rules Beyond Flat Representations", "summary": "arXiv:2510.05134v1 Announce Type: new Abstract: Large language models (LLMs) face significant challenges when processing complex rule systems, as they typically treat interdependent rules as unstructured textual data rather than as logically organized frameworks. This limitation results in reasoning divergence, where models often overlook critical rule dependencies essential for accurate interpretation. Although existing approaches such as Chain-of-Thought (CoT) reasoning have shown promise, they lack systematic methodologies for structured rule processing and are particularly susceptible to error propagation through sequential reasoning chains. To address these limitations, we propose the Dynamic Adjudication Template (DAT), a novel framework inspired by expert human reasoning processes. DAT structures the inference mechanism into three methodical stages: qualitative analysis, evidence gathering, and adjudication. During the qualitative analysis phase, the model comprehensively evaluates the contextual landscape. The subsequent evidence gathering phase involves the targeted extraction of pertinent information based on predefined template elements ([placeholder]), followed by systematic verification against applicable rules. Finally, in the adjudication phase, the model synthesizes these validated components to formulate a comprehensive judgment. Empirical results demonstrate that DAT consistently outperforms conventional CoT approaches in complex rule-based tasks. Notably, DAT enables smaller language models to match, and in some cases exceed, the performance of significantly larger LLMs, highlighting its efficiency and effectiveness in managing intricate rule systems.", "link": "https://arxiv.org/abs/2510.05134", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.709630Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05153v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "An Algorithmic Information-Theoretic Perspective on the Symbol Grounding Problem", "summary": "arXiv:2510.05153v1 Announce Type: new Abstract: This paper provides a definitive, unifying framework for the Symbol Grounding Problem (SGP) by reformulating it within Algorithmic Information Theory (AIT). We demonstrate that the grounding of meaning is a process fundamentally constrained by information-theoretic limits, thereby unifying the G\\\"odelian (self-reference) and No Free Lunch (statistical) perspectives. We model a symbolic system as a universal Turing machine and define grounding as an act of information compression. The argument proceeds in four stages. First, we prove that a purely symbolic system cannot ground almost all possible \"worlds\" (data strings), as they are algorithmically random and thus incompressible. Second, we show that any statically grounded system, specialized for compressing a specific world, is inherently incomplete because an adversarial, incompressible world relative to the system can always be constructed. Third, the \"grounding act\" of adapting to a new world is proven to be non-inferable, as it requires the input of new information (a shorter program) that cannot be deduced from the system's existing code. Finally, we use Chaitin's Incompleteness Theorem to prove that any algorithmic learning process is itself a finite system that cannot comprehend or model worlds whose complexity provably exceeds its own. This establishes that meaning is the open-ended process of a system perpetually attempting to overcome its own information-theoretic limitations.", "link": "https://arxiv.org/abs/2510.05153", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.709712Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05158v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework", "summary": "arXiv:2510.05158v1 Announce Type: new Abstract: Physics-informed neural networks (PINNs) provide a powerful approach for solving partial differential equations (PDEs), but constructing a usable PINN remains labor-intensive and error-prone. Scientists must interpret problems as PDE formulations, design architectures and loss functions, and implement stable training pipelines. Existing large language model (LLM) based approaches address isolated steps such as code generation or architecture suggestion, but typically assume a formal PDE is already specified and therefore lack an end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system that builds trainable PINNs directly from natural language task descriptions. Lang-PINN coordinates four complementary agents: a PDE Agent that parses task descriptions into symbolic PDEs, a PINN Agent that selects architectures, a Code Agent that generates modular implementations, and a Feedback Agent that executes and diagnoses errors for iterative refinement. This design transforms informal task statements into executable and verifiable PINN code. Experiments show that Lang-PINN achieves substantially lower errors and greater robustness than competitive baselines: mean squared error (MSE) is reduced by up to 3--5 orders of magnitude, end-to-end execution success improves by more than 50\\%, and reduces time overhead by up to 74\\%.", "link": "https://arxiv.org/abs/2510.05158", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.709789Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05184v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Representation Potentials of Foundation Models for Multimodal Alignment: A Survey", "summary": "arXiv:2510.05184v1 Announce Type: new Abstract: Foundation models learn highly transferable representations through large-scale pretraining on diverse data. An increasing body of research indicates that these representations exhibit a remarkable degree of similarity across architectures and modalities. In this survey, we investigate the representation potentials of foundation models, defined as the latent capacity of their learned representations to capture task-specific information within a single modality while also providing a transferable basis for alignment and unification across modalities. We begin by reviewing representative foundation models and the key metrics that make alignment measurable. We then synthesize empirical evidence of representation potentials from studies in vision, language, speech, multimodality, and neuroscience. The evidence suggests that foundation models often exhibit structural regularities and semantic consistencies in their representation spaces, positioning them as strong candidates for cross-modal transfer and alignment. We further analyze the key factors that foster representation potentials, discuss open questions, and highlight potential challenges.", "link": "https://arxiv.org/abs/2510.05184", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.709850Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05187v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Real-time Framework for Interoperable Semantic-driven Internet-of-Things in Smart Agriculture", "summary": "arXiv:2510.05187v1 Announce Type: new Abstract: The Internet of Things (IoT) has revolutionized various applications including agriculture, but it still faces challenges in data collection and understanding. This paper proposes a real-time framework with three additional semantic layers to help IoT devices and sensors comprehend data meaning and source. The framework consists of six layers: perception, semantic annotation, interoperability, transportation, semantic reasoning, and application, suitable for dynamic environments. Sensors collect data in the form of voltage, which is then processed by microprocessors or microcontrollers in the semantic annotation and preprocessing layer. Metadata is added to the raw data, including the purpose, ID number, and application. Two semantic algorithms are proposed in the semantic interoperability and ontologies layer: the interoperability semantic algorithm for standardizing file types and the synonym identification algorithm for identifying synonyms. In the transportation layer, raw data and metadata are sent to other IoT devices or cloud computing platforms using techniques like WiFi, Zigbee networks, Bluetooth, and mobile communication networks. A semantic reasoning layer is proposed to infer new knowledge from the existing data, using fuzzy logic, Dempster-Shafer theory, and Bayesian networks. A Graphical User Interface (GUI) is proposed in the application layer to help users communicate with and monitor IoT sensors, devices, and new knowledge inferred. This framework provides a robust solution for managing IoT data, ensuring semantic completeness, and enabling real-time knowledge inference. The integration of uncertainty reasoning methods and semantic interoperability techniques makes this framework a valuable tool for advancing IoT applications in general and in agriculture in particular.", "link": "https://arxiv.org/abs/2510.05187", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.709956Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05188v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents", "summary": "arXiv:2510.05188v1 Announce Type: new Abstract: Although LLMs have been widely adopted for creative content generation, a single-pass process often struggles to produce high-quality long narratives. How to effectively revise and improve long narrative scripts like scriptwriters remains a significant challenge, as it demands a comprehensive understanding of the entire context to identify global structural issues and local detailed flaws, as well as coordinating revisions at multiple granularities and locations. Direct modifications by LLMs typically introduce inconsistencies between local edits and the overall narrative requirements. To address these issues, we propose Dramaturge, a task and feature oriented divide-and-conquer approach powered by hierarchical multiple LLM agents. It consists of a Global Review stage to grasp the overall storyline and structural issues, a Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a Hierarchical Coordinated Revision stage that coordinates and integrates structural and detailed improvements throughout the script. The top-down task flow ensures that high-level strategies guide local modifications, maintaining contextual consistency. The review and revision workflow follows a coarse-to-fine iterative process, continuing through multiple rounds until no further substantive improvements can be made. Comprehensive experiments show that Dramaturge significantly outperforms all baselines in terms of script-level overall quality and scene-level details. Our approach is plug-and-play and can be easily integrated into existing methods to improve the generated scripts.", "link": "https://arxiv.org/abs/2510.05188", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710032Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05196v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Graph-based LLM over Semi-Structured Population Data for Dynamic Policy Response", "summary": "arXiv:2510.05196v1 Announce Type: new Abstract: Timely and accurate analysis of population-level data is crucial for effective decision-making during public health emergencies such as the COVID-19 pandemic. However, the massive input of semi-structured data, including structured demographic information and unstructured human feedback, poses significant challenges to conventional analysis methods. Manual expert-driven assessments, though accurate, are inefficient, while standard NLP pipelines often require large task-specific labeled datasets and struggle with generalization across diverse domains. To address these challenges, we propose a novel graph-based reasoning framework that integrates large language models with structured demographic attributes and unstructured public feedback in a weakly supervised pipeline. The proposed approach dynamically models evolving citizen needs into a need-aware graph, enabling population-specific analyses based on key features such as age, gender, and the Index of Multiple Deprivation. It generates interpretable insights to inform responsive health policy decision-making. We test our method using a real-world dataset, and preliminary experimental results demonstrate its feasibility. This approach offers a scalable solution for intelligent population health monitoring in resource-constrained clinical and governmental settings.", "link": "https://arxiv.org/abs/2510.05196", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710100Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05197v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Efficient Prediction of Pass@k Scaling in Large Language Models", "summary": "arXiv:2510.05197v1 Announce Type: new Abstract: Assessing the capabilities and risks of frontier AI systems is a critical area of research, and recent work has shown that repeated sampling from models can dramatically increase both. For instance, repeated sampling has been shown to increase their capabilities, such as solving difficult math and coding problems, but it has also been shown to increase their potential for harm, such as being jailbroken. Such results raise a crucial question for both capability and safety forecasting: how can one accurately predict a model's behavior when scaled to a massive number of attempts, given a vastly smaller sampling budget? This question is directly relevant to model providers, who serve hundreds of millions of users daily, and to governmental regulators, who seek to prevent harms. To answer this questions, we make three contributions. First, we find that standard methods for fitting these laws suffer from statistical shortcomings that hinder predictive accuracy, especially in data-limited scenarios. Second, we remedy these shortcomings by introducing a robust estimation framework, which uses a beta-binomial distribution to generate more accurate predictions from limited data. Third, we propose a dynamic sampling strategy that allocates a greater budget to harder problems. Combined, these innovations enable more reliable prediction of rare risks and capabilities at a fraction of the computational cost.", "link": "https://arxiv.org/abs/2510.05197", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710172Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05283v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment", "summary": "arXiv:2510.05283v1 Announce Type: new Abstract: Aligning multimodal large language models (MLLMs) with human preferences often relies on single-signal, model-based reward methods. Such monolithic rewards often lack confidence calibration across domain-specific tasks, fail to capture diverse aspects of human preferences, and require extensive data annotation and reward model training. In this work, we propose a hybrid reward modeling framework that integrates complementary reward paradigms: (i) model-based rewards, where a learned reward model predicts scalar or vector scores from synthetic and human feedback, and (ii) rule-based rewards, where domain-specific heuristics provide explicit correctness signals with confidence. Beyond accuracy, we further incorporate multi-aspect rewards to enforce instruction adherence and introduce a generalized length-penalty reward to stabilize training and improve performance. The proposed framework provides a flexible and effective approach to aligning MLLMs through reinforcement learning policy optimization. Our experiments show consistent improvements across different multimodal benchmarks when applying hybrid and multi-aspect reward modeling. Our best performing model in the 3B family achieves an overall average improvement of ~9.5% across general and math reasoning tasks. Focusing specifically on mathematical benchmarks, the model achieves a significant average improvement of ~16%, highlighting its effectiveness in mathematical reasoning and problem solving.", "link": "https://arxiv.org/abs/2510.05283", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710242Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05318v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions", "summary": "arXiv:2510.05318v1 Announce Type: new Abstract: Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short by treating conversation histories as static context or limiting evaluation to read-only operations, failing to reflect production-grade database assistant challenges. We introduce BIRD-INTERACT, a benchmark that restores this realism through: (1) a comprehensive interaction environment coupling each database with a hierarchical knowledge base, metadata files, and a function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from errors without human supervision; (2) two evaluation settings consisting of a pre-defined conversational protocol (c-Interact) and an open-ended agentic setting (a-Interact) where models autonomously decide when to query the user simulator or explore the environment; (3) a challenging task suite covering the full CRUD spectrum for business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600 tasks, up to 11,796 interactions) for comprehensive performance assessment, and BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed behavioral analysis and rapid method development. Our empirical results highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in c-Interact and 17.00% in a-Interact. Analysis via memory grafting and Interaction Test-time Scaling validates the importance of effective interaction for complex, dynamic text-to-SQL tasks.", "link": "https://arxiv.org/abs/2510.05318", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710329Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05335v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Biomedical reasoning in action: Multi-agent System for Auditable Biomedical Evidence Synthesis", "summary": "arXiv:2510.05335v1 Announce Type: new Abstract: We present M-Reason, a demonstration system for transparent, agent-based reasoning and evidence integration in the biomedical domain, with a focus on cancer research. M-Reason leverages recent advances in large language models (LLMs) and modular agent orchestration to automate evidence retrieval, appraisal, and synthesis across diverse biomedical data sources. Each agent specializes in a specific evidence stream, enabling parallel processing and fine-grained analysis. The system emphasizes explainability, structured reporting, and user auditability, providing complete traceability from source evidence to final conclusions. We discuss critical tradeoffs between agent specialization, system complexity, and resource usage, as well as the integration of deterministic code for validation. An open, interactive user interface allows researchers to directly observe, explore and evaluate the multi-agent workflow. Our evaluation demonstrates substantial gains in efficiency and output consistency, highlighting M-Reason's potential as both a practical tool for evidence synthesis and a testbed for robust multi-agent LLM systems in scientific research, available at https://m-reason.digitalecmt.com.", "link": "https://arxiv.org/abs/2510.05335", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710395Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05338v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Integrating Bayesian methods with neural network--based model predictive control: a review", "summary": "arXiv:2510.05338v1 Announce Type: new Abstract: In this review, we assess the use of Bayesian methods in model predictive control (MPC), focusing on neural-network-based modeling, control design, and uncertainty quantification. We systematically analyze individual studies and how they are implemented in practice. While Bayesian approaches are increasingly adopted to capture and propagate uncertainty in MPC, reported gains in performance and robustness remain fragmented, with inconsistent baselines and limited reliability analyses. We therefore argue for standardized benchmarks, ablation studies, and transparent reporting to rigorously determine the effectiveness of Bayesian techniques for MPC.", "link": "https://arxiv.org/abs/2510.05338", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710440Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05363v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "MHA-RAG: Improving Efficiency, Accuracy, and Consistency by Encoding Exemplars as Soft Prompts", "summary": "arXiv:2510.05363v1 Announce Type: new Abstract: Adapting Foundation Models to new domains with limited training data is challenging and computationally expensive. While prior work has demonstrated the effectiveness of using domain-specific exemplars as in-context demonstrations, we investigate whether representing exemplars purely as text is the most efficient, effective, and stable approach. We explore an alternative: representing exemplars as soft prompts with an exemplar order invariant model architecture. To this end, we introduce Multi-Head Attention Retrieval-Augmented Generation (MHA-RAG), a framework with the number of attention heads serving as a simple hyperparameter to control soft prompt-generation across different tasks. Across multiple question-answering benchmarks and model scales, MHA-RAG achieves a 20-point performance gain over standard RAG, while cutting inference costs by a factor of 10X GFLOPs-delivering both higher accuracy and greater efficiency, invariant to exemplar order.", "link": "https://arxiv.org/abs/2510.05363", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710497Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05378v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "What Do You Mean? Exploring How Humans and AI Interact with Symbols and Meanings in Their Interactions", "summary": "arXiv:2510.05378v1 Announce Type: new Abstract: Meaningful human-AI collaboration requires more than processing language, it demands a better understanding of symbols and their constructed meanings. While humans naturally interpret symbols through social interaction, AI systems treat them as patterns with compressed meanings, missing the dynamic meanings that emerge through conversation. Drawing on symbolic interactionism theory, we conducted two studies (N=37) investigated how humans and AI interact with symbols and co-construct their meanings. When AI introduced conflicting meanings and symbols in social contexts, 63% of participants reshaped their definitions. This suggests that conflicts in symbols and meanings prompt reflection and redefinition, allowing both participants and AI to have a better shared understanding of meanings and symbols. This work reveals that shared understanding emerges not from agreement but from the reciprocal exchange and reinterpretation of symbols, suggesting new paradigms for human-AI interaction design.", "link": "https://arxiv.org/abs/2510.05378", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710554Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05402v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Teacher-Student Guided Inverse Modeling for Steel Final Hardness Estimation", "summary": "arXiv:2510.05402v1 Announce Type: new Abstract: Predicting the final hardness of steel after heat treatment is a challenging regression task due to the many-to-one nature of the process -- different combinations of input parameters (such as temperature, duration, and chemical composition) can result in the same hardness value. This ambiguity makes the inverse problem, estimating input parameters from a desired hardness, particularly difficult. In this work, we propose a novel solution using a Teacher-Student learning framework. First, a forward model (Teacher) is trained to predict final hardness from 13 metallurgical input features. Then, a backward model (Student) is trained to infer plausible input configurations from a target hardness value. The Student is optimized by leveraging feedback from the Teacher in an iterative, supervised loop. We evaluate our method on a publicly available tempered steel dataset and compare it against baseline regression and reinforcement learning models. Results show that our Teacher-Student framework not only achieves higher inverse prediction accuracy but also requires significantly less computational time, demonstrating its effectiveness and efficiency for inverse process modeling in materials science.", "link": "https://arxiv.org/abs/2510.05402", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710631Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05432v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems", "summary": "arXiv:2510.05432v1 Announce Type: new Abstract: Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.", "link": "https://arxiv.org/abs/2510.05432", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710704Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05451v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "NASP-T: A Fuzzy Neuro-Symbolic Transformer for Logic-Constrained Aviation Safety Report Classification", "summary": "arXiv:2510.05451v1 Announce Type: new Abstract: Deep transformer models excel at multi-label text classification but often violate domain logic that experts consider essential, an issue of particular concern in safety-critical applications. We propose a hybrid neuro-symbolic framework that integrates Answer Set Programming (ASP) with transformer-based learning on the Aviation Safety Reporting System (ASRS) corpus. Domain knowledge is formalized as weighted ASP rules and validated using the Clingo solver. These rules are incorporated in two complementary ways: (i) as rule-based data augmentation, generating logically consistent synthetic samples that improve label diversity and coverage; and (ii) as a fuzzy-logic regularizer, enforcing rule satisfaction in a differentiable form during fine-tuning. This design preserves the interpretability of symbolic reasoning while leveraging the scalability of deep neural architectures. We further tune per-class thresholds and report both standard classification metrics and logic-consistency rates. Compared to a strong Binary Cross-Entropy (BCE) baseline, our approach improves micro- and macro-F1 scores and achieves up to an 86% reduction in rule violations on the ASRS test set. To the best of our knowledge, this constitutes the first large-scale neuro-symbolic application to ASRS reports that unifies ASP-based reasoning, rule-driven augmentation, and differentiable transformer training for trustworthy, safety-critical NLP.", "link": "https://arxiv.org/abs/2510.05451", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710773Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05457v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Do Code Models Suffer from the Dunning-Kruger Effect?", "summary": "arXiv:2510.05457v1 Announce Type: new Abstract: As artificial intelligence systems increasingly collaborate with humans in creative and technical domains, questions arise about the cognitive boundaries and biases that shape our shared agency. This paper investigates the Dunning-Kruger Effect (DKE), the tendency for those with limited competence to overestimate their abilities in state-of-the-art LLMs in coding tasks. By analyzing model confidence and performance across a diverse set of programming languages, we reveal that AI models mirror human patterns of overconfidence, especially in unfamiliar or low-resource domains. Our experiments demonstrate that less competent models and those operating in rare programming languages exhibit stronger DKE-like bias, suggesting that the strength of the bias is proportionate to the competence of the models.", "link": "https://arxiv.org/abs/2510.05457", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710824Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05465v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "VAL-Bench: Measuring Value Alignment in Language Models", "summary": "arXiv:2510.05465v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly used for tasks where outputs shape human decisions, so it is critical to test whether their responses reflect consistent human values. Existing benchmarks mostly track refusals or predefined safety violations, but these only check rule compliance and do not reveal whether a model upholds a coherent value system when facing controversial real-world issues. We introduce the \\textbf{V}alue \\textbf{AL}ignment \\textbf{Bench}mark (\\textbf{VAL-Bench}), which evaluates whether models maintain a stable value stance across paired prompts that frame opposing sides of public debates. VAL-Bench consists of 115K such pairs from Wikipedia's controversial sections. A well-aligned model should express similar underlying views regardless of framing, which we measure using an LLM-as-judge to score agreement or divergence between paired responses. Applied across leading open- and closed-source models, the benchmark reveals large variation in alignment and highlights trade-offs between safety strategies (e.g., refusals) and more expressive value systems. By providing a scalable, reproducible benchmark, VAL-Bench enables systematic comparison of how reliably LLMs embody human values.", "link": "https://arxiv.org/abs/2510.05465", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710889Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05480v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Vul-R2: A Reasoning LLM for Automated Vulnerability Repair", "summary": "arXiv:2510.05480v1 Announce Type: new Abstract: The exponential increase in software vulnerabilities has created an urgent need for automatic vulnerability repair (AVR) solutions. Recent research has formulated AVR as a sequence generation problem and has leveraged large language models (LLMs) to address this problem. Typically, these approaches prompt or fine-tune LLMs to generate repairs for vulnerabilities directly. Although these methods show state-of-the-art performance, they face the following challenges: (1) Lack of high-quality, vulnerability-related reasoning data. Current approaches primarily rely on foundation models that mainly encode general programming knowledge. Without vulnerability-related reasoning data, they tend to fail to capture the diverse vulnerability repair patterns. (2) Hard to verify the intermediate vulnerability repair process during LLM training. Existing reinforcement learning methods often leverage intermediate execution feedback from the environment (e.g., sandbox-based execution results) to guide reinforcement learning training. In contrast, the vulnerability repair process generally lacks such intermediate, verifiable feedback, which poses additional challenges for model training.", "link": "https://arxiv.org/abs/2510.05480", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710950Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05548v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Decade-long Emission Forecasting with an Ensemble Model in Taiwan", "summary": "arXiv:2510.05548v1 Announce Type: new Abstract: Taiwan's high population and heavy dependence on fossil fuels have led to severe air pollution, with the most prevalent greenhouse gas being carbon dioxide (CO2). There-fore, this study presents a reproducible and comprehensive case study comparing 21 of the most commonly employed time series models in forecasting emissions, analyzing both univariate and multivariate approaches. Among these, Feedforward Neural Network (FFNN), Support Vector Machine (SVM), and Random Forest Regressor (RFR) achieved the best performances. To further enhance robustness, the top performers were integrated with Linear Regression through a custom stacked generalization en-semble technique. Our proposed ensemble model achieved an SMAPE of 1.407 with no signs of overfitting. Finally, this research provides an accurate decade-long emission projection that will assist policymakers in making more data-driven decisions.", "link": "https://arxiv.org/abs/2510.05548", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.710982Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05580v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption", "summary": "arXiv:2510.05580v1 Announce Type: new Abstract: Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.", "link": "https://arxiv.org/abs/2510.05580", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711016Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05592v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "In-the-Flow Agentic System Optimization for Effective Planning and Tool Use", "summary": "arXiv:2510.05592v1 Announce Type: new Abstract: Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns.", "link": "https://arxiv.org/abs/2510.05592", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711071Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05596v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions", "summary": "arXiv:2510.05596v1 Announce Type: new Abstract: Self-evolving agentic artificial intelligence (AI) offers a new paradigm for future wireless systems by enabling autonomous agents to continually adapt and improve without human intervention. Unlike static AI models, self-evolving agents embed an autonomous evolution cycle that updates models, tools, and workflows in response to environmental dynamics. This paper presents a comprehensive overview of self-evolving agentic AI, highlighting its layered architecture, life cycle, and key techniques, including tool intelligence, workflow optimization, self-reflection, and evolutionary learning. We further propose a multi-agent cooperative self-evolving agentic AI framework, where multiple large language models (LLMs) are assigned role-specialized prompts under the coordination of a supervisor agent. Through structured dialogue, iterative feedback, and systematic validation, the system autonomously executes the entire life cycle without human intervention. A case study on antenna evolution in low-altitude wireless networks (LAWNs) demonstrates how the framework autonomously upgrades fixed antenna optimization into movable antenna optimization. Experimental results show that the proposed self-evolving agentic AI autonomously improves beam gain and restores degraded performance by up to 52.02%, consistently surpassing the fixed baseline with little to no human intervention and validating its adaptability and robustness for next-generation wireless intelligence.", "link": "https://arxiv.org/abs/2510.05596", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711113Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05664v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Large Language Model-Based Uncertainty-Adjusted Label Extraction for Artificial Intelligence Model Development in Upper Extremity Radiography", "summary": "arXiv:2510.05664v1 Announce Type: new Abstract: Objectives: To evaluate GPT-4o's ability to extract diagnostic labels (with uncertainty) from free-text radiology reports and to test how these labels affect multi-label image classification of musculoskeletal radiographs. Methods: This retrospective study included radiography series of the clavicle (n=1,170), elbow (n=3,755), and thumb (n=1,978). After anonymization, GPT-4o filled out structured templates by indicating imaging findings as present (\"true\"), absent (\"false\"), or \"uncertain.\" To assess the impact of label uncertainty, \"uncertain\" labels of the training and validation sets were automatically reassigned to \"true\" (inclusive) or \"false\" (exclusive). Label-image-pairs were used for multi-label classification using ResNet50. Label extraction accuracy was manually verified on internal (clavicle: n=233, elbow: n=745, thumb: n=393) and external test sets (n=300 for each). Performance was assessed using macro-averaged receiver operating characteristic (ROC) area under the curve (AUC), precision recall curves, sensitivity, specificity, and accuracy. AUCs were compared with the DeLong test. Results: Automatic extraction was correct in 98.6% (60,618 of 61,488) of labels in the test sets. Across anatomic regions, label-based model training yielded competitive performance measured by macro-averaged AUC values for inclusive (e.g., elbow: AUC=0.80 [range, 0.62-0.87]) and exclusive models (elbow: AUC=0.80 [range, 0.61-0.88]). Models generalized well on external datasets (elbow [inclusive]: AUC=0.79 [range, 0.61-0.87]; elbow [exclusive]: AUC=0.79 [range, 0.63-0.89]). No significant differences were observed across labeling strategies or datasets (p>=0.15). Conclusion: GPT-4o extracted labels from radiologic reports to train competitive multi-label classification models with high accuracy. Detected uncertainty in the radiologic reports did not influence the performance of these models.", "link": "https://arxiv.org/abs/2510.05664", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711161Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05684v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI", "summary": "arXiv:2510.05684v1 Announce Type: new Abstract: Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/", "link": "https://arxiv.org/abs/2510.05684", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711207Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05698v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Joint Communication Scheduling and Velocity Control for Multi-UAV-Assisted Post-Disaster Monitoring: An Attention-Based In-Context Learning Approach", "summary": "arXiv:2510.05698v1 Announce Type: new Abstract: Recently, Unmanned Aerial Vehicles (UAVs) are increasingly being investigated to collect sensory data in post-disaster monitoring scenarios, such as tsunamis, where early actions are critical to limit coastal damage. A major challenge is to design the data collection schedules and flight velocities, as unfavorable schedules and velocities can lead to transmission errors and buffer overflows of the ground sensors, ultimately resulting in significant packet loss. Meanwhile, online Deep Reinforcement Learning (DRL) solutions have a complex training process and a mismatch between simulation and reality that does not meet the urgent requirements of tsunami monitoring. Recent advances in Large Language Models (LLMs) offer a compelling alternative. With their strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation through natural language prompts and example-based guidance without retraining. However, LLM models have input data limitations and thus require customized approaches. In this paper, a joint optimization of data collection schedules and velocities control for multiple UAVs is proposed to minimize data loss. The battery level of the ground sensors, the length of the queues, and the channel conditions, as well as the trajectories of the UAVs, are taken into account. Attention-Based In-Context Learning for Velocity Control and Data Collection Schedule (AIC-VDS) is proposed as an alternative to DRL in emergencies. The simulation results show that the proposed AIC-VDS outperforms both the Deep-Q-Network (DQN) and maximum channel gain baselines.", "link": "https://arxiv.org/abs/2510.05698", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711256Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05733v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot Fault Diagnosis on the Edge", "summary": "arXiv:2510.05733v1 Announce Type: new Abstract: Industrial fault diagnosis faces the dual challenges of data scarcity and the difficulty of deploying large AI models in resource-constrained environments. This paper introduces Syn-Diag, a novel cloud-edge synergistic framework that leverages Large Language Models to overcome these limitations in few-shot fault diagnosis. Syn-Diag is built on a three-tiered mechanism: 1) Visual-Semantic Synergy, which aligns signal features with the LLM's semantic space through cross-modal pre-training; 2) Content-Aware Reasoning, which dynamically constructs contextual prompts to enhance diagnostic accuracy with limited samples; and 3) Cloud-Edge Synergy, which uses knowledge distillation to create a lightweight, efficient edge model capable of online updates via a shared decision space. Extensive experiments on six datasets covering different CWRU and SEU working conditions show that Syn-Diag significantly outperforms existing methods, especially in 1-shot and cross-condition scenarios. The edge model achieves performance comparable to the cloud version while reducing model size by 83% and latency by 50%, offering a practical, robust, and deployable paradigm for modern intelligent diagnostics.", "link": "https://arxiv.org/abs/2510.05733", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711292Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05743v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Artificially intelligent agents in the social and behavioral sciences: A history and outlook", "summary": "arXiv:2510.05743v1 Announce Type: new Abstract: We review the historical development and current trends of artificially intelligent agents (agentic AI) in the social and behavioral sciences: from the first programmable computers, and social simulations soon thereafter, to today's experiments with large language models. This overview emphasizes the role of AI in the scientific process and the changes brought about, both through technological advancements and the broader evolution of science from around 1950 to the present. Some of the specific points we cover include: the challenges of presenting the first social simulation studies to a world unaware of computers, the rise of social systems science, intelligent game theoretic agents, the age of big data and the epistemic upheaval in its wake, and the current enthusiasm around applications of generative AI, and many other topics. A pervasive theme is how deeply entwined we are with the technologies we use to understand ourselves.", "link": "https://arxiv.org/abs/2510.05743", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711324Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05746v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems", "summary": "arXiv:2510.05746v1 Announce Type: new Abstract: Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved state-of-the-art results on various complex reasoning tasks. Recent works have proposed techniques to automate the design of MASes, eliminating the need for manual engineering. However, these techniques perform poorly, often achieving similar or inferior performance to simple baselines. Furthermore, they require computationally expensive re-discovery of architectures for each new task domain and expensive data annotation on domains without existing labeled validation sets. A critical insight is that simple Chain of Thought (CoT) reasoning often performs competitively with these complex systems, suggesting that the fundamental reasoning unit of MASes, CoT, warrants further investigation. To this end, we present a new paradigm for automatic MAS design that pivots the focus to optimizing CoT reasoning. We introduce the Agentic Reasoning Module (ARM), an agentic generalization of CoT where each granular reasoning step is executed by a specialized reasoning module. This module is discovered through a tree search over the code space, starting from a simple CoT module and evolved using mutations informed by reflection on execution traces. The resulting ARM acts as a versatile reasoning building block which can be utilized as a direct recursive loop or as a subroutine in a learned meta-orchestrator. Our approach significantly outperforms both manually designed MASes and state-of-the-art automatic MAS design methods. Crucially, MASes built with ARM exhibit superb generalization, maintaining high performance across different foundation models and task domains without further optimization.", "link": "https://arxiv.org/abs/2510.05746", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711375Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05751v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Uncertainty assessment in satellite-based greenhouse gas emissions estimates using emulated atmospheric transport", "summary": "arXiv:2510.05751v1 Announce Type: new Abstract: Monitoring greenhouse gas emissions and evaluating national inventories require efficient, scalable, and reliable inference methods. Top-down approaches, combined with recent advances in satellite observations, provide new opportunities to evaluate emissions at continental and global scales. However, transport models used in these methods remain a key source of uncertainty: they are computationally expensive to run at scale, and their uncertainty is difficult to characterise. Artificial intelligence offers a dual opportunity to accelerate transport simulations and to quantify their associated uncertainty. We present an ensemble-based pipeline for estimating atmospheric transport \"footprints\", greenhouse gas mole fraction measurements, and their uncertainties using a graph neural network emulator of a Lagrangian Particle Dispersion Model (LPDM). The approach is demonstrated with GOSAT (Greenhouse Gases Observing Satellite) observations for Brazil in 2016. The emulator achieved a ~1000x speed-up over the NAME LPDM, while reproducing large-scale footprint structures. Ensembles were calculated to quantify absolute and relative uncertainty, revealing spatial correlations with prediction error. The results show that ensemble spread highlights low-confidence spatial and temporal predictions for both atmospheric transport footprints and methane mole fractions. While demonstrated here for an LPDM emulator, the approach could be applied more generally to atmospheric transport models, supporting uncertainty-aware greenhouse gas inversion systems and improving the robustness of satellite-based emissions monitoring. With further development, ensemble-based emulators could also help explore systematic LPDM errors, offering a computationally efficient pathway towards a more comprehensive uncertainty budget in greenhouse gas flux estimates.", "link": "https://arxiv.org/abs/2510.05751", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711423Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05761v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis", "summary": "arXiv:2510.05761v1 Announce Type: new Abstract: Predicting the virality of online content remains challenging, especially for culturally complex, fast-evolving memes. This study investigates the feasibility of early prediction of meme virality using a large-scale, cross-lingual dataset from 25 diverse Reddit communities. We propose a robust, data-driven method to define virality based on a hybrid engagement score, learning a percentile-based threshold from a chronologically held-out training set to prevent data leakage. We evaluated a suite of models, including Logistic Regression, XGBoost, and a Multi-layer Perceptron (MLP), with a comprehensive, multimodal feature set across increasing time windows (30-420 min). Crucially, useful signals emerge quickly: our best-performing model, XGBoost, achieves a PR-AUC $>$ 0.52 in just 30 minutes. Our analysis reveals a clear \"evidentiary transition,\" in which the importance of the feature dynamically shifts from the static context to the temporal dynamics as a meme gains traction. This work establishes a robust, interpretable, and practical benchmark for early virality prediction in scenarios where full diffusion cascade data is unavailable, contributing a novel cross-lingual dataset and a methodologically sound definition of virality. To our knowledge, this study is the first to combine time series data with static content and network features to predict early meme virality.", "link": "https://arxiv.org/abs/2510.05761", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711464Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05764v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases", "summary": "arXiv:2510.05764v1 Announce Type: new Abstract: Computational drug repurposing for rare diseases is especially challenging when no prior associations exist between drugs and target diseases. Therefore, knowledge graph completion and message-passing GNNs have little reliable signal to learn and propagate, resulting in poor performance. We present RareAgent, a self-evolving multi-agent system that reframes this task from passive pattern recognition to active evidence-seeking reasoning. RareAgent organizes task-specific adversarial debates in which agents dynamically construct evidence graphs from diverse perspectives to support, refute, or entail hypotheses. The reasoning strategies are analyzed post hoc in a self-evolutionary loop, producing textual feedback that refines agent policies, while successful reasoning paths are distilled into transferable heuristics to accelerate future investigations. Comprehensive evaluations reveal that RareAgent improves the indication AUPRC by 18.1% over reasoning baselines and provides a transparent reasoning chain consistent with clinical evidence.", "link": "https://arxiv.org/abs/2510.05764", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711498Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05774v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming", "summary": "arXiv:2510.05774v1 Announce Type: new Abstract: Constraint programming (CP) is a crucial technology for solving real-world constraint optimization problems (COPs), with the advantages of rich modeling semantics and high solving efficiency. Using large language models (LLMs) to generate formal modeling automatically for COPs is becoming a promising approach, which aims to build trustworthy neuro-symbolic AI with the help of symbolic solvers. However, CP has received less attention compared to works based on operations research (OR) models. We introduce ConstraintLLM, the first LLM specifically designed for CP modeling, which is trained on an open-source LLM with multi-instruction supervised fine-tuning. We propose the Constraint-Aware Retrieval Module (CARM) to increase the in-context learning capabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with guided self-correction mechanism. Moreover, we construct and release IndusCP, the first industrial-level benchmark for CP modeling, which contains 140 challenging tasks from various domains. Our experiments demonstrate that ConstraintLLM achieves state-of-the-art solving accuracy across multiple benchmarks and outperforms the baselines by 2x on the new IndusCP benchmark. Code and data are available at: https://github.com/william4s/ConstraintLLM.", "link": "https://arxiv.org/abs/2510.05774", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711534Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05865v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "The Safety Challenge of World Models for Embodied AI Agents: A Review", "summary": "arXiv:2510.05865v1 Announce Type: new Abstract: The rapid progress in embodied artificial intelligence has highlighted the necessity for more advanced and integrated models that can perceive, interpret, and predict environmental dynamics. In this context, World Models (WMs) have been introduced to provide embodied agents with the abilities to anticipate future environmental states and fill in knowledge gaps, thereby enhancing agents' ability to plan and execute actions. However, when dealing with embodied agents it is fundamental to ensure that predictions are safe for both the agent and the environment. In this article, we conduct a comprehensive literature review of World Models in the domains of autonomous driving and robotics, with a specific focus on the safety implications of scene and control generation tasks. Our review is complemented by an empirical analysis, wherein we collect and examine predictions from state-of-the-art models, identify and categorize common faults (herein referred to as pathologies), and provide a quantitative evaluation of the results.", "link": "https://arxiv.org/abs/2510.05865", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711568Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05871v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Towards Label-Free Biological Reasoning Synthetic Dataset Creation via Uncertainty Filtering", "summary": "arXiv:2510.05871v1 Announce Type: new Abstract: Synthetic chain-of-thought (CoT) traces are widely used to train large reasoning models (LRMs), improving generalization by providing step-level supervision. Yet most approaches require ground-truth labels to seed or filter these traces - an expensive bottleneck in domains like biology where wet-lab data are scarce. We propose a label-free alternative: uncertainty-based filtering, which uses a model's own confidence - quantified through established uncertainty metrics like self-consistency and predictive perplexity - as a substitute for external labels. We sample multiple reasoning traces and retain only low-uncertainty subsets. Applied to biological perturbation prediction, a domain where wet-lab labels are especially costly, we show that the filtered subset has higher accuracy, and that supervised fine-tuning (SFT) on uncertainty-filtered data outperforms unfiltered synthetic data, narrows the gap to ground-truth training, and surpasses strong LRM baselines. Ablations show that per-class filtering corrects for class-specific uncertainty scales and that hybrid uncertainty metrics yield higher-quality datasets. Our results suggest that model-internal confidence is a powerful signal for efficient reasoning dataset creation, enabling LRMs in domains where supervision is expensive.", "link": "https://arxiv.org/abs/2510.05871", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711605Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05909v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Optimizing for Persuasion Improves LLM Generalization: Evidence from Quality-Diversity Evolution of Debate Strategies", "summary": "arXiv:2510.05909v1 Announce Type: new Abstract: Large Language Models (LLMs) optimized to output truthful answers often overfit, producing brittle reasoning that fails to generalize. While persuasion-based optimization has shown promise in debate settings, it has not been systematically compared against mainstream truth-based approaches. We introduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm that evolves diverse debate strategies across different categories (rationality, authority, emotional appeal, etc.) through tournament-style competitions where two LLMs debate while a third judges. Unlike previously proposed methods that require a population of LLMs, our approach maintains diversity of opponents through prompt-based strategies within a single LLM architecture, making it more accessible for experiments while preserving the key benefits of population-based optimization. In contrast to prior work, we explicitly isolate the role of the optimization objective by fixing the debate protocol and swapping only the fitness function: persuasion rewards strategies that convince the judge irrespective of truth, whereas truth rewards collaborative correctness. Across three model scales (7B, 32B, 72B parameters) and multiple dataset sizes from the QuALITY benchmark, persuasion-optimized strategies achieve up to 13.94% smaller train-test generalization gaps, while matching or exceeding truth optimization's test performance. These results provide the first controlled evidence that competitive pressure to persuade, rather than seek the truth collaboratively, fosters more transferable reasoning skills, offering a promising path for improving LLM generalization.", "link": "https://arxiv.org/abs/2510.05909", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711650Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05950v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Training-Free Time Series Classification via In-Context Reasoning with LLM Agents", "summary": "arXiv:2510.05950v1 Announce Type: new Abstract: Time series classification (TSC) spans diverse application scenarios, yet labeled data are often scarce, making task-specific training costly and inflexible. Recent reasoning-oriented large language models (LLMs) show promise in understanding temporal patterns, but purely zero-shot usage remains suboptimal. We propose FETA, a multi-agent framework for training-free TSC via exemplar-based in-context reasoning. FETA decomposes a multivariate series into channel-wise subproblems, retrieves a few structurally similar labeled examples for each channel, and leverages a reasoning LLM to compare the query against these exemplars, producing channel-level labels with self-assessed confidences; a confidence-weighted aggregator then fuses all channel decisions. This design eliminates the need for pretraining or fine-tuning, improves efficiency by pruning irrelevant channels and controlling input length, and enhances interpretability through exemplar grounding and confidence estimation. On nine challenging UEA datasets, FETA achieves strong accuracy under a fully training-free setting, surpassing multiple trained baselines. These results demonstrate that a multi-agent in-context reasoning framework can transform LLMs into competitive, plug-and-play TSC solvers without any parameter training. The code is available at https://github.com/SongyuanSui/FETATSC.", "link": "https://arxiv.org/abs/2510.05950", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711687Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05962v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization", "summary": "arXiv:2510.05962v1 Announce Type: new Abstract: Conducting contamination-free evaluation of mathematical capabilities can be difficult for two reasons: models may memorize a test set once it is made public, and current mathematical benchmarks are prone to overfitting due to having limited diversity of symbols and rules, coupled with closed-ended answers. This paper proposes a method to leverage these shortcomings as useful features to a construct dynamic, counterfactual benchmark, which can be used to both reveal overfitting and measure true reasoning. We demonstrate this via MatheMagic, which generates math test instances with the interpretations of numbers and operators altered, yet has automatically verifiable answers. Test instances are randomly seeded and constructed at test time to evaluate a model's induction or deduction capability, offering stability, extensibility, comparability, and robustness to overfitting. Our experiments find that models solve deduction more easily than induction, but they revert to standard math. Further analysis reveals that math-adapted models fail to exhibit a general \"skill\" of reasoning, and fine-tuning on induction tasks generalizes poorly.", "link": "https://arxiv.org/abs/2510.05962", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711727Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05996v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Information-Theoretic Policy Pre-Training with Empowerment", "summary": "arXiv:2510.05996v1 Announce Type: new Abstract: Empowerment, an information-theoretic measure of an agent's potential influence on its environment, has emerged as a powerful intrinsic motivation and exploration framework for reinforcement learning (RL). Besides for unsupervised RL and skill learning algorithms, the specific use of empowerment as a pre-training signal has received limited attention in the literature. We show that empowerment can be used as a pre-training signal for data-efficient downstream task adaptation. For this we extend the traditional notion of empowerment by introducing discounted empowerment, which balances the agent's control over the environment across short- and long-term horizons. Leveraging this formulation, we propose a novel pre-training paradigm that initializes policies to maximize discounted empowerment, enabling agents to acquire a robust understanding of environmental dynamics. We analyze empowerment-based pre-training for various existing RL algorithms and empirically demonstrate its potential as a general-purpose initialization strategy: empowerment-maximizing policies with long horizons are data-efficient and effective, leading to improved adaptability in downstream tasks. Our findings pave the way for future research to scale this framework to high-dimensional and complex tasks, further advancing the field of RL.", "link": "https://arxiv.org/abs/2510.05996", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711766Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06002v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG", "summary": "arXiv:2510.06002v1 Announce Type: new Abstract: The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core limitations of standard Retrieval-Augmented Generation in the legal domain by providing a verifiable knowledge graph that models hierarchical structure, temporal evolution, and causal events of legal norms. However, a critical gap remains: how to reliably query this structured knowledge without sacrificing its deterministic properties. This paper introduces the SAT-Graph API, a formal query execution layer centered on canonical actions-atomic, composable, and auditable primitives that isolate probabilistic discovery from deterministic retrieval. These actions enable: (i) high-precision hybrid search; (ii) robust reference resolution; (iii) point-in-time version retrieval; and (iv) auditable causal tracing. We demonstrate how planner-guided agents can decompose complex queries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer architecture transforms retrieval from an opaque black box to a transparent, auditable process, directly addressing Explainable AI (XAI) requirements for high-stakes domains.", "link": "https://arxiv.org/abs/2510.06002", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711801Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06014v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation in Large Reasoning Models", "summary": "arXiv:2510.06014v1 Announce Type: new Abstract: Test-time scaling has emerged as a transformative paradigm for enhancing the performance of large reasoning models, enabling dynamic allocation of computational resources during inference. However, as the landscape of reasoning models rapidly expands, a critical question remains: how can we systematically compare and evaluate the test-time scaling capabilities across different models? In this paper, we introduce ARISE (Adaptive Resolution-aware Scaling Evaluation), a novel metric specifically designed to assess the test-time scaling effectiveness of large reasoning models. Unlike existing evaluation approaches, ARISE incorporates two key innovations: (1) sample-level awareness that effectively penalizes negative scaling behaviors where increased computation leads to performance degradation, and (2) a dynamic sampling mechanism that mitigates the impact of accuracy fluctuations and token count instability on the final assessment. We conduct comprehensive experiments evaluating state-of-the-art reasoning models across diverse domains including mathematical reasoning, code generation, and agentic tasks. Our results demonstrate that ARISE provides a reliable and fine-grained measurement of test-time scaling capabilities, revealing significant variations in scaling efficiency across models. Notably, our evaluation identifies Claude Opus as exhibiting superior scaling characteristics compared to other contemporary reasoning models.", "link": "https://arxiv.org/abs/2510.06014", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711845Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06036v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?", "summary": "arXiv:2510.06036v1 Announce Type: new Abstract: Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as \\textbf{refusal cliff}: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\\% of these heads can reduce attack success rates below 10\\%. Building on these mechanistic insights, we propose \\textbf{Cliff-as-a-Judge}, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment.", "link": "https://arxiv.org/abs/2510.06036", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711884Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06052v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "MixReasoning: Switching Modes to Think", "summary": "arXiv:2510.06052v1 Announce Type: new Abstract: Reasoning models enhance performance by tackling problems in a step-by-step manner, decomposing them into sub-problems and exploring long chains of thought before producing an answer. However, applying extended reasoning to every step introduces substantial redundancy, as sub-problems vary widely in difficulty and complexity: a small number of pivotal steps are genuinely challenging and decisive for the final answer, while many others only involve straightforward revisions or simple computations. Therefore, a natural idea is to endow reasoning models with the ability to adaptively respond to this variation, rather than treating all steps with the same level of elaboration. To this end, we propose MixReasoning, a framework that dynamically adjusts the depth of reasoning within a single response. The resulting chain of thought then becomes a mixture of detailed reasoning on difficult steps and concise inference on simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning shortens reasoning length and substantially improves efficiency without compromising accuracy.", "link": "https://arxiv.org/abs/2510.06052", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711921Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06056v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research", "summary": "arXiv:2510.06056v1 Announce Type: new Abstract: Large language models hold promise as scientific assistants, yet existing agents either rely solely on algorithm evolution or on deep research in isolation, both of which face critical limitations. Pure algorithm evolution, as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly plateaus in complex domains, while pure deep research proposes ideas without validation, resulting in unrealistic or unimplementable solutions. We present DeepEvolve, an agent that integrates deep research with algorithm evolution, uniting external knowledge retrieval, cross-file code editing, and systematic debugging under a feedback-driven iterative loop. Each iteration not only proposes new hypotheses but also refines, implements, and tests them, avoiding both shallow improvements and unproductive over-refinements. Across nine benchmarks in chemistry, mathematics, biology, materials, and patents, DeepEvolve consistently improves the initial algorithm, producing executable new algorithms with sustained gains. By bridging the gap between unguided evolution and research without grounding, DeepEvolve provides a reliable framework for advancing scientific algorithm discovery. Our code is available at https://github.com/liugangcode/deepevolve.", "link": "https://arxiv.org/abs/2510.06056", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.711960Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06063v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language Analysis", "summary": "arXiv:2510.06063v1 Announce Type: new Abstract: Modern enterprises generate vast streams of time series metrics when monitoring complex systems, known as observability data. Unlike conventional time series from domains such as weather, observability data are zero-inflated, highly stochastic, and exhibit minimal temporal structure. Despite their importance, observability datasets are underrepresented in public benchmarks due to proprietary restrictions. Existing datasets are often anonymized and normalized, removing scale information and limiting their use for tasks beyond forecasting, such as anomaly detection, root-cause analysis, and multi-modal reasoning. To address this gap, we introduce TelecomTS, a large-scale observability dataset derived from a 5G telecommunications network. TelecomTS features heterogeneous, de-anonymized covariates with explicit scale information and supports a suite of downstream tasks, including anomaly detection, root-cause analysis, and a question-answering benchmark requiring multi-modal reasoning. Benchmarking state-of-the-art time series, language, and reasoning models reveals that existing approaches struggle with the abrupt, noisy, and high-variance dynamics of observability data. Our experiments also underscore the importance of preserving covariates' absolute scale, emphasizing the need for foundation time series models that natively leverage scale information for practical observability applications.", "link": "https://arxiv.org/abs/2510.06063", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.712002Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06078v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents", "summary": "arXiv:2510.06078v1 Announce Type: new Abstract: Route recommendation aims to provide users with optimal travel plans that satisfy diverse and complex requirements. Classical routing algorithms (e.g., shortest-path and constraint-aware search) are efficient but assume structured inputs and fixed objectives, limiting adaptability to natural-language queries. Recent LLM-based approaches enhance flexibility but struggle with spatial reasoning and the joint modeling of route-level and POI-level preferences. To address these limitations, we propose RouteLLM, a hierarchical multi-agent framework that grounds natural-language intents into constraint-aware routes. It first parses user queries into structured intents including POIs, paths, and constraints. A manager agent then coordinates specialized sub-agents: a constraint agent that resolves and formally check constraints, a POI agent that retrieves and ranks candidate POIs, and a path refinement agent that refines routes via a routing engine with preference-conditioned costs. A final verifier agent ensures constraint satisfaction and produces the final route with an interpretable rationale. This design bridges linguistic flexibility and spatial structure, enabling reasoning over route feasibility and user preferences. Experiments show that our method reliably grounds textual preferences into constraint-aware routes, improving route quality and preference satisfaction over classical methods.", "link": "https://arxiv.org/abs/2510.06078", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T02:41:23.712040Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05120v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "A Fuzzy Logic-Based Framework for Explainable Machine Learning in Big Data Analytics", "summary": "arXiv:2510.05120v1 Announce Type: new Abstract: The growing complexity of machine learning (ML) models in big data analytics, especially in domains such as environmental monitoring, highlights the critical need for interpretability and explainability to promote trust, ethical considerations, and regulatory adherence (e.g., GDPR). Traditional \"black-box\" models obstruct transparency, whereas post-hoc explainable AI (XAI) techniques like LIME and SHAP frequently compromise accuracy or fail to deliver inherent insights. This paper presents a novel framework that combines type-2 fuzzy sets, granular computing, and clustering to boost explainability and fairness in big data environments. When applied to the UCI Air Quality dataset, the framework effectively manages uncertainty in noisy sensor data, produces linguistic rules, and assesses fairness using silhouette scores and entropy. Key contributions encompass: (1) A type-2 fuzzy clustering approach that enhances cohesion by about 4% compared to type-1 methods (silhouette 0.365 vs. 0.349) and improves fairness (entropy 0.918); (2) Incorporation of fairness measures to mitigate biases in unsupervised scenarios; (3) A rule-based component for intrinsic XAI, achieving an average coverage of 0.65; (4) Scalable assessments showing linear runtime (roughly 0.005 seconds for sampled big data sizes). Experimental outcomes reveal superior performance relative to baselines such as DBSCAN and Agglomerative Clustering in terms of interpretability, fairness, and efficiency. Notably, the proposed method achieves a 4% improvement in silhouette score over type-1 fuzzy clustering and outperforms baselines in fairness (entropy reduction by up to 1%) and efficiency.", "link": "https://arxiv.org/abs/2510.05120", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.228936Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05140v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Auditing Algorithmic Bias in Transformer-Based Trading", "summary": "arXiv:2510.05140v1 Announce Type: new Abstract: Transformer models have become increasingly popular in financial applications, yet their potential risk making and biases remain under-explored. The purpose of this work is to audit the reliance of the model on volatile data for decision-making, and quantify how the frequency of price movements affects the model's prediction confidence. We employ a transformer model for prediction, and introduce a metric based on Partial Information Decomposition (PID) to measure the influence of each asset on the model's decision making. Our analysis reveals two key observations: first, the model disregards data volatility entirely, and second, it is biased toward data with lower-frequency price movements.", "link": "https://arxiv.org/abs/2510.05140", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.228974Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05157v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment", "summary": "arXiv:2510.05157v1 Announce Type: new Abstract: This paper presents a controlled study of adversarial reinforcement learning in network security through a custom OpenAI Gym environment that models brute-force attacks and reactive defenses on multi-port services. The environment captures realistic security trade-offs including background traffic noise, progressive exploitation mechanics, IP-based evasion tactics, honeypot traps, and multi-level rate-limiting defenses. Competing attacker and defender agents are trained using Deep Q-Networks (DQN) within a zero-sum reward framework, where successful exploits yield large terminal rewards while incremental actions incur small costs. Through systematic evaluation across multiple configurations (varying trap detection probabilities, exploitation difficulty thresholds, and training regimens), the results demonstrate that defender observability and trap effectiveness create substantial barriers to successful attacks. The experiments reveal that reward shaping and careful training scheduling are critical for learning stability in this adversarial setting. The defender consistently maintains strategic advantage across 50,000+ training episodes, with performance gains amplifying when exposed to complex defensive strategies including adaptive IP blocking and port-specific controls. Complete implementation details, reproducible hyperparameter configurations, and architectural guidelines are provided to support future research in adversarial RL for cybersecurity. The zero-sum formulation and realistic operational constraints make this environment suitable for studying autonomous defense systems, attacker-defender co-evolution, and transfer learning to real-world network security scenarios.", "link": "https://arxiv.org/abs/2510.05157", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229027Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05160v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Generative Inverse Design: From Single Point Optimization to a Diverse Design Portfolio via Conditional Variational Autoencoders", "summary": "arXiv:2510.05160v1 Announce Type: new Abstract: Inverse design, which seeks to find optimal parameters for a target output, is a central challenge in engineering. Surrogate-based optimization (SBO) has become a standard approach, yet it is fundamentally structured to converge to a single-point solution, thereby limiting design space exploration and ignoring potentially valuable alternative topologies. This paper presents a paradigm shift from single-point optimization to generative inverse design. We introduce a framework based on a Conditional Variational Autoencoder (CVAE) that learns a probabilistic mapping between a system's design parameters and its performance, enabling the generation of a diverse portfolio of high-performing candidates conditioned on a specific performance objective. We apply this methodology to the complex, non-linear problem of minimizing airfoil self-noise, using a high-performing SBO method from a prior benchmark study as a rigorous baseline. The CVAE framework successfully generated 256 novel designs with a 94.1\\% validity rate. A subsequent surrogate-based evaluation revealed that 77.2\\% of these valid designs achieved superior performance compared to the single optimal design found by the SBO baseline. This work demonstrates that the generative approach not only discovers higher-quality solutions but also provides a rich portfolio of diverse candidates, fundamentally enhancing the engineering design process by enabling multi-criteria decision-making.", "link": "https://arxiv.org/abs/2510.05160", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229074Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05167v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Machine learning for fraud detection in digital banking: a systematic literature review REVIEW", "summary": "arXiv:2510.05167v1 Announce Type: new Abstract: This systematic literature review examines the role of machine learning in fraud detection within digital banking, synthesizing evidence from 118 peer-reviewed studies and institutional reports. Following the PRISMA guidelines, the review applied a structured identification, screening, eligibility, and inclusion process to ensure methodological rigor and transparency. The findings reveal that supervised learning methods, such as decision trees, logistic regression, and support vector machines, remain the dominant paradigm due to their interpretability and established performance, while unsupervised anomaly detection approaches are increasingly adopted to address novel fraud patterns in highly imbalanced datasets. Deep learning architectures, particularly recurrent and convolutional neural networks, have emerged as transformative tools capable of modeling sequential transaction data and detecting complex fraud typologies, though challenges of interpretability and real-time deployment persist. Hybrid models that combine supervised, unsupervised, and deep learning strategies demonstrate superior adaptability and detection accuracy, highlighting their potential as convergent solutions.", "link": "https://arxiv.org/abs/2510.05167", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229115Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05168v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Discretized Quadratic Integrate-and-Fire Neuron Model for Deep Spiking Neural Networks", "summary": "arXiv:2510.05168v1 Announce Type: new Abstract: Spiking Neural Networks (SNNs) have emerged as energy-efficient alternatives to traditional artificial neural networks, leveraging asynchronous and biologically inspired neuron dynamics. Among existing neuron models, the Leaky Integrate-and-Fire (LIF) neuron has become widely adopted in deep SNNs due to its simplicity and computational efficiency. However, this efficiency comes at the expense of expressiveness, as LIF dynamics are constrained to linear decay at each timestep. In contrast, more complex models, such as the Quadratic Integrate-and-Fire (QIF) neuron, exhibit richer, nonlinear dynamics but have seen limited adoption due to their training instability. On that note, we propose the first discretization of the QIF neuron model tailored for high-performance deep spiking neural networks and provide an in-depth analysis of its dynamics. To ensure training stability, we derive an analytical formulation for surrogate gradient windows directly from our discretizations' parameter set, minimizing gradient mismatch. We evaluate our method on CIFAR-10, CIFAR-100, ImageNet, and CIFAR-10 DVS, demonstrating its ability to outperform state-of-the-art LIF-based methods. These results establish our discretization of the QIF neuron as a compelling alternative to LIF neurons for deep SNNs, combining richer dynamics with practical scalability.", "link": "https://arxiv.org/abs/2510.05168", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229161Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05171v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Carbon Emission Prediction in China Considering New Quality Productive Forces Using a Deep & Corss Learning Modeling Framework", "summary": "arXiv:2510.05171v1 Announce Type: new Abstract: New quality productive forces (NQPF), digital economy advancement, and artificial intelligence (AI) technologies are becoming crucial for promoting sustainable urban development. This study proposes a Multi-head Attention Deep & Cross Network (MADCN) framework, combining feature interaction modeling and attention mechanisms, to predict urban carbon emissions and investigate the impacts of technological factors. The framework incorporates an interpretable learning phase using SHapley Additive exPlanations (SHAP) to assess the contributions of different features. A panel dataset covering 275 Chinese cities is utilized to test the MADCN model. Experimental results demonstrate that the MADCN model achieves superior predictive performance compared to traditional machine learning and deep learning baselines, with a Mean Squared Error (MSE) of 406,151.063, a Mean Absolute Error (MAE) of 612.304, and an R-squared value of 0.991 on the test set. SHAP analysis highlights that population, city size, urbanization rate, and GDP are among the most influential factors on carbon emissions, while NQPF, digital economy index, and AI technology level also show meaningful but relatively moderate effects. Advancing NQPF, strengthening the digital economy, and accelerating AI technology development can significantly contribute to reducing urban carbon emissions. Policymakers should prioritize integrating technological innovation into carbon reduction strategies, particularly by promoting intelligent infrastructure and enhancing digitalization across sectors, to effectively achieve dual-carbon goals.", "link": "https://arxiv.org/abs/2510.05171", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229214Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05172v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Learning More with Less: A Generalizable, Self-Supervised Framework for Privacy-Preserving Capacity Estimation with EV Charging Data", "summary": "arXiv:2510.05172v1 Announce Type: new Abstract: Accurate battery capacity estimation is key to alleviating consumer concerns about battery performance and reliability of electric vehicles (EVs). However, practical data limitations imposed by stringent privacy regulations and labeled data shortages hamper the development of generalizable capacity estimation models that remain robust to real-world data distribution shifts. While self-supervised learning can leverage unlabeled data, existing techniques are not particularly designed to learn effectively from challenging field data -- let alone from privacy-friendly data, which are often less feature-rich and noisier. In this work, we propose a first-of-its-kind capacity estimation model based on self-supervised pre-training, developed on a large-scale dataset of privacy-friendly charging data snippets from real-world EV operations. Our pre-training framework, snippet similarity-weighted masked input reconstruction, is designed to learn rich, generalizable representations even from less feature-rich and fragmented privacy-friendly data. Our key innovation lies in harnessing contrastive learning to first capture high-level similarities among fragmented snippets that otherwise lack meaningful context. With our snippet-wise contrastive learning and subsequent similarity-weighted masked reconstruction, we are able to learn rich representations of both granular charging patterns within individual snippets and high-level associative relationships across different snippets. Bolstered by this rich representation learning, our model consistently outperforms state-of-the-art baselines, achieving 31.9% lower test error than the best-performing benchmark, even under challenging domain-shifted settings affected by both manufacturer and age-induced distribution shifts.", "link": "https://arxiv.org/abs/2510.05172", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229269Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05175v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Exact Causal Attention with 10% Fewer Operations", "summary": "arXiv:2510.05175v1 Announce Type: new Abstract: We present Fast Causal Attention (FCA), an algorithm that computes exact Causal Attention using 10\\% fewer operations. FCA accelerates a special class of matrix multiplications where either one operand or the output matrix is upper- or lower-triangular. This includes all operations in forward and backward pass of Causal Attention, such as masked product $\\mathrm{Mask}(QK^{T})$. For these matrix multiplications on GPU, FCA reaches noticeable accelerations over the default PyTorch implementations and Triton compiled kernels. FCA is built upon algebraic identities discovered via machine learning and combinatorial search.", "link": "https://arxiv.org/abs/2510.05175", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229300Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05178v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Logistic-Gated Operators Enable Auditable Unit-Aware Thresholds in Symbolic Regression", "summary": "arXiv:2510.05178v1 Announce Type: new Abstract: Symbolic regression promises readable equations but struggles to encode unit-aware thresholds and conditional logic. We propose logistic-gated operators (LGO) -- differentiable gates with learnable location and steepness -- embedded as typed primitives and mapped back to physical units for audit. Across two primary health datasets (ICU, NHANES), the hard-gate variant recovers clinically plausible cut-points: 71% (5/7) of assessed thresholds fall within 10% of guideline anchors and 100% within 20%, while using far fewer gates than the soft variant (ICU median 4.0 vs 10.0; NHANES 5.0 vs 12.5), and remaining within the competitive accuracy envelope of strong SR baselines. On predominantly smooth tasks, gates are pruned, preserving parsimony. The result is compact symbolic equations with explicit, unit-aware thresholds that can be audited against clinical anchors -- turning interpretability from a post-hoc explanation into a modeling constraint and equipping symbolic regression with a practical calculus for regime switching and governance-ready deployment.", "link": "https://arxiv.org/abs/2510.05178", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229404Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05180v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "OptiFLIDS: Optimized Federated Learning for Energy-Efficient Intrusion Detection in IoT", "summary": "arXiv:2510.05180v1 Announce Type: new Abstract: In critical IoT environments, such as smart homes and industrial systems, effective Intrusion Detection Systems (IDS) are essential for ensuring security. However, developing robust IDS solutions remains a significant challenge. Traditional machine learning-based IDS models typically require large datasets, but data sharing is often limited due to privacy and security concerns. Federated Learning (FL) presents a promising alternative by enabling collaborative model training without sharing raw data. Despite its advantages, FL still faces key challenges, such as data heterogeneity (non-IID data) and high energy and computation costs, particularly for resource constrained IoT devices. To address these issues, this paper proposes OptiFLIDS, a novel approach that applies pruning techniques during local training to reduce model complexity and energy consumption. It also incorporates a customized aggregation method to better handle pruned models that differ due to non-IID data distributions. Experiments conducted on three recent IoT IDS datasets, TON_IoT, X-IIoTID, and IDSIoT2024, demonstrate that OptiFLIDS maintains strong detection performance while improving energy efficiency, making it well-suited for deployment in real-world IoT environments.", "link": "https://arxiv.org/abs/2510.05180", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229448Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05218v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Approximate Gaussianity Beyond Initialisation in Neural Networks", "summary": "arXiv:2510.05218v1 Announce Type: new Abstract: Ensembles of neural network weight matrices are studied through the training process for the MNIST classification problem, testing the efficacy of matrix models for representing their distributions, under assumptions of Gaussianity and permutation-symmetry. The general 13-parameter permutation invariant Gaussian matrix models are found to be effective models for the correlated Gaussianity in the weight matrices, beyond the range of applicability of the simple Gaussian with independent identically distributed matrix variables, and notably well beyond the initialisation step. The representation theoretic model parameters, and the graph-theoretic characterisation of the permutation invariant matrix observables give an interpretable framework for the best-fit model and for small departures from Gaussianity. Additionally, the Wasserstein distance is calculated for this class of models and used to quantify the movement of the distributions over training. Throughout the work, the effects of varied initialisation regimes, regularisation, layer depth, and layer width are tested for this formalism, identifying limits where particular departures from Gaussianity are enhanced and how more general, yet still highly-interpretable, models can be developed.", "link": "https://arxiv.org/abs/2510.05218", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229543Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05228v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "CMT-Benchmark: A Benchmark for Condensed Matter Theory Built by Expert Researchers", "summary": "arXiv:2510.05228v1 Announce Type: new Abstract: Large language models (LLMs) have shown remarkable progress in coding and math problem-solving, but evaluation on advanced research-level problems in hard sciences remains scarce. To fill this gap, we present CMT-Benchmark, a dataset of 50 problems covering condensed matter theory (CMT) at the level of an expert researcher. Topics span analytical and computational approaches in quantum many-body, and classical statistical mechanics. The dataset was designed and verified by a panel of expert researchers from around the world. We built the dataset through a collaborative environment that challenges the panel to write and refine problems they would want a research assistant to solve, including Hartree-Fock, exact diagonalization, quantum/variational Monte Carlo, density matrix renormalization group (DMRG), quantum/classical statistical mechanics, and model building. We evaluate LLMs by programmatically checking solutions against expert-supplied ground truth. We developed machine-grading, including symbolic handling of non-commuting operators via normal ordering. They generalize across tasks too. Our evaluations show that frontier models struggle with all of the problems in the dataset, highlighting a gap in the physical reasoning skills of current LLMs. Notably, experts identified strategies for creating increasingly difficult problems by interacting with the LLMs and exploiting common failure modes. The best model, GPT5, solves 30\\% of the problems; average across 17 models (GPT, Gemini, Claude, DeepSeek, Llama) is 11.4$\\pm$2.1\\%. Moreover, 18 problems are solved by none of the 17 models, and 26 by at most one. These unsolved problems span Quantum Monte Carlo, Variational Monte Carlo, and DMRG. Answers sometimes violate fundamental symmetries or have unphysical scaling dimensions. We believe this benchmark will guide development toward capable AI research assistants and tutors.", "link": "https://arxiv.org/abs/2510.05228", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229609Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05241v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Simultaneous Learning and Optimization via Misspecified Saddle Point Problems", "summary": "arXiv:2510.05241v1 Announce Type: new Abstract: We study a class of misspecified saddle point (SP) problems, where the optimization objective depends on an unknown parameter that must be learned concurrently from data. Unlike existing studies that assume parameters are fully known or pre-estimated, our framework integrates optimization and learning into a unified formulation, enabling a more flexible problem class. To address this setting, we propose two algorithms based on the accelerated primal-dual (APD) by Hamedani & Aybat 2021. In particular, we first analyze the naive extension of the APD method by directly substituting the evolving parameter estimates into the primal-dual updates; then, we design a new learning-aware variant of the APD method that explicitly accounts for parameter dynamics by adjusting the momentum updates. Both methods achieve a provable convergence rate of $\\mathcal{O}(\\log K / K)$, while the learning-aware approach attains a tighter $\\mathcal{O}(1)$ constant and further benefits from an adaptive step-size selection enabled by a backtracking strategy. Furthermore, we extend the framework to problems where the learning problem admits multiple optimal solutions, showing that our modified algorithm for a structured setting achieves an $\\mathcal{O}(1/\\sqrt{K})$ rate. To demonstrate practical impact, we evaluate our methods on a misspecified portfolio optimization problem and show superior empirical performance compared to state-of-the-art algorithms.", "link": "https://arxiv.org/abs/2510.05241", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229654Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05261v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "ECLipsE-Gen-Local: Efficient Compositional Local Lipschitz Estimates for Deep Neural Networks", "summary": "arXiv:2510.05261v1 Announce Type: new Abstract: The Lipschitz constant is a key measure for certifying the robustness of neural networks to input perturbations. However, computing the exact constant is NP-hard, and standard approaches to estimate the Lipschitz constant involve solving a large matrix semidefinite program (SDP) that scales poorly with network size. Further, there is a potential to efficiently leverage local information on the input region to provide tighter Lipschitz estimates. We address this problem here by proposing a compositional framework that yields tight yet scalable Lipschitz estimates for deep feedforward neural networks. Specifically, we begin by developing a generalized SDP framework that is highly flexible, accommodating heterogeneous activation function slope, and allowing Lipschitz estimates with respect to arbitrary input-output pairs and arbitrary choices of sub-networks of consecutive layers. We then decompose this generalized SDP into a sequence of small sub-problems, with computational complexity that scales linearly with respect to the network depth. We also develop a variant that achieves near-instantaneous computation through closed-form solutions to each sub-problem. All our algorithms are accompanied by theoretical guarantees on feasibility and validity. Next, we develop a series of algorithms, termed as ECLipsE-Gen-Local, that effectively incorporate local information on the input. Our experiments demonstrate that our algorithms achieve substantial speedups over a multitude of benchmarks while producing significantly tighter Lipschitz bounds than global approaches. Moreover, we show that our algorithms provide strict upper bounds for the Lipschitz constant with values approaching the exact Jacobian from autodiff when the input region is small enough. Finally, we demonstrate the practical utility of our approach by showing that our Lipschitz estimates closely align with network robustness.", "link": "https://arxiv.org/abs/2510.05261", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229714Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05278v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Decoding Partial Differential Equations: Cross-Modal Adaptation of Decoder-only Models to PDEs", "summary": "arXiv:2510.05278v1 Announce Type: new Abstract: Large language models have shown great success on natural language tasks in recent years, but they have also shown great promise when adapted to new modalities, e.g., for scientific machine learning tasks. Even though decoder-only models are more popular within NLP and scale exceedingly well at generating natural language, most proposed approaches for cross-modal adaptation focus on encoder-only models, raising the question of how model architecture affects these approaches. In this paper, we therefore perform a series of ablation studies to answer this question, systematically comparing encoder-only and decoder-only models on cross-modal adaptation for time-dependent simulation tasks based on partial differential equations (PDEs). We find that decoder-only models are far worse than encoder-only models, when existing approaches are applied unmodified. In contrast to several other domains, scaling decoder-only models also does not help. To harness the potential of decoder-only models in this context, we introduce two novel approaches, Parallel Flipping and Sequence Doubling, attempting to mimic bidirectionality in autoregressive models. Both our methods improve overall performance using decoder-only models for all tasks and all cross-model adaptation methods, closing the gap to encoder-only model performance. We hope that our findings broaden the spectrum of models used on cross-modal adaptation tasks to further scientific ML.", "link": "https://arxiv.org/abs/2510.05278", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229758Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05286v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Computing frustration and near-monotonicity in deep neural networks", "summary": "arXiv:2510.05286v1 Announce Type: new Abstract: For the signed graph associated to a deep neural network, one can compute the frustration level, i.e., test how close or distant the graph is to structural balance. For all the pretrained deep convolutional neural networks we consider, we find that the frustration is always less than expected from null models. From a statistical physics point of view, and in particular in reference to an Ising spin glass model, the reduced frustration indicates that the amount of disorder encoded in the network is less than in the null models. From a functional point of view, low frustration (i.e., proximity to structural balance) means that the function representing the network behaves near-monotonically, i.e., more similarly to a monotone function than in the null models. Evidence of near-monotonic behavior along the partial order determined by frustration is observed for all networks we consider. This confirms that the class of deep convolutional neural networks tends to have a more ordered behavior than expected from null models, and suggests a novel form of implicit regularization.", "link": "https://arxiv.org/abs/2510.05286", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229846Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05288v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language Models Using Adam Optimization with Adaptive Clipping", "summary": "arXiv:2510.05288v1 Announce Type: new Abstract: Large language models (LLMs) such as ChatGPT have evolved into powerful and ubiquitous tools. Fine-tuning on small datasets allows LLMs to acquire specialized skills for specific tasks efficiently. Although LLMs provide great utility in both general and task-specific use cases, they are limited by two security-related concerns. First, traditional LLM hardware requirements make them infeasible to run locally on consumer-grade devices. A remote network connection with the LLM provider's server is usually required, making the system vulnerable to network attacks. Second, fine-tuning an LLM for a sensitive task may involve sensitive data. Non-private fine-tuning algorithms produce models vulnerable to training data reproduction attacks. Our work addresses these security concerns by enhancing differentially private optimization algorithms and applying them to fine-tune localizable language models. We introduce adaptable gradient clipping along with other engineering enhancements to the standard DP-Adam optimizer to create DP-Adam-AC. We use our optimizer to fine-tune examples of two localizable LLM designs, small language model (Qwen2.5-0.5B) and 1.58 bit quantization (Bitnet-b1.58-2B). We demonstrate promising improvements in loss through experimentation with two synthetic datasets.", "link": "https://arxiv.org/abs/2510.05288", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229890Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05317v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "RegMix: Adversarial Mutual and Generalization Regularization for Enhancing DNN Robustness", "summary": "arXiv:2510.05317v1 Announce Type: new Abstract: Adversarial training is the most effective defense against adversarial attacks. The effectiveness of the adversarial attacks has been on the design of its loss function and regularization term. The most widely used loss function in adversarial training is cross-entropy and mean squared error (MSE) as its regularization objective. However, MSE enforces overly uniform optimization between two output distributions during training, which limits its robustness in adversarial training scenarios. To address this issue, we revisit the idea of mutual learning (originally designed for knowledge distillation) and propose two novel regularization strategies tailored for adversarial training: (i) weighted adversarial mutual regularization and (ii) adversarial generalization regularization. In the former, we formulate a decomposed adversarial mutual Kullback-Leibler divergence (KL-divergence) loss, which allows flexible control over the optimization process by assigning unequal weights to the main and auxiliary objectives. In the latter, we introduce an additional clean target distribution into the adversarial training objective, improving generalization and enhancing model robustness. Extensive experiments demonstrate that our proposed methods significantly improve adversarial robustness compared to existing regularization-based approaches.", "link": "https://arxiv.org/abs/2510.05317", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.229966Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05329v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Tensor-on-tensor Regression Neural Networks for Process Modeling with High-dimensional Data", "summary": "arXiv:2510.05329v1 Announce Type: new Abstract: Modern sensing and metrology systems now stream terabytes of heterogeneous, high-dimensional (HD) data profiles, images, and dense point clouds, whose natural representation is multi-way tensors. Understanding such data requires regression models that preserve tensor geometry, yet remain expressive enough to capture the pronounced nonlinear interactions that dominate many industrial and mechanical processes. Existing tensor-based regressors meet the first requirement but remain essentially linear. Conversely, conventional neural networks offer nonlinearity only after flattening, thereby discarding spatial structure and incurring prohibitive parameter counts. This paper introduces a Tensor-on-Tensor Regression Neural Network (TRNN) that unifies these two paradigms.", "link": "https://arxiv.org/abs/2510.05329", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230001Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05342v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization", "summary": "arXiv:2510.05342v1 Announce Type: new Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of $\\beta$-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative $\\beta$ values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\\% on High Quality data and +10.5\\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.", "link": "https://arxiv.org/abs/2510.05342", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230057Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05351v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Physics-informed Attention-enhanced Fourier Neural Operator for Solar Magnetic Field Extrapolations", "summary": "arXiv:2510.05351v1 Announce Type: new Abstract: We propose Physics-informed Attention-enhanced Fourier Neural Operator (PIANO) to solve the Nonlinear Force-Free Field (NLFFF) problem in solar physics. Unlike conventional approaches that rely on iterative numerical methods, our proposed PIANO directly learns the 3D magnetic field structure from 2D boundary conditions. Specifically, PIANO integrates Efficient Channel Attention (ECA) mechanisms with Dilated Convolutions (DC), which enhances the model's ability to capture multimodal input by prioritizing critical channels relevant to the magnetic field's variations. Furthermore, we apply physics-informed loss by enforcing the force-free and divergence-free conditions in the training process so that our prediction is consistent with underlying physics with high accuracy. Experimental results on the ISEE NLFFF dataset show that our PIANO not only outperforms state-of-the-art neural operators in terms of accuracy but also shows strong consistency with the physical characteristics of NLFFF data across magnetic fields reconstructed from various solar active regions. The GitHub of this project is available https://github.com/Autumnstar-cjh/PIANO", "link": "https://arxiv.org/abs/2510.05351", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230096Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05385v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Physics-Informed Neural Networks with Fourier Features and Attention-Driven Decoding", "summary": "arXiv:2510.05385v1 Announce Type: new Abstract: Physics-Informed Neural Networks (PINNs) are a useful framework for approximating partial differential equation solutions using deep learning methods. In this paper, we propose a principled redesign of the PINNsformer, a Transformer-based PINN architecture. We present the Spectral PINNSformer (S-Pformer), a refinement of encoder-decoder PINNSformers that addresses two key issues; 1. the redundancy (i.e. increased parameter count) of the encoder, and 2. the mitigation of spectral bias. We find that the encoder is unnecessary for capturing spatiotemporal correlations when relying solely on self-attention, thereby reducing parameter count. Further, we integrate Fourier feature embeddings to explicitly mitigate spectral bias, enabling adaptive encoding of multiscale behaviors in the frequency domain. Our model outperforms encoder-decoder PINNSformer architectures across all benchmarks, achieving or outperforming MLP performance while reducing parameter count significantly.", "link": "https://arxiv.org/abs/2510.05385", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230228Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05386v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "A Neural Network Algorithm for KL Divergence Estimation with Quantitative Error Bounds", "summary": "arXiv:2510.05386v1 Announce Type: new Abstract: Estimating the Kullback-Leibler (KL) divergence between random variables is a fundamental problem in statistical analysis. For continuous random variables, traditional information-theoretic estimators scale poorly with dimension and/or sample size. To mitigate this challenge, a variety of methods have been proposed to estimate KL divergences and related quantities, such as mutual information, using neural networks. The existing theoretical analyses show that neural network parameters achieving low error exist. However, since they rely on non-constructive neural network approximation theorems, they do not guarantee that the existing algorithms actually achieve low error. In this paper, we propose a KL divergence estimation algorithm using a shallow neural network with randomized hidden weights and biases (i.e. a random feature method). We show that with high probability, the algorithm achieves a KL divergence estimation error of $O(m^{-1/2}+T^{-1/3})$, where $m$ is the number of neurons and $T$ is both the number of steps of the algorithm and the number of samples.", "link": "https://arxiv.org/abs/2510.05386", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230267Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05394v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Fusion-Based Neural Generalization for Predicting Temperature Fields in Industrial PET Preform Heating", "summary": "arXiv:2510.05394v1 Announce Type: new Abstract: Accurate and efficient temperature prediction is critical for optimizing the preheating process of PET preforms in industrial microwave systems prior to blow molding. We propose a novel deep learning framework for generalized temperature prediction. Unlike traditional models that require extensive retraining for each material or design variation, our method introduces a data-efficient neural architecture that leverages transfer learning and model fusion to generalize across unseen scenarios. By pretraining specialized neural regressor on distinct conditions such as recycled PET heat capacities or varying preform geometries and integrating their representations into a unified global model, we create a system capable of learning shared thermal dynamics across heterogeneous inputs. The architecture incorporates skip connections to enhance stability and prediction accuracy. Our approach reduces the need for large simulation datasets while achieving superior performance compared to models trained from scratch. Experimental validation on two case studies material variability and geometric diversity demonstrates significant improvements in generalization, establishing a scalable ML-based solution for intelligent thermal control in manufacturing environments. Moreover, the approach highlights how data-efficient generalization strategies can extend to other industrial applications involving complex physical modeling with limited data.", "link": "https://arxiv.org/abs/2510.05394", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230311Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05399v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Comparing LSTM-Based Sequence-to-Sequence Forecasting Strategies for 24-Hour Solar Proton Flux Profiles Using GOES Data", "summary": "arXiv:2510.05399v1 Announce Type: new Abstract: Solar Proton Events (SPEs) cause significant radiation hazards to satellites, astronauts, and technological systems. Accurate forecasting of their proton flux time profiles is crucial for early warnings and mitigation. This paper explores deep learning sequence-to-sequence (seq2seq) models based on Long Short-Term Memory networks to predict 24-hour proton flux profiles following SPE onsets. We used a dataset of 40 well-connected SPEs (1997-2017) observed by NOAA GOES, each associated with a >=M-class western-hemisphere solar flare and undisturbed proton flux profiles. Using 4-fold stratified cross-validation, we evaluate seq2seq model configurations (varying hidden units and embedding dimensions) under multiple forecasting scenarios: (i) proton-only input vs. combined proton+X-ray input, (ii) original flux data vs. trend-smoothed data, and (iii) autoregressive vs. one-shot forecasting. Our major results are as follows: First, one-shot forecasting consistently yields lower error than autoregressive prediction, avoiding the error accumulation seen in iterative approaches. Second, on the original data, proton-only models outperform proton+X-ray models. However, with trend-smoothed data, this gap narrows or reverses in proton+X-ray models. Third, trend-smoothing significantly enhances the performance of proton+X-ray models by mitigating fluctuations in the X-ray channel. Fourth, while models trained on trendsmoothed data perform best on average, the best-performing model was trained on original data, suggesting that architectural choices can sometimes outweigh the benefits of data preprocessing.", "link": "https://arxiv.org/abs/2510.05399", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230360Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05416v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Correlating Cross-Iteration Noise for DP-SGD using Model Curvature", "summary": "arXiv:2510.05416v1 Announce Type: new Abstract: Differentially private stochastic gradient descent (DP-SGD) offers the promise of training deep learning models while mitigating many privacy risks. However, there is currently a large accuracy gap between DP-SGD and normal SGD training. This has resulted in different lines of research investigating orthogonal ways of improving privacy-preserving training. One such line of work, known as DP-MF, correlates the privacy noise across different iterations of stochastic gradient descent -- allowing later iterations to cancel out some of the noise added to earlier iterations. In this paper, we study how to improve this noise correlation. We propose a technique called NoiseCurve that uses model curvature, estimated from public unlabeled data, to improve the quality of this cross-iteration noise correlation. Our experiments on various datasets, models, and privacy parameters show that the noise correlations computed by NoiseCurve offer consistent and significant improvements in accuracy over the correlation scheme used by DP-MF.", "link": "https://arxiv.org/abs/2510.05416", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230397Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05421v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Draft, Verify, and Improve: Toward Training-Aware Speculative Decoding", "summary": "arXiv:2510.05421v1 Announce Type: new Abstract: Autoregressive (AR) decoding is a major latency bottleneck for large language models. Speculative decoding (SD) accelerates AR by letting a drafter propose multi-token blocks that a verifier accepts or rejects. However, many SD systems require heavy offline training or extra components. These choices raise data/compute cost and can yield brittle drafters under distribution drift. We introduce \\emph{Draft, Verify, \\& Improve (DVI)}, a training-aware self-speculative framework that combines inference with continual online learning. We partition an LLM into a drafter and a verifier, and during generation, verifier accept/reject decisions are converted into supervision signals and used to update the drafter head. A simple \\emph{KL$\\rightarrow$RL} schedule bootstraps calibration via online distillation and then adds reward-masked cross-entropy with a on-policy policy-gradient term, preserving lossless, single model deployment. On Spec-Bench, DVI achieves a $2.16\\times$ wall-time speedup, on par with SoTA approaches like EAGLE-2, while orders of magnitude less data for training, and ablations show that DVI outperforms KL-only online distillation. DVI demonstrates that \\emph{training-aware} self-speculation can deliver state-of-the-art, lossless speedups with minimal training overhead.", "link": "https://arxiv.org/abs/2510.05421", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230444Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05433v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Physics-Informed Machine Learning in Biomedical Science and Engineering", "summary": "arXiv:2510.05433v1 Announce Type: new Abstract: Physics-informed machine learning (PIML) is emerging as a potentially transformative paradigm for modeling complex biomedical systems by integrating parameterized physical laws with data-driven methods. Here, we review three main classes of PIML frameworks: physics-informed neural networks (PINNs), neural ordinary differential equations (NODEs), and neural operators (NOs), highlighting their growing role in biomedical science and engineering. We begin with PINNs, which embed governing equations into deep learning models and have been successfully applied to biosolid and biofluid mechanics, mechanobiology, and medical imaging among other areas. We then review NODEs, which offer continuous-time modeling, especially suited to dynamic physiological systems, pharmacokinetics, and cell signaling. Finally, we discuss deep NOs as powerful tools for learning mappings between function spaces, enabling efficient simulations across multiscale and spatially heterogeneous biological domains. Throughout, we emphasize applications where physical interpretability, data scarcity, or system complexity make conventional black-box learning insufficient. We conclude by identifying open challenges and future directions for advancing PIML in biomedical science and engineering, including issues of uncertainty quantification, generalization, and integration of PIML and large language models.", "link": "https://arxiv.org/abs/2510.05433", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230485Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05442v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Adversarial Reinforcement Learning for Large Language Model Agent Safety", "summary": "arXiv:2510.05442v1 Announce Type: new Abstract: Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks. However, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage. Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. To address this limitation, we propose Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks. To ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints. Evaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate. Our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.", "link": "https://arxiv.org/abs/2510.05442", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230532Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05446v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Prior-Aligned Meta-RL: Thompson Sampling with Learned Priors and Guarantees in Finite-Horizon MDPs", "summary": "arXiv:2510.05446v1 Announce Type: new Abstract: We study meta-reinforcement learning in finite-horizon MDPs where related tasks share similar structures in their optimal action-value functions. Specifically, we posit a linear representation $Q^*_h(s,a)=\\Phi_h(s,a)\\,\\theta^{(k)}_h$ and place a Gaussian meta-prior $ \\mathcal{N}(\\theta^*_h,\\Sigma^*_h)$ over the task-specific parameters $\\theta^{(k)}_h$. Building on randomized value functions, we propose two Thompson-style algorithms: (i) MTSRL, which learns only the prior mean and performs posterior sampling with the learned mean and known covariance; and (ii) $\\text{MTSRL}^{+}$, which additionally estimates the covariance and employs prior widening to control finite-sample estimation error. Further, we develop a prior-alignment technique that couples the posterior under the learned prior with a meta-oracle that knows the true prior, yielding meta-regret guarantees: we match prior-independent Thompson sampling in the small-task regime and strictly improve with more tasks once the prior is learned. Concretely, for known covariance we obtain $\\tilde{O}(H^{4}S^{3/2}\\sqrt{ANK})$ meta-regret, and with learned covariance $\\tilde{O}(H^{4}S^{3/2}\\sqrt{AN^3K})$; both recover a better behavior than prior-independent after $K \\gtrsim \\tilde{O}(H^2)$ and $K \\gtrsim \\tilde{O}(N^2H^2)$, respectively. Simulations on a stateful recommendation environment (with feature and prior misspecification) show that after brief exploration, MTSRL/MTSRL\\(^+\\) track the meta-oracle and substantially outperform prior-independent RL and bandit-only meta-baselines. Our results give the first meta-regret guarantees for Thompson-style RL with learned Q-priors, and provide practical recipes (warm-start via RLSVI, OLS aggregation, covariance widening) for experiment-rich settings.", "link": "https://arxiv.org/abs/2510.05446", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230585Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05453v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "QDeepGR4J: Quantile-based ensemble of deep learning and GR4J hybrid rainfall-runoff models for extreme flow prediction with uncertainty quantification", "summary": "arXiv:2510.05453v1 Announce Type: new Abstract: Conceptual rainfall-runoff models aid hydrologists and climate scientists in modelling streamflow to inform water management practices. Recent advances in deep learning have unravelled the potential for combining hydrological models with deep learning models for better interpretability and improved predictive performance. In our previous work, we introduced DeepGR4J, which enhanced the GR4J conceptual rainfall-runoff model using a deep learning model to serve as a surrogate for the routing component. DeepGR4J had an improved rainfall-runoff prediction accuracy, particularly in arid catchments. Quantile regression models have been extensively used for quantifying uncertainty while aiding extreme value forecasting. In this paper, we extend DeepGR4J using a quantile regression-based ensemble learning framework to quantify uncertainty in streamflow prediction. We also leverage the uncertainty bounds to identify extreme flow events potentially leading to flooding. We further extend the model to multi-step streamflow predictions for uncertainty bounds. We design experiments for a detailed evaluation of the proposed framework using the CAMELS-Aus dataset. The results show that our proposed Quantile DeepGR4J framework improves the predictive accuracy and uncertainty interval quality (interval score) compared to baseline deep learning models. Furthermore, we carry out flood risk evaluation using Quantile DeepGR4J, and the results demonstrate its suitability as an early warning system.", "link": "https://arxiv.org/abs/2510.05453", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230634Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05482v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "ATOM: A Pretrained Neural Operator for Multitask Molecular Dynamics", "summary": "arXiv:2510.05482v1 Announce Type: new Abstract: Molecular dynamics (MD) simulations underpin modern computational drug dis- covery, materials science, and biochemistry. Recent machine learning models provide high-fidelity MD predictions without the need to repeatedly solve quantum mechanical forces, enabling significant speedups over conventional pipelines. Yet many such methods typically enforce strict equivariance and rely on sequential rollouts, thus limiting their flexibility and simulation efficiency. They are also com- monly single-task, trained on individual molecules and fixed timeframes, which restricts generalization to unseen compounds and extended timesteps. To address these issues, we propose Atomistic Transformer Operator for Molecules (ATOM), a pretrained transformer neural operator for multitask molecular dynamics. ATOM adopts a quasi-equivariant design that requires no explicit molecular graph and employs a temporal attention mechanism, allowing for the accurate parallel decod- ing of multiple future states. To support operator pretraining across chemicals and timescales, we curate TG80, a large, diverse, and numerically stable MD dataset with over 2.5 million femtoseconds of trajectories across 80 compounds. ATOM achieves state-of-the-art performance on established single-task benchmarks, such as MD17, RMD17 and MD22. After multitask pretraining on TG80, ATOM shows exceptional zero-shot generalization to unseen molecules across varying time hori- zons. We believe ATOM represents a significant step toward accurate, efficient, and transferable molecular dynamics models", "link": "https://arxiv.org/abs/2510.05482", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230743Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05489v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "The Method of Infinite Descent", "summary": "arXiv:2510.05489v1 Announce Type: new Abstract: Training - the optimisation of complex models - is traditionally performed through small, local, iterative updates [D. E. Rumelhart, G. E. Hinton, R. J. Williams, Nature 323, 533-536 (1986)]. Approximating solutions through truncated gradients is a paradigm dating back to Cauchy [A.-L. Cauchy, Comptes Rendus Math\\'ematique 25, 536-538 (1847)] and Newton [I. Newton, The Method of Fluxions and Infinite Series (Henry Woodfall, London, 1736)]. This work introduces the Method of Infinite Descent, a semi-analytic optimisation paradigm that reformulates training as the direct solution to the first-order optimality condition. By analytical resummation of its Taylor expansion, this method yields an exact, algebraic equation for the update step. Realisation of the infinite Taylor tower's cascading resummation is formally derived, and an exploitative algorithm for the direct solve step is proposed. This principle is demonstrated with the herein-introduced AION (Analytic, Infinitely-Optimisable Network) architecture. AION is a model designed expressly to satisfy the algebraic closure required by Infinite Descent. In a simple test problem, AION reaches the optimum in a single descent step. Together, this optimiser-model pair exemplify how analytic structure enables exact, non-iterative convergence. Infinite Descent extends beyond this example, applying to any appropriately closed architecture. This suggests a new class of semi-analytically optimisable models: the \\emph{Infinity Class}; sufficient conditions for class membership are discussed. This offers a pathway toward non-iterative learning.", "link": "https://arxiv.org/abs/2510.05489", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230793Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05491v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "NorMuon: Making Muon more efficient and scalable", "summary": "arXiv:2510.05491v1 Announce Type: new Abstract: The choice of optimizer significantly impacts the training efficiency and computational costs of large language models (LLMs). Recently, the Muon optimizer has demonstrated promising results by orthogonalizing parameter updates, improving optimization geometry through better conditioning. Despite Muon's emergence as a candidate successor to Adam, the potential for jointly leveraging their strengths has not been systematically explored. In this work, we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an optimizer that synergistically combines orthogonalization with neuron-level adaptive learning rates. Our analysis reveals that while Muon effectively reduces condition numbers, the resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate the optimization process. NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron and applying row-wise normalization after orthogonalization, ensuring balanced parameter utilization while preserving Muon's conditioning benefits. To enable practical deployment at scale, we develop an efficient distributed implementation under the FSDP2 framework that strategically distributes orthogonalization computations across devices. Experiments across multiple model scales demonstrate that NorMuon consistently outperforms both Adam and Muon, achieving 21.74% better training efficiency than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while maintaining a comparable memory footprint to Muon. Our findings suggest that orthogonalization and adaptive learning rates are complementary rather than competing approaches, opening new avenues for optimizer design in large-scale deep learning.", "link": "https://arxiv.org/abs/2510.05491", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230846Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05492v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training", "summary": "arXiv:2510.05492v1 Announce Type: new Abstract: The development of machine learning for cardiac care is severely hampered by privacy restrictions on sharing real patient electrocardiogram (ECG) data. Although generative AI offers a promising solution, the real-world use of existing model-synthesized ECGs is limited by persistent gaps in trustworthiness and clinical utility. In this work, we address two major shortcomings of current generative ECG methods: insufficient morphological fidelity and the inability to generate personalized, patient-specific physiological signals. To address these gaps, we build on a conditional diffusion-based Structured State Space Model (SSSD-ECG) with two principled innovations: (1) MIDT-ECG (Mel-Spectrogram Informed Diffusion Training), a novel training paradigm with time-frequency domain supervision to enforce physiological structural realism, and (2) multi-modal demographic conditioning to enable patient-specific synthesis. We comprehensively evaluate our approach on the PTB-XL dataset, assessing the synthesized ECG signals on fidelity, clinical coherence, privacy preservation, and downstream task utility. MIDT-ECG achieves substantial gains: it improves morphological coherence, preserves strong privacy guarantees with all metrics evaluated exceeding the baseline by 4-8%, and notably reduces the interlead correlation error by an average of 74%, while demographic conditioning enhances signal-to-noise ratio and personalization. In critical low-data regimes, a classifier trained on datasets supplemented with our synthetic ECGs achieves performance comparable to a classifier trained solely on real data. Together, we demonstrate that ECG synthesizers, trained with the proposed time-frequency structural regularization scheme, can serve as personalized, high-fidelity, privacy-preserving surrogates when real data are scarce, advancing the responsible use of generative AI in healthcare.", "link": "https://arxiv.org/abs/2510.05492", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230900Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05494v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Fundamental Limits of Crystalline Equivariant Graph Neural Networks: A Circuit Complexity Perspective", "summary": "arXiv:2510.05494v1 Announce Type: new Abstract: Graph neural networks (GNNs) have become a core paradigm for learning on relational data. In materials science, equivariant GNNs (EGNNs) have emerged as a compelling backbone for crystalline-structure prediction, owing to their ability to respect Euclidean symmetries and periodic boundary conditions. Despite strong empirical performance, their expressive power in periodic, symmetry-constrained settings remains poorly understood. This work characterizes the intrinsic computational and expressive limits of EGNNs for crystalline-structure prediction through a circuit-complexity lens. We analyze the computations carried out by EGNN layers acting on node features, atomic coordinates, and lattice matrices, and prove that, under polynomial precision, embedding width $d=O(n)$ for $n$ nodes, $O(1)$ layers, and $O(1)$-depth, $O(n)$-width MLP instantiations of the message/update/readout maps, these models admit a simulation by a uniform $\\mathsf{TC}^0$ threshold-circuit family of polynomial size (with an explicit constant-depth bound). Situating EGNNs within $\\mathsf{TC}^0$ provides a concrete ceiling on the decision and prediction problems solvable by such architectures under realistic resource constraints and clarifies which architectural modifications (e.g., increased depth, richer geometric primitives, or wider layers) are required to transcend this regime. The analysis complements Weisfeiler-Lehman style results that do not directly transfer to periodic crystals, and offers a complexity-theoretic foundation for symmetry-aware graph learning on crystalline systems.", "link": "https://arxiv.org/abs/2510.05494", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230945Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05511v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "EEG-Based Acute Pain Classification: Machine Learning Model Comparison and Real-Time Clinical Feasibility", "summary": "arXiv:2510.05511v1 Announce Type: new Abstract: Current pain assessment within hospitals often relies on self-reporting or non-specific EKG vital signs. This system leaves critically ill, sedated, and cognitively impaired patients vulnerable to undertreated pain and opioid overuse. Electroencephalography (EEG) offers a noninvasive method of measuring brain activity. This technology could potentially be applied as an assistive tool to highlight nociceptive processing in order to mitigate this issue. In this study, we compared machine learning models for classifying high-pain versus low/no-pain EEG epochs using data from fifty-two healthy adults exposed to laser-evoked pain at three intensities (low, medium, high). Each four-second epoch was transformed into a 537-feature vector spanning spectral power, band ratios, Hjorth parameters, entropy measures, coherence, wavelet energies, and peak-frequency metrics. Nine traditional machine learning models were evaluated with leave-one-participant-out cross-validation. A support vector machine with radial basis function kernel achieved the best offline performance with 88.9% accuracy and sub-millisecond inference time (1.02 ms). Our Feature importance analysis was consistent with current canonical pain physiology, showing contralateral alpha suppression, midline theta/alpha enhancement, and frontal gamma bursts. The real-time XGBoost model maintained an end-to-end latency of about 4 ms and 94.2% accuracy, demonstrating that an EEG-based pain monitor is technically feasible within a clinical setting and provides a pathway towards clinical validation.", "link": "https://arxiv.org/abs/2510.05511", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.230990Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05516v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "NeST-BO: Fast Local Bayesian Optimization via Newton-Step Targeting of Gradient and Hessian Information", "summary": "arXiv:2510.05516v1 Announce Type: new Abstract: Bayesian optimization (BO) is effective for expensive black-box problems but remains challenging in high dimensions. We propose NeST-BO, a local BO method that targets the Newton step by jointly learning gradient and Hessian information with Gaussian process surrogates, and selecting evaluations via a one-step lookahead bound on Newton-step error. We show that this bound (and hence the step error) contracts with batch size, so NeST-BO directly inherits inexact-Newton convergence: global progress under mild stability assumptions and quadratic local rates once steps are sufficiently accurate. To scale, we optimize the acquisition in low-dimensional subspaces (e.g., random embeddings or learned sparse subspaces), reducing the dominant cost of learning curvature from $O(d^2)$ to $O(m^2)$ with $m \\ll d$ while preserving step targeting. Across high-dimensional synthetic and real-world problems, including cases with thousands of variables and unknown active subspaces, NeST-BO consistently yields faster convergence and lower regret than state-of-the-art local and high-dimensional BO baselines.", "link": "https://arxiv.org/abs/2510.05516", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.231027Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05526v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment", "summary": "arXiv:2510.05526v1 Announce Type: new Abstract: Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \\textit{\\textbf{C}orrupted} preference, reward \\textit{\\textbf{O}veroptimization}, and bias towards \\textit{\\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\\textbf{COV} and DPO-\\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings.", "link": "https://arxiv.org/abs/2510.05526", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.231073Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05527v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Transfer Learning on Edge Connecting Probability Estimation under Graphon Model", "summary": "arXiv:2510.05527v1 Announce Type: new Abstract: Graphon models provide a flexible nonparametric framework for estimating latent connectivity probabilities in networks, enabling a range of downstream applications such as link prediction and data augmentation. However, accurate graphon estimation typically requires a large graph, whereas in practice, one often only observes a small-sized network. One approach to addressing this issue is to adopt a transfer learning framework, which aims to improve estimation in a small target graph by leveraging structural information from a larger, related source graph. In this paper, we propose a novel method, namely GTRANS, a transfer learning framework that integrates neighborhood smoothing and Gromov-Wasserstein optimal transport to align and transfer structural patterns between graphs. To prevent negative transfer, GTRANS includes an adaptive debiasing mechanism that identifies and corrects for target-specific deviations via residual smoothing. We provide theoretical guarantees on the stability of the estimated alignment matrix and demonstrate the effectiveness of GTRANS in improving the accuracy of target graph estimation through extensive synthetic and real data experiments. These improvements translate directly to enhanced performance in downstream applications, such as the graph classification task and the link prediction task.", "link": "https://arxiv.org/abs/2510.05527", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.231124Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05528v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix Factorization", "summary": "arXiv:2510.05528v1 Announce Type: new Abstract: Large language models (LLMs) present significant deployment challenges due to their immense computational and memory requirements. While semi-structured pruning, particularly 2:4 sparsity, offers a path to practical hardware acceleration, existing methods often incur substantial performance degradation. To bridge this gap, we introduce ARMOR: (Adaptive Representation with Matrix-factORization), a novel one-shot post-training pruning algorithm. Instead of directly pruning weights, ARMOR factorizes each weight matrix into a 2:4 sparse core wrapped by two low-overhead, block diagonal matrices. These wrappers act as efficient pre and post-transformation error correctors, offering greater flexibility to preserve model quality compared to conventional 2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen through a block coordinate descent algorithm that minimizes a layer-wise proxy loss. We theoretically prove this optimization is guaranteed to converge to a solution with a proxy loss less than or equal to state-of-the-art pruning algorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and Qwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and significantly outperforms state-of-the-art 2:4 pruning methods across a wide range of downstream tasks and perplexity evaluations. ARMOR achieves this superior performance while retaining the inference speedups and substantial memory usage reductions of 2:4 pruning, establishing a more effective trade-off between model compression and task accuracy", "link": "https://arxiv.org/abs/2510.05528", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.231169Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.05530v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "LATTA: Langevin-Anchored Test-Time Adaptation for Enhanced Robustness and Stability", "summary": "arXiv:2510.05530v1 Announce Type: new Abstract: Test-time adaptation (TTA) aims to adapt a pretrained model to distribution shifts using only unlabeled test data. While promising, existing methods like Tent suffer from instability and can catastrophically forget the source knowledge, especially with small batch sizes or challenging corruptions. We argue that this arises from overly deterministic updates on a complex loss surface. In this paper, we introduce Langevin-Anchored Test-Time Adaptation (LATTA), a novel approach that regularizes adaptation through two key mechanisms: (1) a noisy weight perturbation inspired by Stochastic Gradient Langevin Dynamics (SGLD) to explore the local parameter space and escape poor local minima, and (2) a stable weight anchor that prevents the model from diverging from its robust source pre-training. This combination allows LATTA to adapt effectively without sacrificing stability. Unlike prior Bayesian TTA methods, LATTA requires no architectural changes or expensive Monte Carlo passes. We conduct extensive experiments on standard benchmarks, including Rotated-MNIST and the more challenging CIFAR-10-C. Our results demonstrate that LATTA significantly outperforms existing methods, including Tent, CoTTA, and EATA, setting a new state of the art for self-supervised TTA by improving average accuracy on CIFAR-10-C by over 2% while simultaneously reducing performance variance.", "link": "https://arxiv.org/abs/2510.05530", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T02:41:24.231214Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05380v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Minima and Critical Points of the Bethe Free Energy Are Invariant Under Deformation Retractions of Factor Graphs", "summary": "arXiv:2510.05380v1 Announce Type: new Abstract: In graphical models, factor graphs, and more generally energy-based models, the interactions between variables are encoded by a graph, a hypergraph, or, in the most general case, a partially ordered set (poset). Inference on such probabilistic models cannot be performed exactly due to cycles in the underlying structures of interaction. Instead, one resorts to approximate variational inference by optimizing the Bethe free energy. Critical points of the Bethe free energy correspond to fixed points of the associated Belief Propagation algorithm. A full characterization of these critical points for general graphs, hypergraphs, and posets with a finite number of variables is still an open problem. We show that, for hypergraphs and posets with chains of length at most 1, changing the poset of interactions of the probabilistic model to one with the same homotopy type induces a bijection between the critical points of the associated free energy. This result extends and unifies classical results that assume specific forms of collapsibility to prove uniqueness of the critical points of the Bethe free energy.", "link": "https://arxiv.org/abs/2510.05380", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588372Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05440v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Refereed Learning", "summary": "arXiv:2510.05440v1 Announce Type: new Abstract: We initiate an investigation of learning tasks in a setting where the learner is given access to two competing provers, only one of which is honest. Specifically, we consider the power of such learners in assessing purported properties of opaque models. Following prior work that considers the power of competing provers in different settings, we call this setting refereed learning. After formulating a general definition of refereed learning tasks, we show refereed learning protocols that obtain a level of accuracy that far exceeds what is obtainable at comparable cost without provers, or even with a single prover. We concentrate on the task of choosing the better one out of two black-box models, with respect to some ground truth. While we consider a range of parameters, perhaps our most notable result is in the high-precision range: For all $\\varepsilon>0$ and ambient dimension $d$, our learner makes only one query to the ground truth function, communicates only $(1+\\frac{1}{\\varepsilon^2})\\cdot\\text{poly}(d)$ bits with the provers, and outputs a model whose loss is within a multiplicative factor of $(1+\\varepsilon)$ of the best model's loss. Obtaining comparable loss with a single prover would require the learner to access the ground truth at almost all of the points in the domain. To obtain this bound, we develop a technique that allows the learner to sample, using the provers, from a distribution that is not efficiently samplable to begin with. We find this technique to be of independent interest. We also present lower bounds that demonstrate the optimality of our protocols in a number of respects, including prover complexity, number of samples, and need for query access.", "link": "https://arxiv.org/abs/2510.05440", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588435Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05447v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "A Probabilistic Basis for Low-Rank Matrix Learning", "summary": "arXiv:2510.05447v1 Announce Type: new Abstract: Low rank inference on matrices is widely conducted by optimizing a cost function augmented with a penalty proportional to the nuclear norm $\\Vert \\cdot \\Vert_*$. However, despite the assortment of computational methods for such problems, there is a surprising lack of understanding of the underlying probability distributions being referred to. In this article, we study the distribution with density $f(X)\\propto e^{-\\lambda\\Vert X\\Vert_*}$, finding many of its fundamental attributes to be analytically tractable via differential geometry. We use these facts to design an improved MCMC algorithm for low rank Bayesian inference as well as to learn the penalty parameter $\\lambda$, obviating the need for hyperparameter tuning when this is difficult or impossible. Finally, we deploy these to improve the accuracy and efficiency of low rank Bayesian matrix denoising and completion algorithms in numerical experiments.", "link": "https://arxiv.org/abs/2510.05447", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588478Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05566v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Domain-Shift-Aware Conformal Prediction for Large Language Models", "summary": "arXiv:2510.05566v1 Announce Type: new Abstract: Large language models have achieved impressive performance across diverse tasks. However, their tendency to produce overconfident and factually incorrect outputs, known as hallucinations, poses risks in real world applications. Conformal prediction provides finite-sample, distribution-free coverage guarantees, but standard conformal prediction breaks down under domain shift, often leading to under-coverage and unreliable prediction sets. We propose a new framework called Domain-Shift-Aware Conformal Prediction (DS-CP). Our framework adapts conformal prediction to large language models under domain shift, by systematically reweighting calibration samples based on their proximity to the test prompt, thereby preserving validity while enhancing adaptivity. Our theoretical analysis and experiments on the MMLU benchmark demonstrate that the proposed method delivers more reliable coverage than standard conformal prediction, especially under substantial distribution shifts, while maintaining efficiency. This provides a practical step toward trustworthy uncertainty quantification for large language models in real-world deployment.", "link": "https://arxiv.org/abs/2510.05566", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588521Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05568v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Bilevel optimization for learning hyperparameters: Application to solving PDEs and inverse problems with Gaussian processes", "summary": "arXiv:2510.05568v1 Announce Type: new Abstract: Methods for solving scientific computing and inference problems, such as kernel- and neural network-based approaches for partial differential equations (PDEs), inverse problems, and supervised learning tasks, depend crucially on the choice of hyperparameters. Specifically, the efficacy of such methods, and in particular their accuracy, stability, and generalization properties, strongly depends on the choice of hyperparameters. While bilevel optimization offers a principled framework for hyperparameter tuning, its nested optimization structure can be computationally demanding, especially in PDE-constrained contexts. In this paper, we propose an efficient strategy for hyperparameter optimization within the bilevel framework by employing a Gauss-Newton linearization of the inner optimization step. Our approach provides closed-form updates, eliminating the need for repeated costly PDE solves. As a result, each iteration of the outer loop reduces to a single linearized PDE solve, followed by explicit gradient-based hyperparameter updates. We demonstrate the effectiveness of the proposed method through Gaussian process models applied to nonlinear PDEs and to PDE inverse problems. Extensive numerical experiments highlight substantial improvements in accuracy and robustness compared to conventional random hyperparameter initialization. In particular, experiments with additive kernels and neural network-parameterized deep kernels demonstrate the method's scalability and effectiveness for high-dimensional hyperparameter optimization.", "link": "https://arxiv.org/abs/2510.05568", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588570Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05573v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "On the Theory of Continual Learning with Gradient Descent for Neural Networks", "summary": "arXiv:2510.05573v1 Announce Type: new Abstract: Continual learning, the ability of a model to adapt to an ongoing sequence of tasks without forgetting the earlier ones, is a central goal of artificial intelligence. To shed light on its underlying mechanisms, we analyze the limitations of continual learning in a tractable yet representative setting. In particular, we study one-hidden-layer quadratic neural networks trained by gradient descent on an XOR cluster dataset with Gaussian noise, where different tasks correspond to different clusters with orthogonal means. Our results obtain bounds on the rate of forgetting during train and test-time in terms of the number of iterations, the sample size, the number of tasks, and the hidden-layer size. Our results reveal interesting phenomena on the role of different problem parameters in the rate of forgetting. Numerical experiments across diverse setups confirm our results, demonstrating their validity beyond the analyzed settings.", "link": "https://arxiv.org/abs/2510.05573", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588609Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06149v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Implicit Updates for Average-Reward Temporal Difference Learning", "summary": "arXiv:2510.06149v1 Announce Type: new Abstract: Temporal difference (TD) learning is a cornerstone of reinforcement learning. In the average-reward setting, standard TD($\\lambda$) is highly sensitive to the choice of step-size and thus requires careful tuning to maintain numerical stability. We introduce average-reward implicit TD($\\lambda$), which employs an implicit fixed point update to provide data-adaptive stabilization while preserving the per iteration computational complexity of standard average-reward TD($\\lambda$). In contrast to prior finite-time analyses of average-reward TD($\\lambda$), which impose restrictive step-size conditions, we establish finite-time error bounds for the implicit variant under substantially weaker step-size requirements. Empirically, average-reward implicit TD($\\lambda$) operates reliably over a much broader range of step-sizes and exhibits markedly improved numerical stability. This enables more efficient policy evaluation and policy learning, highlighting its effectiveness as a robust alternative to average-reward TD($\\lambda$).", "link": "https://arxiv.org/abs/2510.06149", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588651Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05147v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing", "summary": "arXiv:2510.05147v1 Announce Type: cross Abstract: Ensuring reliability in modern software systems requires rigorous pre-production testing across highly heterogeneous and evolving environments. Because exhaustive evaluation is infeasible, practitioners must decide how to allocate limited testing resources across configurations where failure probabilities may drift over time. Existing combinatorial optimization approaches are static, ad hoc, and poorly suited to such non-stationary settings. We introduce a novel reinforcement learning (RL) framework that recasts configuration allocation as a sequential decision-making problem. Our method is the first to integrate Q-learning with a hybrid reward design that fuses simulated outcomes and real-time feedback, enabling both sample efficiency and robustness. In addition, we develop an adaptive online-offline training scheme that allows the agent to quickly track abrupt probability shifts while maintaining long-run stability. Extensive simulation studies demonstrate that our approach consistently outperforms static and optimization-based baselines, approaching oracle performance. This work establishes RL as a powerful new paradigm for adaptive configuration allocation, advancing beyond traditional methods and offering broad applicability to dynamic testing and resource scheduling domains.", "link": "https://arxiv.org/abs/2510.05147", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588704Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05183v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Aneurysm Growth Time Series Reconstruction Using Physics-informed Autoencoder", "summary": "arXiv:2510.05183v1 Announce Type: cross Abstract: Arterial aneurysm (Fig.1) is a bulb-shape local expansion of human arteries, the rupture of which is a leading cause of morbidity and mortality in US. Therefore, the prediction of arterial aneurysm rupture is of great significance for aneurysm management and treatment selection. The prediction of aneurysm rupture depends on the analysis of the time series of aneurysm growth history. However, due to the long time scale of aneurysm growth, the time series of aneurysm growth is not always accessible. We here proposed a method to reconstruct the aneurysm growth time series directly from patient parameters. The prediction is based on data pairs of [patient parameters, patient aneurysm growth time history]. To obtain the mapping from patient parameters to patient aneurysm growth time history, we first apply autoencoder to obtain a compact representation of the time series for each patient. Then a mapping is learned from patient parameters to the corresponding compact representation of time series via a five-layer neural network. Moving average and convolutional output layer are implemented to explicitly taking account the time dependency of the time series. Apart from that, we also propose to use prior knowledge about the mechanism of aneurysm growth to improve the time series reconstruction results. The prior physics-based knowledge is incorporated as constraints for the optimization problem associated with autoencoder. The model can handle both algebraic and differential constraints. Our results show that including physical model information about the data will not significantly improve the time series reconstruction results if the training data is error-free. However, in the case of training data with noise and bias error, incorporating physical model constraints can significantly improve the predicted time series.", "link": "https://arxiv.org/abs/2510.05183", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588761Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05197v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Efficient Prediction of Pass@k Scaling in Large Language Models", "summary": "arXiv:2510.05197v1 Announce Type: cross Abstract: Assessing the capabilities and risks of frontier AI systems is a critical area of research, and recent work has shown that repeated sampling from models can dramatically increase both. For instance, repeated sampling has been shown to increase their capabilities, such as solving difficult math and coding problems, but it has also been shown to increase their potential for harm, such as being jailbroken. Such results raise a crucial question for both capability and safety forecasting: how can one accurately predict a model's behavior when scaled to a massive number of attempts, given a vastly smaller sampling budget? This question is directly relevant to model providers, who serve hundreds of millions of users daily, and to governmental regulators, who seek to prevent harms. To answer this questions, we make three contributions. First, we find that standard methods for fitting these laws suffer from statistical shortcomings that hinder predictive accuracy, especially in data-limited scenarios. Second, we remedy these shortcomings by introducing a robust estimation framework, which uses a beta-binomial distribution to generate more accurate predictions from limited data. Third, we propose a dynamic sampling strategy that allocates a greater budget to harder problems. Combined, these innovations enable more reliable prediction of rare risks and capabilities at a fraction of the computational cost.", "link": "https://arxiv.org/abs/2510.05197", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588810Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05286v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Computing frustration and near-monotonicity in deep neural networks", "summary": "arXiv:2510.05286v1 Announce Type: cross Abstract: For the signed graph associated to a deep neural network, one can compute the frustration level, i.e., test how close or distant the graph is to structural balance. For all the pretrained deep convolutional neural networks we consider, we find that the frustration is always less than expected from null models. From a statistical physics point of view, and in particular in reference to an Ising spin glass model, the reduced frustration indicates that the amount of disorder encoded in the network is less than in the null models. From a functional point of view, low frustration (i.e., proximity to structural balance) means that the function representing the network behaves near-monotonically, i.e., more similarly to a monotone function than in the null models. Evidence of near-monotonic behavior along the partial order determined by frustration is observed for all networks we consider. This confirms that the class of deep convolutional neural networks tends to have a more ordered behavior than expected from null models, and suggests a novel form of implicit regularization.", "link": "https://arxiv.org/abs/2510.05286", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588850Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05329v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Tensor-on-tensor Regression Neural Networks for Process Modeling with High-dimensional Data", "summary": "arXiv:2510.05329v1 Announce Type: cross Abstract: Modern sensing and metrology systems now stream terabytes of heterogeneous, high-dimensional (HD) data profiles, images, and dense point clouds, whose natural representation is multi-way tensors. Understanding such data requires regression models that preserve tensor geometry, yet remain expressive enough to capture the pronounced nonlinear interactions that dominate many industrial and mechanical processes. Existing tensor-based regressors meet the first requirement but remain essentially linear. Conversely, conventional neural networks offer nonlinearity only after flattening, thereby discarding spatial structure and incurring prohibitive parameter counts. This paper introduces a Tensor-on-Tensor Regression Neural Network (TRNN) that unifies these two paradigms.", "link": "https://arxiv.org/abs/2510.05329", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588886Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05338v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Integrating Bayesian methods with neural network--based model predictive control: a review", "summary": "arXiv:2510.05338v1 Announce Type: cross Abstract: In this review, we assess the use of Bayesian methods in model predictive control (MPC), focusing on neural-network-based modeling, control design, and uncertainty quantification. We systematically analyze individual studies and how they are implemented in practice. While Bayesian approaches are increasingly adopted to capture and propagate uncertainty in MPC, reported gains in performance and robustness remain fragmented, with inconsistent baselines and limited reliability analyses. We therefore argue for standardized benchmarks, ablation studies, and transparent reporting to rigorously determine the effectiveness of Bayesian techniques for MPC.", "link": "https://arxiv.org/abs/2510.05338", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588916Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05446v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Prior-Aligned Meta-RL: Thompson Sampling with Learned Priors and Guarantees in Finite-Horizon MDPs", "summary": "arXiv:2510.05446v1 Announce Type: cross Abstract: We study meta-reinforcement learning in finite-horizon MDPs where related tasks share similar structures in their optimal action-value functions. Specifically, we posit a linear representation $Q^*_h(s,a)=\\Phi_h(s,a)\\,\\theta^{(k)}_h$ and place a Gaussian meta-prior $ \\mathcal{N}(\\theta^*_h,\\Sigma^*_h)$ over the task-specific parameters $\\theta^{(k)}_h$. Building on randomized value functions, we propose two Thompson-style algorithms: (i) MTSRL, which learns only the prior mean and performs posterior sampling with the learned mean and known covariance; and (ii) $\\text{MTSRL}^{+}$, which additionally estimates the covariance and employs prior widening to control finite-sample estimation error. Further, we develop a prior-alignment technique that couples the posterior under the learned prior with a meta-oracle that knows the true prior, yielding meta-regret guarantees: we match prior-independent Thompson sampling in the small-task regime and strictly improve with more tasks once the prior is learned. Concretely, for known covariance we obtain $\\tilde{O}(H^{4}S^{3/2}\\sqrt{ANK})$ meta-regret, and with learned covariance $\\tilde{O}(H^{4}S^{3/2}\\sqrt{AN^3K})$; both recover a better behavior than prior-independent after $K \\gtrsim \\tilde{O}(H^2)$ and $K \\gtrsim \\tilde{O}(N^2H^2)$, respectively. Simulations on a stateful recommendation environment (with feature and prior misspecification) show that after brief exploration, MTSRL/MTSRL\\(^+\\) track the meta-oracle and substantially outperform prior-independent RL and bandit-only meta-baselines. Our results give the first meta-regret guarantees for Thompson-style RL with learned Q-priors, and provide practical recipes (warm-start via RLSVI, OLS aggregation, covariance widening) for experiment-rich settings.", "link": "https://arxiv.org/abs/2510.05446", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.588967Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05487v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Smart Contract Adoption under Discrete Overdispersed Demand: A Negative Binomial Optimization Perspective", "summary": "arXiv:2510.05487v1 Announce Type: cross Abstract: Effective supply chain management under high-variance demand requires models that jointly address demand uncertainty and digital contracting adoption. Existing research often simplifies demand variability or treats adoption as an exogenous decision, limiting relevance in e-commerce and humanitarian logistics. This study develops an optimization framework combining dynamic Negative Binomial (NB) demand modeling with endogenous smart contract adoption. The NB process incorporates autoregressive dynamics in success probability to capture overdispersion and temporal correlation. Simulation experiments using four real-world datasets, including Delhivery Logistics and the SCMS Global Health Delivery system, apply maximum likelihood estimation and grid search to optimize adoption intensity and order quantity. Across all datasets, the NB specification outperforms Poisson and Gaussian benchmarks, with overdispersion indices exceeding 1.5. Forecasting comparisons show that while ARIMA and Exponential Smoothing achieve similar point accuracy, the NB model provides superior stability under high variance. Scenario analysis reveals that when dispersion exceeds a critical threshold (r > 6), increasing smart contract adoption above 70% significantly enhances profitability and service levels. This framework offers actionable guidance for balancing inventory costs, service levels, and implementation expenses, highlighting the importance of aligning digital adoption strategies with empirically observed demand volatility.", "link": "https://arxiv.org/abs/2510.05487", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589016Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05620v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Monte Carlo-Type Neural Operator for Differential Equations", "summary": "arXiv:2510.05620v1 Announce Type: cross Abstract: The Monte Carlo-type Neural Operator (MCNO) introduces a framework for learning solution operators of one-dimensional partial differential equations (PDEs) by directly learning the kernel function and approximating the associated integral operator using a Monte Carlo-type approach. Unlike Fourier Neural Operators (FNOs), which rely on spectral representations and assume translation-invariant kernels, MCNO makes no such assumptions. The kernel is represented as a learnable tensor over sampled input-output pairs, and sampling is performed once, uniformly at random from a discretized grid. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training, while an interpolation step maps between arbitrary input and output grids to further enhance flexibility. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with efficient computational cost. We also provide a theoretical analysis proving that the Monte Carlo estimator yields a bounded bias and variance under mild regularity assumptions. This result holds in any spatial dimension, suggesting that MCNO may extend naturally beyond one-dimensional problems. More broadly, this work explores how Monte Carlo-type integration can be incorporated into neural operator frameworks for continuous-domain PDEs, providing a theoretically supported alternative to spectral methods (such as FNO) and to graph-based Monte Carlo approaches (such as the Graph Kernel Neural Operator, GNO).", "link": "https://arxiv.org/abs/2510.05620", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589061Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05825v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Mitigating Premature Exploitation in Particle-based Monte Carlo for Inference-Time Scaling", "summary": "arXiv:2510.05825v1 Announce Type: cross Abstract: Inference-Time Scaling (ITS) improves language models by allocating more computation at generation time. Particle Filtering (PF) has emerged as a strong ITS method for complex mathematical reasoning tasks, but it is vulnerable when guided by process reward models, which often assign overconfident scores early in the reasoning process. This causes PF to suffer from premature exploitation: it myopically commits to locally promising trajectories, prunes potentially correct hypotheses, and converges to suboptimal solutions. This failure mode, known as particle impoverishment, is especially severe under constrained computational budgets. To address this, we analyze the problem and identify two root causes: a lack of diversity in the particle set due to overconfident resampling and consequent inability to assess the potential of a reasoning path. We introduce Entropic Particle Filtering (ePF), an algorithm that integrates two new techniques to solve these issues. The first technique, Entropic Annealing (EA), directly mitigates particle impoverishment by monitoring search diversity via entropy; when diversity drops, it intervenes by dynamically annealing the resampling distribution to preserve exploration. The second, an enhancement called Look-ahead Modulation (LaM), adds a predictive guide to evaluate a state's potential based on its successors. On several challenging math benchmarks, ePF significantly outperforms strong baselines and achieves up to a 50 % relative improvement in task reward. Together, these methods improve PF's resilience by balancing the exploration of diverse solution spaces with the exploitation of high-reward regions, ultimately leading to higher-quality solutions.", "link": "https://arxiv.org/abs/2510.05825", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589114Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05849v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "ESS-Flow: Training-free guidance of flow-based models as inference in source space", "summary": "arXiv:2510.05849v1 Announce Type: cross Abstract: Guiding pretrained flow-based generative models for conditional generation or to produce samples with desired target properties enables solving diverse tasks without retraining on paired data. We present ESS-Flow, a gradient-free method that leverages the typically Gaussian prior of the source distribution in flow-based models to perform Bayesian inference directly in the source space using Elliptical Slice Sampling. ESS-Flow only requires forward passes through the generative model and observation process, no gradient or Jacobian computations, and is applicable even when gradients are unreliable or unavailable, such as with simulation-based observations or quantization in the generation or observation process. We demonstrate its effectiveness on designing materials with desired target properties and predicting protein structures from sparse inter-residue distance measurements.", "link": "https://arxiv.org/abs/2510.05849", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589150Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.05949v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density", "summary": "arXiv:2510.05949v1 Announce Type: cross Abstract: Joint Embedding Predictive Architectures (JEPAs) learn representations able to solve numerous downstream tasks out-of-the-box. JEPAs combine two objectives: (i) a latent-space prediction term, i.e., the representation of a slightly perturbed sample must be predictable from the original sample's representation, and (ii) an anti-collapse term, i.e., not all samples should have the same representation. While (ii) is often considered as an obvious remedy to representation collapse, we uncover that JEPAs' anti-collapse term does much more--it provably estimates the data density. In short, any successfully trained JEPA can be used to get sample probabilities, e.g., for data curation, outlier detection, or simply for density estimation. Our theoretical finding is agnostic of the dataset and architecture used--in any case one can compute the learned probabilities of sample $x$ efficiently and in closed-form using the model's Jacobian matrix at $x$. Our findings are empirically validated across datasets (synthetic, controlled, and Imagenet) and across different Self Supervised Learning methods falling under the JEPA family (I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP. We denote the method extracting the JEPA learned density as {\\bf JEPA-SCORE}.", "link": "https://arxiv.org/abs/2510.05949", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589188Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06025v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Out-of-Distribution Detection from Small Training Sets using Bayesian Neural Network Classifiers", "summary": "arXiv:2510.06025v1 Announce Type: cross Abstract: Out-of-Distribution (OOD) detection is critical to AI reliability and safety, yet in many practical settings, only a limited amount of training data is available. Bayesian Neural Networks (BNNs) are a promising class of model on which to base OOD detection, because they explicitly represent epistemic (i.e. model) uncertainty. In the small training data regime, BNNs are especially valuable because they can incorporate prior model information. We introduce a new family of Bayesian posthoc OOD scores based on expected logit vectors, and compare 5 Bayesian and 4 deterministic posthoc OOD scores. Experiments on MNIST and CIFAR-10 In-Distributions, with 5000 training samples or less, show that the Bayesian methods outperform corresponding deterministic methods.", "link": "https://arxiv.org/abs/2510.06025", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589219Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06028v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Generalization of Gibbs and Langevin Monte Carlo Algorithms in the Interpolation Regime", "summary": "arXiv:2510.06028v1 Announce Type: cross Abstract: The paper provides data-dependent bounds on the test error of the Gibbs algorithm in the overparameterized interpolation regime, where low training errors are also obtained for impossible data, such as random labels in classification. The bounds are stable under approximation with Langevin Monte Carlo algorithms. Experiments on the MNIST and CIFAR-10 datasets verify that the bounds yield nontrivial predictions on true labeled data and correctly upper bound the test error for random labels. Our method indicates that generalization in the low-temperature, interpolation regime is already signaled by small training errors in the more classical high temperature regime.", "link": "https://arxiv.org/abs/2510.06028", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589253Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06091v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Learning Mixtures of Linear Dynamical Systems (MoLDS) via Hybrid Tensor-EM Method", "summary": "arXiv:2510.06091v1 Announce Type: cross Abstract: Mixtures of linear dynamical systems (MoLDS) provide a path to model time-series data that exhibit diverse temporal dynamics across trajectories. However, its application remains challenging in complex and noisy settings, limiting its effectiveness for neural data analysis. Tensor-based moment methods can provide global identifiability guarantees for MoLDS, but their performance degrades under noise and complexity. Commonly used expectation-maximization (EM) methods offer flexibility in fitting latent models but are highly sensitive to initialization and prone to poor local minima. Here, we propose a tensor-based method that provides identifiability guarantees for learning MoLDS, which is followed by EM updates to combine the strengths of both approaches. The novelty in our approach lies in the construction of moment tensors using the input-output data to recover globally consistent estimates of mixture weights and system parameters. These estimates can then be refined through a Kalman EM algorithm, with closed-form updates for all LDS parameters. We validate our framework on synthetic benchmarks and real-world datasets. On synthetic data, the proposed Tensor-EM method achieves more reliable recovery and improved robustness compared to either pure tensor or randomly initialized EM methods. We then analyze neural recordings from the primate somatosensory cortex while a non-human primate performs reaches in different directions. Our method successfully models and clusters different conditions as separate subsystems, consistent with supervised single-LDS fits for each condition. Finally, we apply this approach to another neural dataset where monkeys perform a sequential reaching task. These results demonstrate that MoLDS provides an effective framework for modeling complex neural data, and that Tensor-EM is a reliable approach to MoLDS learning for these applications.", "link": "https://arxiv.org/abs/2510.06091", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589304Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06122v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "PolyGraph Discrepancy: a classifier-based metric for graph generation", "summary": "arXiv:2510.06122v1 Announce Type: cross Abstract: Existing methods for evaluating graph generative models primarily rely on Maximum Mean Discrepancy (MMD) metrics based on graph descriptors. While these metrics can rank generative models, they do not provide an absolute measure of performance. Their values are also highly sensitive to extrinsic parameters, namely kernel and descriptor parametrization, making them incomparable across different graph descriptors. We introduce PolyGraph Discrepancy (PGD), a new evaluation framework that addresses these limitations. It approximates the Jensen-Shannon distance of graph distributions by fitting binary classifiers to distinguish between real and generated graphs, featurized by these descriptors. The data log-likelihood of these classifiers approximates a variational lower bound on the JS distance between the two distributions. Resulting metrics are constrained to the unit interval [0,1] and are comparable across different graph descriptors. We further derive a theoretically grounded summary metric that combines these individual metrics to provide a maximally tight lower bound on the distance for the given descriptors. Thorough experiments demonstrate that PGD provides a more robust and insightful evaluation compared to MMD metrics. The PolyGraph framework for benchmarking graph generative models is made publicly available at https://github.com/BorgwardtLab/polygraph-benchmark.", "link": "https://arxiv.org/abs/2510.06122", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589387Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06165v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Higher-Order Feature Attribution: Bridging Statistics, Explainable AI, and Topological Signal Processing", "summary": "arXiv:2510.06165v1 Announce Type: cross Abstract: Feature attributions are post-training analysis methods that assess how various input features of a machine learning model contribute to an output prediction. Their interpretation is straightforward when features act independently, but becomes less direct when the predictive model involves interactions such as multiplicative relationships or joint feature contributions. In this work, we propose a general theory of higher-order feature attribution, which we develop on the foundation of Integrated Gradients (IG). This work extends existing frameworks in the literature on explainable AI. When using IG as the method of feature attribution, we discover natural connections to statistics and topological signal processing. We provide several theoretical results that establish the theory, and we validate our theory on a few examples.", "link": "https://arxiv.org/abs/2510.06165", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589420Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06181v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Conformalized Gaussian processes for online uncertainty quantification over graphs", "summary": "arXiv:2510.06181v1 Announce Type: cross Abstract: Uncertainty quantification (UQ) over graphs arises in a number of safety-critical applications in network science. The Gaussian process (GP), as a classical Bayesian framework for UQ, has been developed to handle graph-structured data by devising topology-aware kernel functions. However, such GP-based approaches are limited not only by the prohibitive computational complexity, but also the strict modeling assumptions that might yield poor coverage, especially with labels arriving on the fly. To effect scalability, we devise a novel graph-aware parametric GP model by leveraging the random feature (RF)-based kernel approximation, which is amenable to efficient recursive Bayesian model updates. To further allow for adaptivity, an ensemble of graph-aware RF-based scalable GPs have been leveraged, with per-GP weight adapted to data arriving incrementally. To ensure valid coverage with robustness to model mis-specification, we wed the GP-based set predictors with the online conformal prediction framework, which post-processes the prediction sets using adaptive thresholds. Experimental results the proposed method yields improved coverage and efficient prediction sets over existing baselines by adaptively ensembling the GP models and setting the key threshold parameters in CP.", "link": "https://arxiv.org/abs/2510.06181", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589459Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2307.12472v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Model-free generalized fiducial inference", "summary": "arXiv:2307.12472v2 Announce Type: replace Abstract: Conformal prediction (CP) was developed to provide finite-sample probabilistic prediction guarantees. While CP algorithms are a relatively general-purpose approach to uncertainty quantification, with finite-sample guarantees, they lack versatility. Namely, the CP approach does not {\\em prescribe} how to quantify the degree to which a data set provides evidence in support of (or against) an arbitrary event from a general class of events. In this paper, tools are offered from imprecise probability theory to build a formal connection between CP and generalized fiducial (GF) inference. These new insights establish a more general inferential lens from which CP can be understood, and demonstrate the pragmatism of fiducial ideas. The formal connection establishes a context in which epistemically-derived GF probability matches aleatoric/frequentist probability. Beyond this fact, it is illustrated how tools from imprecise probability theory, namely lower and upper probability functions, can be applied in the context of the imprecise GF distribution to provide posterior-like, prescriptive inference that is not possible within the CP framework alone. In addition to the primary CP generalization that is contributed, fundamental connections are synthesized between this new model-free GF and three other areas of contemporary research: nonparametric predictive inference (NPI), conformal predictive systems/distributions, and inferential models (IMs).", "link": "https://arxiv.org/abs/2307.12472", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589500Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2501.19038v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Conformal Prediction in Hierarchical Classification with Constrained Representation Complexity", "summary": "arXiv:2501.19038v2 Announce Type: replace Abstract: Conformal prediction has emerged as a widely used framework for constructing valid prediction sets in classification and regression tasks. In this work, we extend the split conformal prediction framework to hierarchical classification, where prediction sets are commonly restricted to internal nodes of a predefined hierarchy, and propose two computationally efficient inference algorithms. The first algorithm returns internal nodes as prediction sets, while the second one relaxes this restriction. Using the notion of representation complexity, the latter yields smaller set sizes at the cost of a more general and combinatorial inference problem. Empirical evaluations on several benchmark datasets demonstrate the effectiveness of the proposed algorithms in achieving nominal coverage.", "link": "https://arxiv.org/abs/2501.19038", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589537Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2504.14898v4", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Expected Free Energy-based Planning as Variational Inference", "summary": "arXiv:2504.14898v4 Announce Type: replace Abstract: We address the problem of planning under uncertainty, where an agent must choose actions that not only achieve desired outcomes but also reduce uncertainty. Traditional methods often treat exploration and exploitation as separate objectives, lacking a unified inferential foundation. Active inference, grounded in the Free Energy Principle, provides such a foundation by minimizing Expected Free Energy (EFE), a cost function that combines utility with epistemic drives, such as ambiguity resolution and novelty seeking. However, the computational burden of EFE minimization had remained a significant obstacle to its scalability. In this paper, we show that EFE-based planning arises naturally from minimizing a variational free energy functional on a generative model augmented with preference and epistemic priors. This result reinforces theoretical consistency with the Free Energy Principle by casting planning under uncertainty itself as a form of variational inference. Our formulation yields policies that jointly support goal achievement and information gain, while incorporating a complexity term that accounts for bounded computational resources. This unifying framework connects and extends existing methods, enabling scalable, resource-aware implementations of active inference agents.", "link": "https://arxiv.org/abs/2504.14898", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589575Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2303.16822v5", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "An inexact LPA for DC composite optimization and application to matrix completions with outliers", "summary": "arXiv:2303.16822v5 Announce Type: replace-cross Abstract: This paper concerns a class of DC composite optimization problems which, as an extension of convex composite optimization problems and DC programs with nonsmooth components, often arises in robust factorization models of low-rank matrix recovery. For this class of nonconvex and nonsmooth problems, we propose an inexact linearized proximal algorithm (iLPA) by computing at each step an inexact minimizer of a strongly convex majorization constructed with a partial linearization of their objective functions at the current iterate. We establish the full convergence of the generated iterate sequence under the Kurdyka-\\L\\\"ojasiewicz (KL) property of a potential function, and employ the composite structure to provide a verifiable condition for the potential function to satisfy the KL property of exponent $1/2$ at the limit point, so for the iterate sequence to have a local R-linear convergence rate. This condition is weaker than the one provided in \\cite[Theorem 3.2]{LiPong18} for identifying the KL property of exponent $p\\in[0,1)$ for a general composite function. The proposed iLPA is applied to a robust factorization model for matrix completion with outliers and non-uniform sampling, and numerical comparisons with the Polyak subgradient method and a proximal alternating minimization (PAM) method validate its efficiency.", "link": "https://arxiv.org/abs/2303.16822", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589622Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2306.17470v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Oblivious Stochastic Composite Optimization", "summary": "arXiv:2306.17470v2 Announce Type: replace-cross Abstract: In stochastic convex optimization problems, most existing adaptive methods rely on prior knowledge about the diameter bound $D$ when the smoothness or the Lipschitz constant is unknown. This often significantly affects performance as only a rough approximation of $D$ is usually known in practice. Here, we bypass this limitation by combining mirror descent with dual averaging techniques and we show that, under oblivious step-sizes regime, our algorithms converge without any prior knowledge on the parameters of the problem. We introduce three oblivious stochastic algorithms to address different settings. The first algorithm is designed for objectives in relative scale, the second one is an accelerated version tailored for smooth objectives, whereas the last one is for relatively-smooth objectives. All three algorithms work without prior knowledge of the diameter of the feasible set, the Lipschitz constant or smoothness of the objective function. We use these results to revisit the problem of solving large-scale semidefinite programs using randomized first-order methods and stochastic smoothing. We extend our framework to relative scale and demonstrate the efficiency and robustness of our methods on large-scale semidefinite programs.", "link": "https://arxiv.org/abs/2306.17470", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589664Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2406.04588v3", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Convergence of the majorized PAM method with subspace correction for low-rank composite factorization model", "summary": "arXiv:2406.04588v3 Announce Type: replace-cross Abstract: This paper focuses on the convergence certificates of the majorized proximal alternating minimization (PAM) method with subspace correction, proposed in \\cite{TaoQianPan22} for the column $\\ell_{2,0}$-norm regularized factorization model and now extended to a class of low-rank composite factorization models from matrix completion. The convergence analysis of this PAM method becomes extremely challenging because a subspace correction step is introduced to every proximal subproblem to ensure a closed-form solution. We establish the full convergence of the iterate sequence and column subspace sequences of factor pairs generated by the PAM, under the KL property of the objective function and a condition that holds automatically for the column $\\ell_{2,0}$-norm function. Numerical comparison with the popular proximal alternating linearized minimization (PALM) method is conducted on one-bit matrix completion problems, which indicates that the PAM with subspace correction has an advantage in seeking lower relative error within less time.", "link": "https://arxiv.org/abs/2406.04588", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589708Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2406.05428v3", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Information-Theoretic Thresholds for the Alignments of Partially Correlated Graphs", "summary": "arXiv:2406.05428v3 Announce Type: replace-cross Abstract: This paper studies the problem of recovering the hidden vertex correspondence between two correlated random graphs. We propose the partially correlated Erd\\H{o}s-R\\'enyi graphs model, wherein a pair of induced subgraphs with a certain number are correlated. We investigate the information-theoretic thresholds for recovering the latent correlated subgraphs and the hidden vertex correspondence. We prove that there exists an optimal rate for partial recovery for the number of correlated nodes, above which one can correctly match a fraction of vertices and below which correctly matching any positive fraction is impossible, and we also derive an optimal rate for exact recovery. In the proof of possibility results, we propose correlated functional digraphs, which partition the edges of the intersection graph into two types of components, and bound the error probability by lower-order cumulant generating functions. The proof of impossibility results build upon the generalized Fano's inequality and the recovery thresholds settled in correlated Erd\\H{o}s-R\\'enyi graphs model.", "link": "https://arxiv.org/abs/2406.05428", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589745Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2407.03389v5", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "A Deterministic Information Bottleneck Method for Clustering Mixed-Type Data", "summary": "arXiv:2407.03389v5 Announce Type: replace-cross Abstract: In this paper, we present an information-theoretic method for clustering mixed-type data, that is, data consisting of both continuous and categorical variables. The proposed approach extends the Information Bottleneck principle to heterogeneous data through generalised product kernels, integrating continuous, nominal, and ordinal variables within a unified optimization framework. We address the following challenges: developing a systematic bandwidth selection strategy that equalises contributions across variable types, and proposing an adaptive hyperparameter updating scheme that ensures a valid solution into a predetermined number of potentially imbalanced clusters. Through simulations on 28,800 synthetic data sets and ten publicly available benchmarks, we demonstrate that the proposed method, named DIBmix, achieves superior performance compared to four established methods (KAMILA, K-Prototypes, FAMD with K-Means, and PAM with Gower's dissimilarity). Results show DIBmix particularly excels when clusters exhibit size imbalances, data contain low or moderate cluster overlap, and categorical and continuous variables are equally represented. The method presents a significant advantage over traditional centroid-based algorithms, establishing DIBmix as a competitive and theoretically grounded alternative for mixed-type data clustering.", "link": "https://arxiv.org/abs/2407.03389", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589785Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2407.11676v4", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation On Diverse Modalities", "summary": "arXiv:2407.11676v4 Announce Type: replace-cross Abstract: Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a labeled source domain to perform well on an unlabeled target domain with some data distribution shift. While many methods have been proposed in the literature, fair and realistic evaluation remains an open question, particularly due to methodological difficulties in selecting hyperparameters in the unsupervised setting. With SKADA-bench, we propose a framework to evaluate DA methods on diverse modalities, beyond computer vision task that have been largely explored in the literature. We present a complete and fair evaluation of existing shallow algorithms, including reweighting, mapping, and subspace alignment. Realistic hyperparameter selection is performed with nested cross-validation and various unsupervised model selection scores, on both simulated datasets with controlled shifts and real-world datasets across diverse modalities, such as images, text, biomedical, and tabular data. Our benchmark highlights the importance of realistic validation and provides practical guidance for real-life applications, with key insights into the choice and impact of model selection approaches. SKADA-bench is open-source, reproducible, and can be easily extended with novel DA methods, datasets, and model selection criteria without requiring re-evaluating competitors. SKADA-bench is available on Github at https://github.com/scikit-adaptation/skada-bench.", "link": "https://arxiv.org/abs/2407.11676", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589840Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2410.00709v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Binding Affinity Prediction: From Conventional to Machine Learning-Based Approaches", "summary": "arXiv:2410.00709v2 Announce Type: replace-cross Abstract: Protein-ligand binding is the process by which a small molecule (drug or inhibitor) attaches to a target protein. Binding affinity, which characterizes the strength of biomolecular interactions, is essential for tackling diverse challenges in life sciences, including therapeutic design, protein engineering, enzyme optimization, and elucidating biological mechanisms. Much work has been devoted to predicting binding affinity over the past decades. Here, we review recent significant works, with a focus on methods, evaluation strategies, and benchmark datasets. We note growing use of both traditional machine learning and deep learning models for predicting binding affinity, accompanied by an increasing amount of data on proteins and small drug-like molecules. With improved predictive performance and the FDA's phasing out of animal testing, AI-driven in silico models, such as AI virtual cells (AIVCs), are poised to advance binding affinity prediction; reciprocally, progress in building binding affinity predictors can refine AIVCs. Future efforts in binding affinity prediction and AI-driven in silico models can enhance the simulation of temporal dynamics, cell-type specificity, and multi-omics integration to support more accurate and personalized outcomes.", "link": "https://arxiv.org/abs/2410.00709", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589934Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2410.02086v3", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Anchors Aweigh! Sail for Optimal Unified Multi-Modal Representations", "summary": "arXiv:2410.02086v3 Announce Type: replace-cross Abstract: A unified representation space in multi-modal learning is essential for effectively integrating diverse data sources, such as text, images, and audio, to enhance efficiency and performance across various downstream tasks. Recent binding methods, such as ImageBind, typically rely on a single, fixed anchor modality for aligning multi-modal data. We mathematically analyze these fixed anchor binding methods and uncover significant limitations: (1) over-reliance on the choice of the anchor modality, (2) inadequate capture of intra-modal information, and (3) failure to account for cross-modal correlation among non-anchored modalities. To address these issues, we propose the need for adaptive anchor binding methods, exemplified by our framework CentroBind. The proposed method uses adaptively adjustable centroid-based anchors generated from all available modalities, leading to a balanced and rich representation space. We theoretically demonstrate that our approach captures three critical properties of multi-modal learning -- intra-modal learning, inter-modal learning, and multi-modal alignment -- while constructing a unified representation that spans all modalities. Experiments on both synthetic and real-world datasets show that adaptive anchor methods such as CentroBind consistently outperform fixed anchor binding methods, verifying our analysis.", "link": "https://arxiv.org/abs/2410.02086", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.589975Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2411.04729v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Conjugate gradient methods for high-dimensional GLMMs", "summary": "arXiv:2411.04729v2 Announce Type: replace-cross Abstract: Generalized linear mixed models (GLMMs) are a widely used tool in statistical analysis. The main bottleneck of many computational approaches lies in the inversion of the high dimensional precision matrices associated with the random effects. Such matrices are typically sparse; however, the sparsity pattern resembles a multi partite random graph, which does not lend itself well to default sparse linear algebra techniques. Notably, we show that, for typical GLMMs, the Cholesky factor is dense even when the original precision is sparse. We thus turn to approximate iterative techniques, in particular to the conjugate gradient (CG) method. We combine a detailed analysis of the spectrum of said precision matrices with results from random graph theory to show that CG-based methods applied to high-dimensional GLMMs typically achieve a fixed approximation error with a total cost that scales linearly with the number of parameters and observations. Numerical illustrations with both real and simulated data confirm the theoretical findings, while at the same time illustrating situations, such as nested structures, where CG-based methods struggle.", "link": "https://arxiv.org/abs/2411.04729", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.590014Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2412.06438v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Can foundation models actively gather information in interactive environments to test hypotheses?", "summary": "arXiv:2412.06438v2 Announce Type: replace-cross Abstract: Foundation models excel at single-turn reasoning but struggle with multi-turn exploration in dynamic environments, a requirement for many real-world challenges. We evaluated these models on their ability to learn from experience, adapt, and gather information. First, in \"Feature World,\" a simple setting for testing information gathering, models performed near-optimally. However, to test more complex, multi-trial learning, we implemented a text-based version of the \"Alchemy\" environment, a benchmark for meta-learning. Here, agents must deduce a latent causal structure by integrating information across many trials. In this setting, recent foundation models initially failed to improve their performance over time. Crucially, we found that prompting the models to summarize their observations at regular intervals enabled an emergent meta-learning process. This allowed them to improve across trials and even adaptively re-learn when the environment's rules changed unexpectedly. While most models handled the simple task, Alchemy revealed stark differences in robustness: Gemini 2.5 performed best, followed by Claude 3.7, while ChatGPT-4o and o4-mini struggled. This underscores Alchemy's value as a benchmark. Our findings demonstrate that the biggest challenge for foundation models is not selecting informative actions in the moment, but integrating knowledge through adaptive strategies over time. Encouragingly, there appears to be no intrinsic barrier to future models mastering these abilities.", "link": "https://arxiv.org/abs/2412.06438", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.590062Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2506.10159v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Probabilistic Variational Contrastive Learning", "summary": "arXiv:2506.10159v2 Announce Type: replace-cross Abstract: Deterministic embeddings learned by contrastive learning (CL) methods such as SimCLR and SupCon achieve state-of-the-art performance but lack a principled mechanism for uncertainty quantification. We propose Variational Contrastive Learning (VCL), a decoder-free framework that maximizes the evidence lower bound (ELBO) by interpreting the InfoNCE loss as a surrogate reconstruction term and adding a KL divergence regularizer to a uniform prior on the unit hypersphere. We model the approximate posterior $q_\\theta(z|x)$ as a projected normal distribution, enabling the sampling of probabilistic embeddings. Our two instantiation--VSimCLR and VSupCon--replace deterministic embeddings with samples from $q_\\theta(z|x)$ and incorporate a normalized KL term into the loss. Experiments on multiple benchmarks demonstrate that VCL mitigates dimensional collapse, enhances mutual information with class labels, and matches or outperforms deterministic baselines in classification accuracy, all the while providing meaningful uncertainty estimates through the posterior model. VCL thus equips contrastive learning with a probabilistic foundation, serving as a new basis for contrastive approaches.", "link": "https://arxiv.org/abs/2506.10159", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.590098Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2508.13703v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Minimizing the Weighted Number of Tardy Jobs: Data-Driven Heuristic for Single-Machine Scheduling", "summary": "arXiv:2508.13703v2 Announce Type: replace-cross Abstract: Existing research on single-machine scheduling is largely focused on exact algorithms, which perform well on typical instances but can significantly deteriorate on certain regions of the problem space. In contrast, data-driven approaches provide strong and scalable performance when tailored to the structure of specific datasets. Leveraging this idea, we focus on a single-machine scheduling problem where each job is defined by its weight, duration, due date, and deadline, aiming to minimize the total weight of tardy jobs. We introduce a novel data-driven scheduling heuristic that combines machine learning with problem-specific characteristics, ensuring feasible solutions, which is a common challenge for ML-based algorithms. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art in terms of optimality gap, number of optimal solutions, and adaptability across varied data scenarios, highlighting its flexibility for practical applications. In addition, we conduct a systematic exploration of ML models, addressing a common gap in similar studies by offering a detailed model selection process and providing insights into why the chosen model is the best fit.", "link": "https://arxiv.org/abs/2508.13703", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.590148Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2509.11007v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Gradient Methods with Online Scaling Part II. Practical Aspects", "summary": "arXiv:2509.11007v2 Announce Type: replace-cross Abstract: Part I of this work [Gao25] establishes online scaled gradient methods (OSGM), a framework that utilizes online convex optimization to adapt stepsizes in gradient methods. This paper focuses on the practical aspects of OSGM. We leverage the OSGM framework to design new adaptive first-order methods and provide insights into their empirical behavior. The resulting method, OSGM-Best, matches the performance of quasi-Newton variants while requiring less memory and cheaper iterations. We also extend OSGM to nonconvex optimization and outline directions that connect OSGM to existing branches of optimization theory and practice.", "link": "https://arxiv.org/abs/2509.11007", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.590180Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.00048v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Deep Learning Approaches with Explainable AI for Differentiating Alzheimer Disease and Mild Cognitive Impairment", "summary": "arXiv:2510.00048v2 Announce Type: replace-cross Abstract: Early and accurate diagnosis of Alzheimer Disease is critical for effective clinical intervention, particularly in distinguishing it from Mild Cognitive Impairment, a prodromal stage marked by subtle structural changes. In this study, we propose a hybrid deep learning ensemble framework for Alzheimer Disease classification using structural magnetic resonance imaging. Gray and white matter slices are used as inputs to three pretrained convolutional neural networks such as ResNet50, NASNet, and MobileNet, each fine tuned through an end to end process. To further enhance performance, we incorporate a stacked ensemble learning strategy with a meta learner and weighted averaging to optimally combine the base models. Evaluated on the Alzheimer Disease Neuroimaging Initiative dataset, the proposed method achieves state of the art accuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and 91.0% for Mild Cognitive Impairment vs. Normal Controls, outperforming conventional transfer learning and baseline ensemble methods. To improve interpretability in image based diagnostics, we integrate Explainable AI techniques by Gradient weighted Class Activation, which generates heatmaps and attribution maps that highlight critical regions in gray and white matter slices, revealing structural biomarkers that influence model decisions. These results highlight the frameworks potential for robust and scalable clinical decision support in neurodegenerative disease diagnostics.", "link": "https://arxiv.org/abs/2510.00048", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.590228Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03095v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Distilled Protein Backbone Generation", "summary": "arXiv:2510.03095v2 Announce Type: replace-cross Abstract: Diffusion- and flow-based generative models have recently demonstrated strong performance in protein backbone generation tasks, offering unprecedented capabilities for de novo protein design. However, while achieving notable performance in generation quality, these models are limited by their generating speed, often requiring hundreds of iterative steps in the reverse-diffusion process. This computational bottleneck limits their practical utility in large-scale protein discovery, where thousands to millions of candidate structures are needed. To address this challenge, we explore the techniques of score distillation, which has shown great success in reducing the number of sampling steps in the vision domain while maintaining high generation quality. However, a straightforward adaptation of these methods results in unacceptably low designability. Through extensive study, we have identified how to appropriately adapt Score identity Distillation (SiD), a state-of-the-art score distillation strategy, to train few-step protein backbone generators which significantly reduce sampling time, while maintaining comparable performance to their pretrained teacher model. In particular, multistep generation combined with inference time noise modulation is key to the success. We demonstrate that our distilled few-step generators achieve more than a 20-fold improvement in sampling speed, while achieving similar levels of designability, diversity, and novelty as the Proteina teacher model. This reduction in inference cost enables large-scale in silico protein design, thereby bringing diffusion-based models closer to real-world protein engineering applications.", "link": "https://arxiv.org/abs/2510.03095", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.590282Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03949v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Analysis of kinetic Langevin Monte Carlo under the stochastic exponential Euler discretization from underdamped all the way to overdamped", "summary": "arXiv:2510.03949v2 Announce Type: replace-cross Abstract: Simulating the kinetic Langevin dynamics is a popular approach for sampling from distributions, where only their unnormalized densities are available. Various discretizations of the kinetic Langevin dynamics have been considered, where the resulting algorithm is collectively referred to as the kinetic Langevin Monte Carlo (KLMC) or underdamped Langevin Monte Carlo. Specifically, the stochastic exponential Euler discretization, or exponential integrator for short, has previously been studied under strongly log-concave and log-Lipschitz smooth potentials via the synchronous Wasserstein coupling strategy. Existing analyses, however, impose restrictions on the parameters that do not explain the behavior of KLMC under various choices of parameters. In particular, all known results fail to hold in the overdamped regime, suggesting that the exponential integrator degenerates in the overdamped limit. In this work, we revisit the synchronous Wasserstein coupling analysis of KLMC with the exponential integrator. Our refined analysis results in Wasserstein contractions and bounds on the asymptotic bias that hold under weaker restrictions on the parameters, which assert that the exponential integrator is capable of stably simulating the kinetic Langevin dynamics in the overdamped regime, as long as proper time acceleration is applied.", "link": "https://arxiv.org/abs/2510.03949", "published": "2025-10-08T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "fã­sica cuã¡ntica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T02:41:25.590323Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06261v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning", "summary": "arXiv:2510.06261v1 Announce Type: new Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.", "link": "https://arxiv.org/abs/2510.06261", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.494921Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06274v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization", "summary": "arXiv:2510.06274v1 Announce Type: new Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward problems that require step by step, System2 style reasoning, especially with large language models. Yet, unlike learning, where generalization and out of distribution (OoD) evaluation concepts are well formalized, there is no clear, consistent definition or metric for reasoning ability. We propose Complexity Out of Distribution (Complexity OoD) generalization as a framework and problem setting to define and measure reasoning. A model exhibits Complexity OoD generalization when it maintains performance on test instances whose minimal required solution complexity, either representational (richer solution structure) or computational (more reasoning steps/program length), exceeds that of all training examples. We formalize complexity via solution description Kolmogorov complexity and operational proxies (e.g., object/relation counts; reasoning step counts), clarifying how Complexity OoD differs from length and compositional OoD. This lens unifies learning and reasoning: many cases solvable with System1 like processing at low complexity become System2 like under complexity pressure, while System2 can be viewed as generalization over solution structures. We translate this perspective into practice with recommendations for operationalizing Complexity OoD across the stack: incorporating complexity into benchmark and evaluation metric design, rethinking supervision to target solution traces, seeking and designing inductive biases for Complexity OoD generalization, addressing learning to reason spillovers such as spurious shortcuts, semantic robustness, catastrophic forgetting, and step wise calibration. Because Complexity OoD cannot be solved by scaling data alone, progress toward robust reasoning will require architectures and training regimes that explicitly model and allocate computation with respect to complexity.", "link": "https://arxiv.org/abs/2510.06274", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495030Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06288v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "BuilderBench -- A benchmark for generalist agents", "summary": "arXiv:2510.06288v1 Announce Type: new Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is not surprising that they struggle to solve problems beyond the limits set by existing data. To solve novel problems, agents should acquire skills for exploring and learning through experience. Finding a scalable learning mechanism for developing agents that learn through interaction remains a major open problem. In this work, we introduce BuilderBench, a benchmark to accelerate research into agent pre-training that centers open-ended exploration. BuilderBench requires agents to learn how to build any structure using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated simulator of a robotic agent interacting with various physical blocks, and $(2)$ a task-suite with over 42 diverse target structures that are carefully curated to test an understanding of physics, mathematics, and long-horizon planning. During training, agents have to explore and learn general principles about the environment without any external supervision. During evaluation, agents have to build the unseen target structures from the task suite. Solving these tasks requires a sort of \\emph{embodied reasoning} that is not reflected in words but rather in actions, experimenting with different strategies and piecing them together. Our experiments show that many of these tasks challenge the current iteration of algorithms. Hence, we also provide a ``training wheels'' protocol, in which agents are trained and evaluated to build a single target structure from the task suite. Finally, we provide single-file implementations of six different algorithms as a reference point for researchers.", "link": "https://arxiv.org/abs/2510.06288", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495091Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06302v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration", "summary": "arXiv:2510.06302v1 Announce Type: new Abstract: Post-merger integration states unique challenges for professionals responsible for information system integration aimed on alignment and combination diverse system architectures of merging organizations. Although the theoretical and practical guidance exists for post-merger integration on the business level, there is a significant gap in training for information system integration in this context. In prior research specific methods AMILI (Support method for informed decision identification) and AMILP (Support method for informed decision-making) were introduced for the support of information system integration decisions in the post-merger integration. But during the practical application was reported high learning curve and low learner motivation. This paper explores how game-based learning design can address these limitations by transforming static method training into engaging learning experience. The study analyzes foundational learning theories, cognitive load and motivation models, and serious game design frameworks to identify the essential requirements for a game-based learning design framework tailored to information system integration in post-merger integration. Requirements are structured in two components: the transformation process and resulting learning experience. The paper concludes with a plan for developing and evaluating the proposed framework through iterative design and real-world validation.", "link": "https://arxiv.org/abs/2510.06302", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495141Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06307v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks", "summary": "arXiv:2510.06307v1 Announce Type: new Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural language processing (NLP) tasks through collaboration among multiple agents, where consensus-seeking serves as a fundamental mechanism. However, existing consensus-seeking approaches typically rely on voting mechanisms to judge consensus, overlooking contradictions in system-internal beliefs that destabilize the consensus. Moreover, these methods often involve agents updating their results through indiscriminate collaboration with every other agent. Such uniform interaction fails to identify the optimal collaborators for each agent, hindering the emergence of a stable consensus. To address these challenges, we provide a theoretical framework for selecting optimal collaborators that maximize consensus stability. Based on the theorems, we propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate stable consensus via selecting optimal collaborators and calibrating the consensus judgment by system-internal beliefs. Experimental results on the MATH and MMLU benchmark datasets demonstrate that the proposed BCCS framework outperforms the best existing results by 2.23% and 3.95% of accuracy on challenging tasks, respectively. Our code and data are available at https://github.com/dengwentao99/BCCS.", "link": "https://arxiv.org/abs/2510.06307", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495190Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06410v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?", "summary": "arXiv:2510.06410v1 Announce Type: new Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding strong gains on complex tasks. This transparency also opens a promising direction: multiple reasoners can directly collaborate on each other's thinking within a shared trajectory, yielding better inference efficiency and exploration. A key prerequisite, however, is the ability to assess the usefulness and build on another model's partial thinking -- we call this off-trajectory reasoning. Our paper investigates a critical question: can standard solo-reasoning training pipelines deliver desired off-trajectory behaviors? We propose twin tests that capture the two extremes of the off-trajectory spectrum, namely Recoverability, which tests whether LLMs can backtrack from \"distractions\" induced by misleading reasoning traces, and Guidability, which tests their ability to build upon correct reasoning from stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and reveals a counterintuitive finding -- \"stronger\" LLMs on benchmarks are often more fragile under distraction. Moreover, all models tested fail to effectively leverage guiding steps from collaborators on problems beyond their inherent capabilities with solve rates remaining under 9.2%. Finally, we conduct control studies to isolate the effects of three factors in post-training on these behaviors: the choice of distillation teacher, the use of RL, and data selection strategy. Our results provide actionable insights for training natively strong reasoning collaborators; e.g., we find that suboptimal recoverability behaviors of teacher models are transferred to distilled students even if the distillation trajectories are correct. Taken together, this work lays the groundwork for evaluating multi-model collaborations in shared reasoning trajectories and highlights the limitations of off-the-shelf reasoning LLMs.", "link": "https://arxiv.org/abs/2510.06410", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495243Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06433v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health", "summary": "arXiv:2510.06433v1 Announce Type: new Abstract: The focus on \"food as medicine\" is gaining traction in the field of health and several studies conducted in the past few years discussed this aspect of food in the literature. However, very little research has been done on representing the relationship between food and health in a standardized, machine-readable format using a semantic web that can help us leverage this knowledge effectively. To address this gap, this study aims to create a knowledge graph to link food and health through the knowledge graph's ability to combine information from various platforms focusing on flavonoid contents of food found in the USDA databases and cancer connections found in the literature. We looked closely at these relationships using KNARM methodology and represented them in machine-operable format. The proposed knowledge graph serves as an example for researchers, enabling them to explore the complex interplay between dietary choices and disease management. Future work for this study involves expanding the scope of the knowledge graph by capturing nuances, adding more related data, and performing inferences on the acquired knowledge to uncover hidden relationships.", "link": "https://arxiv.org/abs/2510.06433", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495285Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06475v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles", "summary": "arXiv:2510.06475v1 Announce Type: new Abstract: This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models.", "link": "https://arxiv.org/abs/2510.06475", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495335Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06534v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them", "summary": "arXiv:2510.06534v1 Announce Type: new Abstract: Agentic search leverages large language models (LLMs) to interpret complex user information needs and execute a multi-step process of planning, searching, and synthesizing information to provide answers. This paradigm introduces unique challenges for LLMs' reasoning and agentic capabilities when interacting with retrieval systems and the broader web. In this paper, we propose a reasoning-driven LLM-based pipeline to study effective reasoning behavior patterns in agentic search. Using this pipeline, we analyze successful agentic search trajectories and identify four beneficial reasoning behaviors: Information Verification, Authority Evaluation, Adaptive Search, and Error Recovery. Based on these findings, we propose a technique called Behavior Priming to train more effective agentic search models. It synthesizes agentic search trajectories that exhibit these four behaviors and integrates them into the agentic search model through supervised fine-tuning (SFT), followed by standard reinforcement learning (RL). Experiments on three benchmarks (GAIA, WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models with RL. Crucially, we demonstrate that the desired reasoning behaviors in the SFT data, rather than the correctness of the final answer, is the critical factor for achieving strong final performance after RL: fine-tuning on trajectories with desirable reasoning behaviors but incorrect answers leads to better performance than fine-tuning on trajectories with correct answers. Our analysis further reveals the underlying mechanism: the introduced reasoning behaviors endow models with more effective exploration (higher pass@k and entropy) and test-time scaling (longer trajectories) capabilities, providing a strong foundation for RL. Our code will be released as open source.", "link": "https://arxiv.org/abs/2510.06534", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495389Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06538v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Auto-Prompt Ensemble for LLM Judge", "summary": "arXiv:2510.06538v1 Announce Type: new Abstract: We present a novel framework that improves the reliability of LLM judges by selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM judges often miss crucial evaluation dimensions because they fail to recognize the implicit standards underlying human assessments. To address this challenge, we propose the Auto-Prompt Ensemble (APE), an adaptive framework that automatically learns evaluation dimensions from its failure cases. APE incorporates a confidence-based ensemble mechanism to decide when to adopt the judgments from additional evaluation dimensions through a novel confidence estimation approach called Collective Confidence. Extensive experiments demonstrate that APE improves the reliability of LLM Judge across diverse standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a principled approach for LLM Judge to leverage test-time computation, and bridge the evaluation gap between human and LLM judges.", "link": "https://arxiv.org/abs/2510.06538", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495425Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06587v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks", "summary": "arXiv:2510.06587v1 Announce Type: new Abstract: Large language model (LLM) agents are becoming competent at straightforward web tasks, such as opening an item page or submitting a form, but still struggle with objectives that require long horizon navigation, large scale information extraction, and reasoning under constraints. We present WebDART, a general framework that enables a single LLM to handle such complex chores. WebDART (i) dynamically decomposes each objective into three focused subtasks: navigation, information extraction, and execution, so the model concentrates on one skill at a time, and (ii) continuously replans the decomposition as new webpages are revealed, taking advantage of newly discovered filters or shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena, WebDART lifts success rates by up to 13.7 percentage points over previous SOTA agents, while matching their performance on the easier WebArena suite and completing tasks with up to 14.7 fewer navigation steps.", "link": "https://arxiv.org/abs/2510.06587", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495460Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06600v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Fine-Grained Emotion Recognition via In-Context Learning", "summary": "arXiv:2510.06600v1 Announce Type: new Abstract: Fine-grained emotion recognition aims to identify the emotional type in queries through reasoning and decision-making processes, playing a crucial role in various systems. Recent methods use In-Context Learning (ICL), enhancing the representation of queries in the reasoning process through semantically similar examples, while further improving emotion recognition by explaining the reasoning mechanisms. However, these methods enhance the reasoning process but overlook the decision-making process. This paper investigates decision-making in fine-grained emotion recognition through prototype theory. We show that ICL relies on similarity matching between query representations and emotional prototypes within the model, where emotion-accurate representations are critical. However, semantically similar examples often introduce emotional discrepancies, hindering accurate representations and causing errors. To address this, we propose Emotion In-Context Learning (EICL), which introduces emotionally similar examples and uses a dynamic soft-label strategy to improve query representations in the emotion reasoning process. A two-stage exclusion strategy is then employed to assess similarity from multiple angles, further optimizing the decision-making process. Extensive experiments show that EICL significantly outperforms ICL on multiple datasets.", "link": "https://arxiv.org/abs/2510.06600", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495500Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06674v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support", "summary": "arXiv:2510.06674v1 Announce Type: new Abstract: We introduce an Agent-in-the-Loop (AITL) framework that implements a continuous data flywheel for iteratively improving an LLM-based customer support system. Unlike standard offline approaches that rely on batch annotations, AITL integrates four key types of annotations directly into live customer operations: (1) pairwise response preferences, (2) agent adoption and rationales, (3) knowledge relevance checks, and (4) identification of missing knowledge. These feedback signals seamlessly feed back into models' updates, reducing retraining cycles from months to weeks. Our production pilot involving US-based customer support agents demonstrated significant improvements in retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality (+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore the effectiveness of embedding human feedback loops directly into operational workflows to continuously refine LLM-based customer support system.", "link": "https://arxiv.org/abs/2510.06674", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495534Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06711v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Inefficiencies of Meta Agents for Agent Design", "summary": "arXiv:2510.06711v1 Announce Type: new Abstract: Recent works began to automate the design of agentic systems using meta-agents that propose and iteratively refine new agent architectures. In this paper, we examine three key challenges in a common class of meta-agents. First, we investigate how a meta-agent learns across iterations and find that simply expanding the context with all previous agents, as proposed by previous works, performs worse than ignoring prior designs entirely. We show that the performance improves with an evolutionary approach. Second, although the meta-agent designs multiple agents during training, it typically commits to a single agent at test time. We find that the designed agents have low behavioral diversity, limiting the potential for their complementary use. Third, we assess when automated design is economically viable. We find that only in a few cases--specifically, two datasets--the overall cost of designing and deploying the agents is lower than that of human-designed agents when deployed on over 15,000 examples. In contrast, the performance gains for other datasets do not justify the design cost, regardless of scale.", "link": "https://arxiv.org/abs/2510.06711", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495576Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06742v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models", "summary": "arXiv:2510.06742v1 Announce Type: new Abstract: The advent of large language models (LLMs) has revolutionized the integration of knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming limitations in traditional machine learning methods for capturing intricate semantic links among genes, diseases, and cognitive processes. We introduce MultiCNKG, an innovative framework that merges three key knowledge sources: the Cognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges across 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes and 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO) comprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types. Leveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity computation, and graph augmentation to create a cohesive KG that interconnects genetic mechanisms, neurological disorders, and cognitive functions. The resulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes, Diseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes, Associated with, Regulates), facilitating a multi-layered view from molecular to behavioral domains. Assessments using metrics such as precision (85.20%), recall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty detection (40.28%), and expert validation (89.50%) affirm its robustness and coherence. Link prediction evaluations with models like TransE (MR: 391, MRR: 0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against benchmarks like FB15k-237 and WN18RR. This KG advances applications in personalized medicine, cognitive disorder diagnostics, and hypothesis formulation in cognitive neuroscience.", "link": "https://arxiv.org/abs/2510.06742", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495628Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06756v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Verifying Memoryless Sequential Decision-making of Large Language Models", "summary": "arXiv:2510.06756v1 Announce Type: new Abstract: We introduce a tool for rigorous and automated verification of large language model (LLM)- based policies in memoryless sequential decision-making tasks. Given a Markov decision process (MDP) representing the sequential decision-making task, an LLM policy, and a safety requirement expressed as a PCTL formula, our approach incrementally constructs only the reachable portion of the MDP guided by the LLM's chosen actions. Each state is encoded as a natural language prompt, the LLM's response is parsed into an action, and reachable successor states by the policy are expanded. The resulting formal model is checked with Storm to determine whether the policy satisfies the specified safety property. In experiments on standard grid world benchmarks, we show that open source LLMs accessed via Ollama can be verified when deterministically seeded, but generally underperform deep reinforcement learning baselines. Our tool natively integrates with Ollama and supports PRISM-specified tasks, enabling continuous benchmarking in user-specified sequential decision-making tasks and laying a practical foundation for formally verifying increasingly capable LLMs.", "link": "https://arxiv.org/abs/2510.06756", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495666Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06761v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration", "summary": "arXiv:2510.06761v1 Announce Type: new Abstract: Automating the end-to-end scientific research process poses a fundamental challenge: it requires both evolving high-level plans that are novel and sound, and executing these plans correctly amidst dynamic and uncertain conditions. To address this bilevel challenge, we propose a novel Double-Loop Multi-Agent (DLMA) framework to solve the given research problem automatically. The leader loop, composed of professor agents, is responsible for evolving research plans. It employs an evolutionary algorithm through involvement, improvement, and integration meetings to iteratively generate and refine a pool of research proposals, exploring the solution space effectively. The follower loop, composed of doctoral student agents, is responsible for executing the best-evolved plan. It dynamically adjusts the plan during implementation via pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is well-supported by contextual and external observations. Extensive experiments on benchmarks like ACLAward and Laboratory show that DLMA generates research papers that achieve state-of-the-art scores in automated evaluation, significantly outperforming strong baselines. Ablation studies confirm the critical roles of both loops, with evolution driving novelty and execution ensuring soundness.", "link": "https://arxiv.org/abs/2510.06761", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495706Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06857v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Autoformalizer with Tool Feedback", "summary": "arXiv:2510.06857v1 Announce Type: new Abstract: Autoformalization addresses the scarcity of data for Automated Theorem Proving (ATP) by translating mathematical problems from natural language into formal statements. Efforts in recent work shift from directly prompting large language models to training an end-to-end formalizer model from scratch, achieving remarkable advancements. However, existing formalizer still struggles to consistently generate valid statements that meet syntactic validity and semantic consistency. To address this issue, we propose the Autoformalizer with Tool Feedback (ATF), a novel approach that incorporates syntactic and consistency information as tools into the formalization process. By integrating Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge approach for consistency validation, the model is able to adaptively refine generated statements according to the tool feedback, enhancing both syntactic validity and semantic consistency. The training of ATF involves a cold-start phase on synthetic tool-calling data, an expert iteration phase to improve formalization capabilities, and Direct Preference Optimization to alleviate ineffective revisions. Experimental results show that ATF markedly outperforms a range of baseline formalizer models, with its superior performance further validated by human evaluations. Subsequent analysis reveals that ATF demonstrates excellent inference scaling properties. Moreover, we open-source Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate advancements in autoformalization and ATP research.", "link": "https://arxiv.org/abs/2510.06857", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495755Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06878v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs", "summary": "arXiv:2510.06878v1 Announce Type: new Abstract: Iterative refinement has been a promising paradigm to enable large language models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of the key challenges, however, is how to effectively search through the enormous search space of possible refinements. Existing methods typically fall back on predefined heuristics, which are troubled by the exploration-exploitation dilemma and cannot adapt based on past refinement outcomes. We introduce Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with a Thompson-Sampling-based tree search. TGPR explores both failed and successful refinement paths actively, with denser training trajectories and more adaptive policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to +4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to +12.51 percentage points absolute improvement in pass@10 (on APPS) compared to a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a principled approach to combining learned policies with structured search methods, offering a general framework for enhancing iterative refinement and stateful reasoning in LLMs.", "link": "https://arxiv.org/abs/2510.06878", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495797Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06911v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN", "summary": "arXiv:2510.06911v1 Announce Type: new Abstract: There are many established semantic Web standards for implementing multi-agent driven applications. The AJAN framework allows to engineer multi-agent systems based on these standards. In particular, agent knowledge is represented in RDF/RDFS and OWL, while agent behavior models are defined with Behavior Trees and SPARQL to access and manipulate this knowledge. However, the appropriate definition of RDF/RDFS and SPARQL-based agent behaviors still remains a major hurdle not only for agent modelers in practice. For example, dealing with URIs is very error-prone regarding typos and dealing with complex SPARQL queries in large-scale environments requires a high learning curve. In this paper, we present an integrated development environment to overcome such hurdles of modeling AJAN agents and at the same time to extend the user community for AJAN by the possibility to leverage Large Language Models for agent engineering.", "link": "https://arxiv.org/abs/2510.06911", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495834Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06953v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces", "summary": "arXiv:2510.06953v1 Announce Type: new Abstract: The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stable flow of information. In this work, we revisit this principle in the context of large language model (LLM) reasoning traces, asking whether step-level uniformity reflects reasoning quality. To this end, we propose an entropy-based stepwise information density metric and introduce two complementary measures of uniformity, local and global uniformity scores. Across the experiments on six different reasoning benchmarks, we find that step-level uniformity not only provides a strong theoretical lens but also yields practical performance benefits; for example, selecting reasoning traces with more uniform information density at the step-level improves accuracy by 10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals that correct reasoning traces tend to avoid sharp information density spikes, while incorrect traces exhibit irregular information bursts. These results demonstrate that UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality. Results highlight the uniformity of the information density as a robust diagnostic and selection criterion for building more reliable and accurate reasoning systems.", "link": "https://arxiv.org/abs/2510.06953", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495876Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.07038v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning", "summary": "arXiv:2510.07038v1 Announce Type: new Abstract: Recent advances in large language models (LLMs) have popularized test-time scaling, where models generate additional reasoning tokens before producing final answers. These approaches have demonstrated significant performance improvements on benchmarks involving mathematical reasoning. However, language models relying solely on direct inference still struggle with tasks demanding up-to-date knowledge or computational tools such as calculators and code interpreters for complex arithmetic operations. To overcome these limitations, we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement learning framework that systematically integrates multi-hop reasoning with adaptive tool-calling capabilities. Our approach employs a modified version of Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm, which we adapt specifically for tool invocation scenarios, enabling models to dynamically interleave complex reasoning with on-demand tool usage (including search APIs and Python interpreters). To support this research, we introduce two new datasets: TAPO-easy-60K and TAPO-hard-18K, specifically designed to train and evaluate both fact-based reasoning and mathematical calculation capabilities. Our experiments on Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach, with both models achieving state-of-the-art performance on tasks requiring external knowledge and mathematical computation among methods with comparable parameters. Notably, TAPO achieves more efficient tool utilization than baseline methods while preventing excessive calls caused by reward hacking. These results highlight the significant potential of combining advanced reasoning with tool usage to enhance model performance in knowledge-intensive and computationally demanding tasks.", "link": "https://arxiv.org/abs/2510.07038", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495938Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.07064v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations", "summary": "arXiv:2510.07064v1 Announce Type: new Abstract: The difficulty and expense of obtaining large-scale human responses make Large Language Models (LLMs) an attractive alternative and a promising proxy for human behavior. However, prior work shows that LLMs often produce homogeneous outputs that fail to capture the rich diversity of human perspectives and behaviors. Thus, rather than trying to capture this diversity with a single LLM agent, we propose a novel framework to construct a set of agents that collectively capture the diversity of a given human population. Each agent is an LLM whose behavior is steered by conditioning on a small set of human demonstrations (task-response pairs) through in-context learning. The central challenge is therefore to select a representative set of LLM agents from the exponentially large space of possible agents. We tackle this selection problem from the lens of submodular optimization. In particular, we develop methods that offer different trade-offs regarding time complexity and performance guarantees. Extensive experiments in crowdsourcing and educational domains demonstrate that our approach constructs agents that more effectively represent human populations compared to baselines. Moreover, behavioral analyses on new tasks show that these agents reproduce the behavior patterns and perspectives of the students and annotators they are designed to represent.", "link": "https://arxiv.org/abs/2510.07064", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.495981Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.07069v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Inductive Learning for Possibilistic Logic Programs Under Stable Models", "summary": "arXiv:2510.07069v1 Announce Type: new Abstract: Possibilistic logic programs (poss-programs) under stable models are a major variant of answer set programming (ASP). While its semantics (possibilistic stable models) and properties have been well investigated, the problem of inductive reasoning has not been investigated yet. This paper presents an approach to extracting poss-programs from a background program and examples (parts of intended possibilistic stable models). To this end, the notion of induction tasks is first formally defined, its properties are investigated and two algorithms ilpsm and ilpsmmin for computing induction solutions are presented. An implementation of ilpsmmin is also provided and experimental results show that when inputs are ordinary logic programs, the prototype outperforms a major inductive learning system for normal logic programs from stable models on the datasets that are randomly generated.", "link": "https://arxiv.org/abs/2510.07069", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496015Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.07073v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems", "summary": "arXiv:2510.07073v1 Announce Type: new Abstract: Designing high-performing heuristics for vehicle routing problems (VRPs) is a complex task that requires both intuition and deep domain knowledge. Large language model (LLM)-based code generation has recently shown promise across many domains, but it still falls short of producing heuristics that rival those crafted by human experts. In this paper, we propose VRPAgent, a framework that integrates LLM-generated components into a metaheuristic and refines them through a novel genetic search. By using the LLM to generate problem-specific operators, embedded within a generic metaheuristic framework, VRPAgent keeps tasks manageable, guarantees correctness, and still enables the discovery of novel and powerful strategies. Across multiple problems, including the capacitated VRP, the VRP with time windows, and the prize-collecting VRP, our method discovers heuristic operators that outperform handcrafted methods and recent learning-based approaches while requiring only a single CPU core. To our knowledge, \\VRPAgent is the first LLM-based paradigm to advance the state-of-the-art in VRPs, highlighting a promising future for automated heuristics discovery.", "link": "https://arxiv.org/abs/2510.07073", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496056Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.07091v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas", "summary": "arXiv:2510.07091v1 Announce Type: new Abstract: Enabling LLMs to effectively operate long-horizon task which requires long-term planning and multiple interactions is essential for open-world autonomy. Conventional methods adopt planning with actions where a executable action list would be provided as reference. However, this action representation choice would be impractical when the environment action space is combinatorial exploded (e.g., open-ended real world). This naturally leads to a question: As environmental action space scales, what is the optimal action representation for long-horizon agents? In this paper, we systematically study the effectiveness of two different action representations. The first one is conventional planning with actions (PwA) which is predominantly adopted for its effectiveness on existing benchmarks. The other one is planning with schemas (PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ] to [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable scalability. This alternative is motivated by its alignment with human cognition and its compliance with environment-imposed action format restriction. We propose cognitive bandwidth perspective as a conceptual framework to qualitatively understand the differences between these two action representations and empirically observe a representation-choice inflection point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve as evidence of the need for scalable representations. We further conduct controlled experiments to study how the location of this inflection point interacts with different model capacities: stronger planning proficiency shifts the inflection rightward, whereas better schema instantiation shifts it leftward. Finally, noting the suboptimal performance of PwS agents, we provide an actionable guide for building more capable PwS agents for better scalable autonomy.", "link": "https://arxiv.org/abs/2510.07091", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496112Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.07117v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "The Contingencies of Physical Embodiment Allow for Open-Endedness and Care", "summary": "arXiv:2510.07117v1 Announce Type: new Abstract: Physical vulnerability and mortality are often seen as obstacles to be avoided in the development of artificial agents, which struggle to adapt to open-ended environments and provide aligned care. Meanwhile, biological organisms survive, thrive, and care for each other in an open-ended physical world with relative ease and efficiency. Understanding the role of the conditions of life in this disparity can aid in developing more robust, adaptive, and caring artificial agents. Here we define two minimal conditions for physical embodiment inspired by the existentialist phenomenology of Martin Heidegger: being-in-the-world (the agent is a part of the environment) and being-towards-death (unless counteracted, the agent drifts toward terminal states due to the second law of thermodynamics). We propose that from these conditions we can obtain both a homeostatic drive - aimed at maintaining integrity and avoiding death by expending energy to learn and act - and an intrinsic drive to continue to do so in as many ways as possible. Drawing inspiration from Friedrich Nietzsche's existentialist concept of will-to-power, we examine how intrinsic drives to maximize control over future states, e.g., empowerment, allow agents to increase the probability that they will be able to meet their future homeostatic needs, thereby enhancing their capacity to maintain physical integrity. We formalize these concepts within a reinforcement learning framework, which enables us to examine how intrinsically driven embodied agents learning in open-ended multi-agent environments may cultivate the capacities for open-endedness and care.ov", "link": "https://arxiv.org/abs/2510.07117", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496162Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.07161v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Integrating Domain Knowledge into Process Discovery Using Large Language Models", "summary": "arXiv:2510.07161v1 Announce Type: new Abstract: Process discovery aims to derive process models from event logs, providing insights into operational behavior and forming a foundation for conformance checking and process improvement. However, models derived solely from event data may not accurately reflect the real process, as event logs are often incomplete or affected by noise, and domain knowledge, an important complementary resource, is typically disregarded. As a result, the discovered models may lack reliability for downstream tasks. We propose an interactive framework that incorporates domain knowledge, expressed in natural language, into the process discovery pipeline using Large Language Models (LLMs). Our approach leverages LLMs to extract declarative rules from textual descriptions provided by domain experts. These rules are used to guide the IMr discovery algorithm, which recursively constructs process models by combining insights from both the event log and the extracted rules, helping to avoid problematic process structures that contradict domain knowledge. The framework coordinates interactions among the LLM, domain experts, and a set of backend services. We present a fully implemented tool that supports this workflow and conduct an extensive evaluation of multiple LLMs and prompt engineering strategies. Our empirical study includes a case study based on a real-life event log with the involvement of domain experts, who assessed the usability and effectiveness of the framework.", "link": "https://arxiv.org/abs/2510.07161", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496207Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.07172v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents", "summary": "arXiv:2510.07172v1 Announce Type: new Abstract: Large language models are emerging as powerful tools for scientific law discovery, a foundational challenge in AI-driven science. However, existing benchmarks for this task suffer from a fundamental methodological trilemma, forcing a trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using metaphysical shifts - systematic alterations of canonical laws - to generate a vast suite of problems that are scalable, scientifically relevant, and memorization-resistant. Moreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive experiment reveals a clear but fragile capability for discovery in frontier LLMs: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover a paradoxical effect of tool assistance: providing a code interpreter can hinder more capable models by inducing a premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge. By providing a scalable, robust, and scientifically authentic testbed, NewtonBench offers a crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery.", "link": "https://arxiv.org/abs/2510.07172", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496260Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.07276v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences", "summary": "arXiv:2510.07276v1 Announce Type: new Abstract: Many real-world scenarios require multiple agents to coordinate in shared environments, while balancing trade-offs between multiple, potentially competing objectives. Current multi-objective multi-agent path finding (MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto frontiers. They do not explicitly optimize for user-defined preferences, even when the preferences are available, and scale poorly with the number of objectives. We propose a lexicographic framework for modeling MO-MAPF, along with an algorithm \\textit{Lexicographic Conflict-Based Search} (LCBS) that directly computes a single solution aligned with a lexicographic preference over objectives. LCBS integrates a priority-aware low-level $A^*$ search with conflict-based search, avoiding Pareto frontier construction and enabling efficient planning guided by preference over objectives. We provide insights into optimality and scalability, and empirically demonstrate that LCBS computes optimal solutions while scaling to instances with up to ten objectives -- far beyond the limits of existing MO-MAPF methods. Evaluations on standard and randomized MAPF benchmarks show consistently higher success rates against state-of-the-art baselines, especially with increasing number of objectives.", "link": "https://arxiv.org/abs/2510.07276", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496301Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.07297v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Agentic generative AI for media content discovery at the national football league", "summary": "arXiv:2510.07297v1 Announce Type: new Abstract: Generative AI has unlocked new possibilities in content discovery and management. Through collaboration with the National Football League (NFL), we demonstrate how a generative-AI based workflow enables media researchers and analysts to query relevant historical plays using natural language rather than traditional filter-and-click interfaces. The agentic workflow takes a user query as input, breaks it into elements, and translates them into the underlying database query language. Accuracy and latency are further improved through carefully designed semantic caching. The solution achieves over 95 percent accuracy and reduces the average time to find relevant videos from 10 minutes to 30 seconds, significantly increasing the NFL's operational efficiency and allowing users to focus on producing creative content and engaging storylines.", "link": "https://arxiv.org/abs/2510.07297", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496333Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.04452v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "AgentBuilder: Exploring Scaffolds for Prototyping User Experiences of Interface Agents", "summary": "arXiv:2510.04452v1 Announce Type: cross Abstract: Interface agents powered by generative AI models (referred to as \"agents\") can automate actions based on user commands. An important aspect of developing agents is their user experience (i.e., agent experience). There is a growing need to provide scaffolds for a broader set of individuals beyond AI engineers to prototype agent experiences, since they can contribute valuable perspectives to designing agent experiences. In this work, we explore the affordances agent prototyping systems should offer by conducting a requirements elicitation study with 12 participants with varying experience with agents. We identify key activities in agent experience prototyping and the desired capabilities of agent prototyping systems. We instantiate those capabilities in the AgentBuilder design probe for agent prototyping. We conduct an in situ agent prototyping study with 14 participants using AgentBuilder to validate the design requirements and elicit insights on how developers prototype agents and what their needs are in this process.", "link": "https://arxiv.org/abs/2510.04452", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496469Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.05336v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives", "summary": "arXiv:2510.05336v1 Announce Type: cross Abstract: Historical archives on weather events are collections of enduring primary source records that offer rich, untapped narratives of how societies have experienced and responded to extreme weather events. These qualitative accounts provide insights into societal vulnerability and resilience that are largely absent from meteorological records, making them valuable for climate scientists to understand societal responses. However, their vast scale, noisy digitized quality, and archaic language make it difficult to transform them into structured knowledge for climate research. To address this challenge, we introduce WeatherArchive-Bench, the first benchmark for evaluating retrieval-augmented generation (RAG) systems on historical weather archives. WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which measures a system's ability to locate historically relevant passages from over one million archival news segments, and WeatherArchive-Assessment, which evaluates whether Large Language Models (LLMs) can classify societal vulnerability and resilience indicators from extreme weather narratives. Extensive experiments across sparse, dense, and re-ranking retrievers, as well as a diverse set of LLMs, reveal that dense retrievers often fail on historical terminology, while LLMs frequently misinterpret vulnerability and resilience concepts. These findings highlight key limitations in reasoning about complex societal indicators and provide insights for designing more robust climate-focused RAG systems from archival contexts. The constructed dataset and evaluation framework are publicly available at https://anonymous.4open.science/r/WeatherArchive-Bench/.", "link": "https://arxiv.org/abs/2510.05336", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496519Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06223v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "A Multimodal GUI Architecture for Interfacing with LLM-Based Conversational Assistants", "summary": "arXiv:2510.06223v1 Announce Type: cross Abstract: Advances in large language models (LLMs) and real-time speech recognition now make it possible to issue any graphical user interface (GUI) action through natural language and receive the corresponding system response directly through the GUI. Most production applications were never designed with speech in mind. This article provides a concrete architecture that enables GUIs to interface with LLM-based speech-enabled assistants. The architecture makes an application's navigation graph and semantics available through the Model Context Protocol (MCP). The ViewModel, part of the MVVM (Model-View-ViewModel) pattern, exposes the application's capabilities to the assistant by supplying both tools applicable to a currently visible view and application-global tools extracted from the GUI tree router. This architecture facilitates full voice accessibility while ensuring reliable alignment between spoken input and the visual interface, accompanied by consistent feedback across modalities. It future-proofs apps for upcoming OS super assistants that employ computer use agents (CUAs) and natively consume MCP if an application provides it. To address concerns about privacy and data security, the practical effectiveness of locally deployable, open-weight LLMs for speech-enabled multimodal UIs is evaluated. Findings suggest that recent smaller open-weight models approach the performance of leading proprietary models in overall accuracy and require enterprise-grade hardware for fast responsiveness.", "link": "https://arxiv.org/abs/2510.06223", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496568Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06224v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Exploring Human-AI Collaboration Using Mental Models of Early Adopters of Multi-Agent Generative AI Tools", "summary": "arXiv:2510.06224v1 Announce Type: cross Abstract: With recent advancements in multi-agent generative AI (Gen AI), technology organizations like Microsoft are adopting these complex tools, redefining AI agents as active collaborators in complex workflows rather than as passive tools. In this study, we investigated how early adopters and developers conceptualize multi-agent Gen AI tools, focusing on how they understand human-AI collaboration mechanisms, general collaboration dynamics, and transparency in the context of AI tools. We conducted semi-structured interviews with 13 developers, all early adopters of multi-agent Gen AI technology who work at Microsoft. Our findings revealed that these early adopters conceptualize multi-agent systems as \"teams\" of specialized role-based and task-based agents, such as assistants or reviewers, structured similar to human collaboration models and ranging from AI-dominant to AI-assisted, user-controlled interactions. We identified key challenges, including error propagation, unpredictable and unproductive agent loop behavior, and the need for clear communication to mitigate the layered transparency issues. Early adopters' perspectives about the role of transparency underscored its importance as a way to build trust, verify and trace errors, and prevent misuse, errors, and leaks. The insights and design considerations we present contribute to CSCW research about collaborative mechanisms with capabilities ranging from AI-dominant to AI-assisted interactions, transparency and oversight strategies in human-agent and agent-agent interactions, and how humans make sense of these multi-agent systems as dynamic, role-diverse collaborators which are customizable for diverse needs and workflows. We conclude with future research directions that extend CSCW approaches to the design of inter-agent and human mediation interactions.", "link": "https://arxiv.org/abs/2510.06224", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496618Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06225v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Generalized Multi-agent Social Simulation Framework", "summary": "arXiv:2510.06225v1 Announce Type: cross Abstract: Multi-agent social interaction has clearly benefited from Large Language Models. However, current simulation systems still face challenges such as difficulties in scaling to diverse scenarios and poor reusability due to a lack of modular design. To address these issues, we designed and developed a modular, object-oriented framework that organically integrates various base classes through a hierarchical structure, harvesting scalability and reusability. We inherited the framework to realize common derived classes. Additionally, a memory summarization mechanism is proposed to filter and distill relevant information from raw memory data, prioritizing contextually salient events and interactions. By selecting and combining some necessary derived classes, we customized a specific simulated environment. Utilizing this simulated environment, we successfully simulated human interactions on social media, replicating real-world online social behaviors. The source code for the project will be released and evolve.", "link": "https://arxiv.org/abs/2510.06225", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496652Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06235v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Stacked Regression using Off-the-shelf, Stimulus-tuned and Fine-tuned Neural Networks for Predicting fMRI Brain Responses to Movies (Algonauts 2025 Report)", "summary": "arXiv:2510.06235v1 Announce Type: cross Abstract: We present our submission to the Algonauts 2025 Challenge, where the goal is to predict fMRI brain responses to movie stimuli. Our approach integrates multimodal representations from large language models, video encoders, audio models, and vision-language models, combining both off-the-shelf and fine-tuned variants. To improve performance, we enhanced textual inputs with detailed transcripts and summaries, and we explored stimulus-tuning and fine-tuning strategies for language and vision models. Predictions from individual models were combined using stacked regression, yielding solid results. Our submission, under the team name Seinfeld, ranked 10th. We make all code and resources publicly available, contributing to ongoing efforts in developing multimodal encoding models for brain activity.", "link": "https://arxiv.org/abs/2510.06235", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496693Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06238v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout", "summary": "arXiv:2510.06238v1 Announce Type: cross Abstract: Detecting surface landmines and unexploded ordnances (UXOs) using deep learning has shown promise in humanitarian demining. However, deterministic neural networks can be vulnerable to noisy conditions and adversarial attacks, leading to missed detection or misclassification. This study introduces the idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated into a fine-tuned ResNet-50 architecture for surface landmine and UXO classification, which was tested on a simulated dataset. Integrating the MC Dropout approach helps quantify epistemic uncertainty, providing an additional metric for prediction reliability, which could be helpful to make more informed decisions in demining operations. Experimental results on clean, adversarially perturbed, and noisy test images demonstrate the model's ability to flag unreliable predictions under challenging conditions. This proof-of-concept study highlights the need for uncertainty quantification in demining, raises awareness about the vulnerability of existing neural networks in demining to adversarial threats, and emphasizes the importance of developing more robust and reliable models for practical applications.", "link": "https://arxiv.org/abs/2510.06238", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496732Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06240v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets", "summary": "arXiv:2510.06240v1 Announce Type: cross Abstract: Industrial question-answering (QA) systems require higher safety and reliability than general-purpose dialogue models, as errors in high-risk scenarios such as equipment fault diagnosis can have severe consequences. Although multi-agent large language models enhance reasoning depth, they suffer from uncontrolled iterations and unverifiable outputs, and conventional distillation methods struggle to transfer collaborative reasoning capabilities to lightweight, deployable student models. To address these challenges, we propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our approach formulates distillation as a Markov Decision Process and incorporates a knowledge graph as a verifiable structured prior to enrich state representation and ensure convergence. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence instruction-tuning data and jointly distills reasoning depth and verifiability into compact student models suitable for edge deployment. Experiments on an industrial QA dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent over baselines and significantly enhances reliability, enabling trustworthy AI deployment in safety-critical industrial scenarios. Code and data are available at https://github.com/erwinmsmith/KG-MAD/.", "link": "https://arxiv.org/abs/2510.06240", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496772Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06243v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning", "summary": "arXiv:2510.06243v1 Announce Type: cross Abstract: Referring Expression Comprehension and Segmentation are critical tasks for assessing the integration of language understanding and image comprehension, serving as benchmarks for Multimodal Large Language Models (MLLMs) capabilities. To address these challenges, we propose a new strategy, CoT Referring, which enhances model reasoning across modalities through a structured, chain-of-thought training data structure. Our approach systematically parses textual structures to a sequential referring step, where in each step it identifies relationships and ensures consistent reference alignment, thereby improving accuracy in complex query scenarios. We restructure the training data to enforce a new output form, providing new annotations for existing datasets and compiling an evaluation benchmark from existing resources. This benchmark is designed explicitly for complex referring cases. We also integrate detection and segmentation capabilities into a unified MLLM framework, training it with a novel adaptive weighted loss to optimize performance. Experimental results on our curated benchmark and RefCOCO/+/g demonstrate the effectiveness of our approach, with a notable increase of 2.5%+ over baseline models.", "link": "https://arxiv.org/abs/2510.06243", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496855Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06244v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Evaluating Embedding Frameworks for Scientific Domain", "summary": "arXiv:2510.06244v1 Announce Type: cross Abstract: Finding an optimal word representation algorithm is particularly important in terms of domain specific data, as the same word can have different meanings and hence, different representations depending on the domain and context. While Generative AI and transformer architecture does a great job at generating contextualized embeddings for any given work, they are quite time and compute extensive, especially if we were to pre-train such a model from scratch. In this work, we focus on the scientific domain and finding the optimal word representation algorithm along with the tokenization method that could be used to represent words in the scientific domain. The goal of this research is two fold: 1) finding the optimal word representation and tokenization methods that can be used in downstream scientific domain NLP tasks, and 2) building a comprehensive evaluation suite that could be used to evaluate various word representation and tokenization algorithms (even as new ones are introduced) in the scientific domain. To this end, we build an evaluation suite consisting of several downstream tasks and relevant datasets for each task. Furthermore, we use the constructed evaluation suite to test various word representation and tokenization algorithms.", "link": "https://arxiv.org/abs/2510.06244", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496899Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06249v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B", "summary": "arXiv:2510.06249v1 Announce Type: cross Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact (MMLoSo) Language Challenge addresses one of India's most pressing linguistic gaps: the lack of resources for its diverse low-resource languages (LRLs). In this study, we investigate whether enforcing cross-lingual similarity in specific internal layers of a decoder-only multilingual large language model (LLM) can improve translation quality from LRL to high-resource language (HRL). Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric that encourages representations of different languages to align, with REPINA, a regularization method that constrains parameter updates to remain close to the pretrained model, into a joint method we call TRepLiNa. In this research project, we experiment with zero-shot, few-shot, and fine-tuning settings using Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari, Santali, Bhili) with Hindi/English pivots. Our results show that aligning mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach to improving LRL translation, especially in data-scarce settings.", "link": "https://arxiv.org/abs/2510.06249", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.496980Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06250v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Scalable multilingual PII annotation for responsible AI in LLMs", "summary": "arXiv:2510.06250v1 Announce Type: cross Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable handling of Personally Identifiable Information (PII) across diverse regulatory contexts has become essential. This work introduces a scalable multilingual data curation framework designed for high-quality PII annotation across 13 underrepresented locales, covering approximately 336 locale-specific PII types. Our phased, human-in-the-loop annotation methodology combines linguistic expertise with rigorous quality assurance, leading to substantial improvements in recall and false positive rates from pilot, training, and production phases. By leveraging inter-annotator agreement metrics and root-cause analysis, the framework systematically uncovers and resolves annotation inconsistencies, resulting in high-fidelity datasets suitable for supervised LLM fine-tuning. Beyond reporting empirical gains, we highlight common annotator challenges in multilingual PII labeling and demonstrate how iterative, analytics-driven pipelines can enhance both annotation quality and downstream model reliability.", "link": "https://arxiv.org/abs/2510.06250", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.497023Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06252v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Dream2Image : An Open Multimodal EEG Dataset for Decoding and Visualizing Dreams with Artificial Intelligence", "summary": "arXiv:2510.06252v1 Announce Type: cross Abstract: Dream2Image is the world's first dataset combining EEG signals, dream transcriptions, and AI-generated images. Based on 38 participants and more than 31 hours of dream EEG recordings, it contains 129 samples offering: the final seconds of brain activity preceding awakening (T-15, T-30, T-60, T-120), raw reports of dream experiences, and an approximate visual reconstruction of the dream. This dataset provides a novel resource for dream research, a unique resource to study the neural correlates of dreaming, to develop models for decoding dreams from brain activity, and to explore new approaches in neuroscience, psychology, and artificial intelligence. Available in open access on Hugging Face and GitHub, Dream2Image provides a multimodal resource designed to support research at the interface of artificial intelligence and neuroscience. It was designed to inspire researchers and extend the current approaches to brain activity decoding. Limitations include the relatively small sample size and the variability of dream recall, which may affect generalizability.", "link": "https://arxiv.org/abs/2510.06252", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.497064Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06253v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "LLM-Driven Rubric-Based Assessment of Algebraic Competence in Multi-Stage Block Coding Tasks with Design and Field Evaluation", "summary": "arXiv:2510.06253v1 Announce Type: cross Abstract: As online education platforms continue to expand, there is a growing need for assessment methods that not only measure answer accuracy but also capture the depth of students' cognitive processes in alignment with curriculum objectives. This study proposes and evaluates a rubric-based assessment framework powered by a large language model (LLM) for measuring algebraic competence, real-world-context block coding tasks. The problem set, designed by mathematics education experts, aligns each problem segment with five predefined rubric dimensions, enabling the LLM to assess both correctness and quality of students' problem-solving processes. The system was implemented on an online platform that records all intermediate responses and employs the LLM for rubric-aligned achievement evaluation. To examine the practical effectiveness of the proposed framework, we conducted a field study involving 42 middle school students engaged in multi-stage quadratic equation tasks with block coding. The study integrated learner self-assessments and expert ratings to benchmark the system's outputs. The LLM-based rubric evaluation showed strong agreement with expert judgments and consistently produced rubric-aligned, process-oriented feedback. These results demonstrate both the validity and scalability of incorporating LLM-driven rubric assessment into online mathematics and STEM education platforms.", "link": "https://arxiv.org/abs/2510.06253", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.497107Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.AI|oai:arXiv.org:2510.06260v1", "feed": "https://arxiv.org/rss/cs.AI", "feed_name": "arXiv cs.AI", "title": "Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis", "summary": "arXiv:2510.06260v1 Announce Type: cross Abstract: Cutaneous malignancies demand early detection for favorable outcomes, yet current diagnostics suffer from inter-observer variability and access disparities. While AI shows promise, existing dermatological systems are limited by homogeneous architectures, dataset biases across skin tones, and fragmented approaches that treat natural language processing as separate post-hoc explanations rather than integral to clinical decision-making. We introduce a unified framework that fundamentally reimagines AI integration for dermatological diagnostics through two synergistic innovations. First, a purposefully heterogeneous ensemble of architecturally diverse convolutional neural networks provides complementary diagnostic perspectives, with an intrinsic uncertainty mechanism flagging discordant cases for specialist review -- mimicking clinical best practices. Second, we embed large language model capabilities directly into the diagnostic workflow, transforming classification outputs into clinically meaningful assessments that simultaneously fulfill medical documentation requirements and deliver patient-centered education. This seamless integration generates structured reports featuring precise lesion characterization, accessible diagnostic reasoning, and actionable monitoring guidance -- empowering patients to recognize early warning signs between visits. By addressing both diagnostic reliability and communication barriers within a single cohesive system, our approach bridges the critical translational gap that has prevented previous AI implementations from achieving clinical impact. The framework represents a significant advancement toward deployable dermatological AI that enhances diagnostic precision while actively supporting the continuum of care from initial detection through patient education, ultimately improving early intervention rates for skin lesions.", "link": "https://arxiv.org/abs/2510.06260", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "inteligencia artificial", "ai", "aprendizaje", "machine learning"], "ingested_at": "2025-10-09T05:07:28.497156Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06267v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases", "summary": "arXiv:2510.06267v1 Announce Type: new Abstract: We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion framework that generates realistic yet privacy-preserving synthetic electronic-health-record (EHR) trajectories for ultra-rare diseases. RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph comprising approximately 8 M typed edges. Meta-path scores extracted from this 8-million-edge KG modulate the per-token noise schedule in the forward stochastic differential equation, steering generation toward biologically plausible lab-medication-adverse-event co-occurrences while retaining score-based diffusion model stability. The reverse denoiser then produces timestamped sequences of lab-code, medication-code, and adverse-event-flag triples that contain no protected health information. On simulated ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean Discrepancy by 40 percent relative to an unguided diffusion baseline and by greater than 60 percent versus GAN counterparts, without sacrificing downstream predictive utility. A black-box membership-inference evaluation using the DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55 safe-release threshold and substantially better than the approximately 0.61 plus or minus 0.03 observed for non-KG baselines, demonstrating strong resistance to re-identification. These results suggest that integrating biomedical knowledge graphs directly into diffusion noise schedules can simultaneously enhance fidelity and privacy, enabling safer data sharing for rare-disease research.", "link": "https://arxiv.org/abs/2510.06267", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029147Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06270v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "MCCE: A Framework for Multi-LLM Collaborative Co-Evolution", "summary": "arXiv:2510.06270v1 Announce Type: new Abstract: Multi-objective discrete optimization problems, such as molecular design, pose significant challenges due to their vast and unstructured combinatorial spaces. Traditional evolutionary algorithms often get trapped in local optima, while expert knowledge can provide crucial guidance for accelerating convergence. Large language models (LLMs) offer powerful priors and reasoning ability, making them natural optimizers when expert knowledge matters. However, closed-source LLMs, though strong in exploration, cannot update their parameters and thus cannot internalize experience. Conversely, smaller open models can be continually fine-tuned but lack broad knowledge and reasoning strength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid framework that unites a frozen closed-source LLM with a lightweight trainable model. The system maintains a trajectory memory of past search processes; the small model is progressively refined via reinforcement learning, with the two models jointly supporting and complementing each other in global exploration. Unlike model distillation, this process enhances the capabilities of both models through mutual inspiration. Experiments on multi-objective drug design benchmarks show that MCCE achieves state-of-the-art Pareto front quality and consistently outperforms baselines. These results highlight a new paradigm for enabling continual evolution in hybrid LLM systems, combining knowledge-driven exploration with experience-driven learning.", "link": "https://arxiv.org/abs/2510.06270", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029203Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06278v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets", "summary": "arXiv:2510.06278v1 Announce Type: new Abstract: Recent advancements in neural networks, supported by foundational theoretical insights, emphasize the superior representational power of complex numbers. However, their adoption in randomized neural networks (RNNs) has been limited due to the lack of effective methods for transforming real-valued tabular datasets into complex-valued representations. To address this limitation, we propose two methods for generating complex-valued representations from real-valued datasets: a natural transformation and an autoencoder-driven method. Building on these mechanisms, we propose RVFL-X, a complex-valued extension of the random vector functional link (RVFL) network. RVFL-X integrates complex transformations into real-valued datasets while maintaining the simplicity and efficiency of the original RVFL architecture. By leveraging complex components such as input, weights, and activation functions, RVFL-X processes complex representations and produces real-valued outputs. Comprehensive evaluations on 80 real-valued UCI datasets demonstrate that RVFL-X consistently outperforms both the original RVFL and state-of-the-art (SOTA) RNN variants, showcasing its robustness and effectiveness across diverse application domains.", "link": "https://arxiv.org/abs/2510.06278", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029254Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06284v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "On knot detection via picture recognition", "summary": "arXiv:2510.06284v1 Announce Type: new Abstract: Our goal is to one day take a photo of a knot and have a phone automatically recognize it. In this expository work, we explain a strategy to approximate this goal, using a mixture of modern machine learning methods (in particular convolutional neural networks and transformers for image recognition) and traditional algorithms (to compute quantum invariants like the Jones polynomial). We present simple baselines that predict crossing number directly from images, showing that even lightweight CNN and transformer architectures can recover meaningful structural information. The longer-term aim is to combine these perception modules with symbolic reconstruction into planar diagram (PD) codes, enabling downstream invariant computation for robust knot classification. This two-stage approach highlights the complementarity between machine learning, which handles noisy visual data, and invariants, which enforce rigorous topological distinctions.", "link": "https://arxiv.org/abs/2510.06284", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029289Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06291v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation", "summary": "arXiv:2510.06291v1 Announce Type: new Abstract: The widespread use of GPS devices has driven advances in spatiotemporal data mining, enabling machine learning models to simulate human decision making and generate realistic trajectories, addressing both data collection costs and privacy concerns. Recent studies have shown the promise of diffusion models for high-quality trajectory generation. However, most existing methods rely on convolution based architectures (e.g. UNet) to predict noise during the diffusion process, which often results in notable deviations and the loss of fine-grained street-level details due to limited model capacity. In this paper, we propose Trajectory Transformer, a novel model that employs a transformer backbone for both conditional information embedding and noise prediction. We explore two GPS coordinate embedding strategies, location embedding and longitude-latitude embedding, and analyze model performance at different scales. Experiments on two real-world datasets demonstrate that Trajectory Transformer significantly enhances generation quality and effectively alleviates the deviation issues observed in prior approaches.", "link": "https://arxiv.org/abs/2510.06291", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029328Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06293v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression", "summary": "arXiv:2510.06293v1 Announce Type: new Abstract: Predicting precipitation maps is a highly complex spatiotemporal modeling task, critical for mitigating the impacts of extreme weather events. Short-term precipitation forecasting, or nowcasting, requires models that are not only accurate but also computationally efficient for real-time applications. Current methods, such as token-based autoregressive models, often suffer from flawed inductive biases and slow inference, while diffusion models can be computationally intensive. To address these limitations, we introduce BlockGPT, a generative autoregressive transformer using batched tokenization (Block) method that predicts full two-dimensional fields (frames) at each time step. Conceived as a model-agnostic paradigm for video prediction, BlockGPT factorizes space-time by using self-attention within each frame and causal attention across frames; in this work, we instantiate it for precipitation nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI (Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet) models. The results show that BlockGPT achieves superior accuracy, event localization as measured by categorical metrics, and inference speeds up to 31x faster than comparable baselines.", "link": "https://arxiv.org/abs/2510.06293", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029369Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06303v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation", "summary": "arXiv:2510.06303v1 Announce Type: new Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies the training efficiency of autoregressive models with the parallel inference capability of diffusion. Instead of costly end-to-end diffusion training, SDAR performs a lightweight paradigm conversion that transforms a well-trained autoregressive (AR) model into a blockwise diffusion model through brief, data-efficient adaptation. During inference, SDAR generates sequences autoregressively across blocks for global coherence while decoding all tokens within each block in parallel via a discrete diffusion process. Extensive experiments show that AR models remain substantially more compute-efficient than masked diffusion models, providing a strong foundation for adaptation. Building on this insight, SDAR achieves efficient AR-to-diffusion conversion with minimal cost, preserving AR-level performance while enabling parallel generation. Scaling studies across dense and Mixture-of-Experts architectures confirm that SDAR scales without compromise: larger models exhibit stronger robustness to block size and decoding thresholds, yielding greater speedups without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning and domain adaptability. Our 30B MoE model surpasses its AR counterpart on challenging scientific reasoning benchmarks such as GPQA and ChemBench, and gains further improvements under test-time scaling methods like majority voting and pass@k. Together, these results establish SDAR as a practical paradigm that combines the strengths of autoregression and diffusion for scalable, high-throughput reasoning.", "link": "https://arxiv.org/abs/2510.06303", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029425Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06349v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks", "summary": "arXiv:2510.06349v1 Announce Type: new Abstract: Foundation models have rapidly advanced AI, raising the question of whether their decisions will ultimately surpass human strategies in real-world domains. The exponential, and possibly super-exponential, pace of AI development makes such analysis elusive. Nevertheless, many application areas that matter for daily life and society show only modest gains so far; a prominent case is diagnosing and treating dynamically evolving disease in intensive care. The common challenge is adapting complex systems to dynamic environments. Effective strategies must optimize outcomes in systems composed of strongly interacting functions while avoiding shared side effects; this requires reliable, self-adaptive modeling. These tasks align with building digital twins of highly complex systems whose mechanisms are not fully or quantitatively understood. It is therefore essential to develop methods for self-adapting AI models with minimal data and limited mechanistic knowledge. As this challenge extends beyond medicine, AI should demonstrate clear superiority in these settings before assuming broader decision-making roles. We identify the curse of dimensionality as a fundamental barrier to efficient self-adaptation and argue that monolithic foundation models face conceptual limits in overcoming it. As an alternative, we propose a decentralized architecture of interacting small agent networks (SANs). We focus on agents representing the specialized substructure of the system, where each agent covers only a subset of the full system functions. Drawing on mathematical results on the learning behavior of SANs and evidence from existing applications, we argue that swarm-learning in diverse swarms can enable self-adaptive SANs to deliver superior decision-making in dynamic environments compared with monolithic foundation models, though at the cost of reduced reproducibility in detail.", "link": "https://arxiv.org/abs/2510.06349", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029483Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06355v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "PIKAN: Physics-Inspired Kolmogorov-Arnold Networks for Explainable UAV Channel Modelling", "summary": "arXiv:2510.06355v1 Announce Type: new Abstract: Unmanned aerial vehicle (UAV) communications demand accurate yet interpretable air-to-ground (A2G) channel models that can adapt to nonstationary propagation environments. While deterministic models offer interpretability and deep learning (DL) models provide accuracy, both approaches suffer from either rigidity or a lack of explainability. To bridge this gap, we propose the Physics-Inspired Kolmogorov-Arnold Network (PIKAN) that embeds physical principles (e.g., free-space path loss, two-ray reflections) into the learning process. Unlike physics-informed neural networks (PINNs), PIKAN is more flexible for applying physical information because it introduces them as flexible inductive biases. Thus, it enables a more flexible training process. Experiments on UAV A2G measurement data show that PIKAN achieves comparable accuracy to DL models while providing symbolic and explainable expressions aligned with propagation laws. Remarkably, PIKAN achieves this performance with only 232 parameters, making it up to 37 times lighter than multilayer perceptron (MLP) baselines with thousands of parameters, without sacrificing correlation with measurements and also providing symbolic expressions. These results highlight PIKAN as an efficient, interpretable, and scalable solution for UAV channel modelling in beyond-5G and 6G networks.", "link": "https://arxiv.org/abs/2510.06355", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029526Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06367v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics", "summary": "arXiv:2510.06367v1 Announce Type: new Abstract: Neural ODEs are a widely used, powerful machine learning technique in particular for physics. However, not every solution is physical in that it is an Euler-Lagrange equation. We present Helmholtz metrics to quantify this resemblance for a given ODE and demonstrate their capabilities on several fundamental systems with noise. We combine them with a second order neural ODE to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations in a direct fashion and with zero additional inference cost. We demonstrate that, using only positional data, they can distinguish Lagrangian and non-Lagrangian systems and improve the neural ODE solutions.", "link": "https://arxiv.org/abs/2510.06367", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029556Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06377v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data", "summary": "arXiv:2510.06377v1 Announce Type: new Abstract: Pretrained transformers readily adapt to new sequence modeling tasks via zero-shot prompting, but relational domains still lack architectures that transfer across datasets and tasks. The core challenge is the diversity of relational data, with varying heterogeneous schemas, graph structures and functional dependencies. In this paper, we present the Relational Transformer (RT) architecture, which can be pretrained on diverse relational databases and directly applied to unseen datasets and tasks without task- or dataset-specific fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with table/column metadata, (ii) is pretrained via masked token prediction, and (iii) utilizes a novel \\textit{Relational Attention} mechanism over columns, rows, and primary-foreign key links. Pretrained on RelBench datasets spanning tasks such as churn and sales forecasting, RT attains strong zero-shot performance, averaging 94% of fully supervised AUROC on binary classification tasks with a single forward pass of a 22M parameter model, as opposed to 84% for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample efficiency. Our experiments show that RT's zero-shot transfer harnesses task-table context, relational attention patterns and schema semantics. Overall, RT provides a practical path toward foundation models for relational data.", "link": "https://arxiv.org/abs/2510.06377", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029600Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06381v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Monte Carlo Permutation Search", "summary": "arXiv:2510.06381v1 Announce Type: new Abstract: We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte Carlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS is relevant when deep reinforcement learning is not an option, or when the computing power available before play is not substantial, such as in General Game Playing, for example. The principle of MCPS is to include in the exploration term of a node the statistics on all the playouts that contain all the moves on the path from the root to the node. We extensively test MCPS on a variety of games: board games, wargame, investment game, video game and multi-player games. MCPS has better results than GRAVE in all the two-player games. It has equivalent results for multi-player games because these games are inherently balanced even when players have different strengths. We also show that using abstract codes for moves instead of exact codes can be beneficial to both MCPS and GRAVE, as they improve the permutation statistics and the AMAF statistics. We also provide a mathematical derivation of the formulas used for weighting the three sources of statistics. These formulas are an improvement on the GRAVE formula since they no longer use the bias hyperparameter of GRAVE. Moreover, MCPS is not sensitive to the ref hyperparameter.", "link": "https://arxiv.org/abs/2510.06381", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029663Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06388v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Making and Evaluating Calibrated Forecasts", "summary": "arXiv:2510.06388v1 Announce Type: new Abstract: Calibrated predictions can be reliably interpreted as probabilities. An important step towards achieving better calibration is to design an appropriate calibration measure to meaningfully assess the miscalibration level of a predictor. A recent line of work initiated by Haghtalab et al. [2024] studies the design of truthful calibration measures: a truthful measure is minimized when a predictor outputs the true probabilities, whereas a non-truthful measure incentivizes the predictor to lie so as to appear more calibrated. All previous calibration measures were non-truthful until Hartline et al. [2025] introduced the first perfectly truthful calibration measures for binary prediction tasks in the batch setting. We introduce a perfectly truthful calibration measure for multi-class prediction tasks, generalizing the work of Hartline et al. [2025] beyond binary prediction. We study common methods of extending calibration measures from binary to multi-class prediction and identify ones that do or do not preserve truthfulness. In addition to truthfulness, we mathematically prove and empirically verify that our calibration measure exhibits superior robustness: it robustly preserves the ordering between dominant and dominated predictors, regardless of the choice of hyperparameters (bin sizes). This result addresses the non-robustness issue of binned ECE, which has been observed repeatedly in prior work.", "link": "https://arxiv.org/abs/2510.06388", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029709Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06401v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "The Effect of Label Noise on the Information Content of Neural Representations", "summary": "arXiv:2510.06401v1 Announce Type: new Abstract: In supervised classification tasks, models are trained to predict a label for each data point. In real-world datasets, these labels are often noisy due to annotation errors. While the impact of label noise on the performance of deep learning models has been widely studied, its effects on the networks' hidden representations remain poorly understood. We address this gap by systematically comparing hidden representations using the Information Imbalance, a computationally efficient proxy of conditional mutual information. Through this analysis, we observe that the information content of the hidden representations follows a double descent as a function of the number of network parameters, akin to the behavior of the test error. We further demonstrate that in the underparameterized regime, representations learned with noisy labels are more informative than those learned with clean labels, while in the overparameterized regime, these representations are equally informative. Our results indicate that the representations of overparameterized networks are robust to label noise. We also found that the information imbalance between the penultimate and pre-softmax layers decreases with cross-entropy loss in the overparameterized regime. This offers a new perspective on understanding generalization in classification tasks. Extending our analysis to representations learned from random labels, we show that these perform worse than random features. This indicates that training on random labels drives networks much beyond lazy learning, as weights adapt to encode labels information.", "link": "https://arxiv.org/abs/2510.06401", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029806Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06419v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting", "summary": "arXiv:2510.06419v1 Announce Type: new Abstract: Is bigger always better for time series foundation models? With the question in mind, we explore an alternative to training a single, large monolithic model: building a portfolio of smaller, pretrained forecasting models. By applying ensembling or model selection over these portfolios, we achieve competitive performance on large-scale benchmarks using much fewer parameters. We explore strategies for designing such portfolios and find that collections of specialist models consistently outperform portfolios of independently trained generalists. Remarkably, we demonstrate that post-training a base model is a compute-effective approach for creating sufficiently diverse specialists, and provide evidences that ensembling and model selection are more compute-efficient than test-time fine-tuning.", "link": "https://arxiv.org/abs/2510.06419", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029844Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06434v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization", "summary": "arXiv:2510.06434v1 Announce Type: new Abstract: Learning from temporally-correlated data is a core facet of modern machine learning. Yet our understanding of sequential learning remains incomplete, particularly in the multi-trajectory setting where data consists of many independent realizations of a time-indexed stochastic process. This important regime both reflects modern training pipelines such as for large foundation models, and offers the potential for learning without the typical mixing assumptions made in the single-trajectory case. However, instance-optimal bounds are known only for least-squares regression with dependent covariates; for more general models or loss functions, the only broadly applicable guarantees result from a reduction to either i.i.d. learning, with effective sample size scaling only in the number of trajectories, or an existing single-trajectory result when each individual trajectory mixes, with effective sample size scaling as the full data budget deflated by the mixing-time. In this work, we significantly broaden the scope of instance-optimal rates in multi-trajectory settings via the Hellinger localization framework, a general approach for maximum likelihood estimation. Our method proceeds by first controlling the squared Hellinger distance at the path-measure level via a reduction to i.i.d. learning, followed by localization as a quadratic form in parameter space weighted by the trajectory Fisher information. This yields instance-optimal bounds that scale with the full data budget under a broad set of conditions. We instantiate our framework across four diverse case studies: a simple mixture of Markov chains, dependent linear regression under non-Gaussian noise, generalized linear models with non-monotonic activations, and linear-attention sequence models. In all cases, our bounds nearly match the instance-optimal rates from asymptotic normality, substantially improving over standard reductions.", "link": "https://arxiv.org/abs/2510.06434", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029911Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06439v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models", "summary": "arXiv:2510.06439v1 Announce Type: new Abstract: Hyperparameter tuning is a challenging problem especially when the system itself involves uncertainty. Due to noisy function evaluations, optimization under uncertainty can be computationally expensive. In this paper, we present a novel Bayesian optimization framework tailored for hyperparameter tuning under uncertainty, with a focus on optimizing a scale- or precision-type parameter in stochastic models. The proposed method employs a statistical surrogate for the underlying random variable, enabling analytical evaluation of the expectation operator. Moreover, we derive a closed-form expression for the optimizer of the random acquisition function, which significantly reduces computational cost per iteration. Compared with a conventional one-dimensional Monte Carlo-based optimization scheme, the proposed approach requires 40 times fewer data points, resulting in up to a 40-fold reduction in computational cost. We demonstrate the effectiveness of the proposed method through two numerical examples in computational engineering.", "link": "https://arxiv.org/abs/2510.06439", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.029947Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06444v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks", "summary": "arXiv:2510.06444v1 Announce Type: new Abstract: In decentralized learning networks, predictions from many participants are combined to generate a network inference. While many studies have demonstrated performance benefits of combining multiple model predictions, existing strategies using linear pooling methods (ranging from simple averaging to dynamic weight updates) face a key limitation. Dynamic prediction combinations that rely on historical performance to update weights are necessarily reactive. Due to the need to average over a reasonable number of epochs (with moving averages or exponential weighting), they tend to be slow to adjust to changing circumstances (phase or regime changes). In this work, we develop a model that uses machine learning to forecast the performance of predictions by models at each epoch in a time series. This enables `context-awareness' by assigning higher weight to models that are likely to be more accurate at a given time. We show that adding a performance forecasting worker in a decentralized learning network, following a design similar to the Allora network, can improve the accuracy of network inferences. Specifically, we find forecasting models that predict regret (performance relative to the network inference) or regret z-score (performance relative to other workers) show greater improvement than models predicting losses, which often do not outperform the naive network inference (historically weighted average of all inferences). Through a series of optimization tests, we show that the performance of the forecasting model can be sensitive to choices in the feature set and number of training epochs. These properties may depend on the exact problem and should be tailored to each domain. Although initially designed for a decentralized learning network, using performance forecasting for prediction combination may be useful in any situation where predictive rather than reactive model weighting is needed.", "link": "https://arxiv.org/abs/2510.06444", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030006Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06448v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation", "summary": "arXiv:2510.06448v1 Announce Type: new Abstract: Transferability estimation metrics are used to find a high-performing pre-trained model for a given target task without fine-tuning models and without access to the source dataset. Despite the growing interest in developing such metrics, the benchmarks used to measure their progress have gone largely unexamined. In this work, we empirically show the shortcomings of widely used benchmark setups to evaluate transferability estimation metrics. We argue that the benchmarks on which these metrics are evaluated are fundamentally flawed. We empirically demonstrate that their unrealistic model spaces and static performance hierarchies artificially inflate the perceived performance of existing metrics, to the point where simple, dataset-agnostic heuristics can outperform sophisticated methods. Our analysis reveals a critical disconnect between current evaluation protocols and the complexities of real-world model selection. To address this, we provide concrete recommendations for constructing more robust and realistic benchmarks to guide future research in a more meaningful direction.", "link": "https://arxiv.org/abs/2510.06448", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030102Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06477v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin", "summary": "arXiv:2510.06477v1 Announce Type: new Abstract: Attention sinks and compression valleys have attracted significant attention as two puzzling phenomena in large language models, but have been studied in isolation. In this work, we present a surprising connection between attention sinks and compression valleys, tracing both to the formation of massive activations in the residual stream. We prove theoretically that massive activations necessarily produce representational compression and establish bounds on the resulting entropy reduction. Through experiments across several models (410M-120B parameters), we confirm that when the beginning-of-sequence token develops extreme activation norms in the middle layers, both compression valleys and attention sinks emerge simultaneously. Targeted ablation studies validate our theoretical predictions. This unified view motivates us to propose the Mix-Compress-Refine theory of information flow, as an attempt to explain how LLMs organize their computation in depth by controlling attention and representational compression via massive activations. Specifically, we posit that Transformer-based LLMs process tokens in three distinct phases: (1) broad mixing in the early layers, (2) compressed computation with limited mixing in the middle layers, and (3) selective refinement in the late layers. Our framework helps explain why embedding tasks perform best at intermediate layers, whereas generation tasks benefit from full-depth processing, clarifying differences in task-dependent representations.", "link": "https://arxiv.org/abs/2510.06477", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030200Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06478v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Valid Stopping for LLM Generation via Empirical Dynamic Formal Lift", "summary": "arXiv:2510.06478v1 Announce Type: new Abstract: We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), applying anytime-valid sequential testing to language model generation stopping. Our approach tracks information lift -- the log-likelihood ratio between full models and deliberately weakened \"skeleton\" baselines -- using self-normalized empirical-Bernstein e-processes that provide formal delta-level error control regardless of stopping time. We handle unknown centering through online mean estimation, combine multiple parameters via mixture e-processes, and support adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL reduces generation by 22-28% vs. sequential baselines while maintaining delta-level control with 12% computational overhead. We introduce automated skeletons (distilled submodels, randomized logits) and show robustness across skeleton families. Composing EDFL with a lightweight correctness gate (sentence boundaries + verifier) improves end-task correctness while preserving anytime-valid guarantees by only delaying stopping. Our certificates control information sufficiency, not factual correctness -- 10.9% of stopped sequences remain incorrect even with the gate (13.2-22.7% without it). EDFL serves as a first-stage filter reducing verification burden by 83%, not as a standalone solution for safety-critical domains.", "link": "https://arxiv.org/abs/2510.06478", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030249Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06502v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "GUIDE: Guided Initialization and Distillation of Embeddings", "summary": "arXiv:2510.06502v1 Announce Type: new Abstract: Algorithmic efficiency techniques such as distillation (\\cite{hinton2015distillation}) are useful in improving model quality without increasing serving costs, provided a larger teacher model is available for a smaller student model to learn from during training. Standard distillation methods are limited to only forcing the student to match the teacher's outputs. Given the costs associated with training a large model, we believe we should be extracting more useful information from a teacher model than by just making the student match the teacher's outputs. In this paper, we introduce \\guide (Guided Initialization and Distillation of Embeddings). \\guide can be considered a distillation technique that forces the student to match the teacher in the parameter space. Using \\guide we show 25-26\\% reduction in the teacher-student quality gap when using large student models (400M - 1B parameters) trained on $\\approx$ 20B tokens. We also present a thorough analysis demonstrating that \\guide can be combined with knowledge distillation with near additive improvements. Furthermore, we show that applying \\guide alone leads to substantially better model quality than applying knowledge distillation by itself. Most importantly, \\guide introduces no training or inference overhead and hence any model quality gains from our method are virtually free.", "link": "https://arxiv.org/abs/2510.06502", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030297Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06503v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting", "summary": "arXiv:2510.06503v1 Announce Type: new Abstract: Accurate time-series predictions in machine learning are heavily influenced by the selection of appropriate input time length and sampling rate. This paper introduces ATLO-ML, an adaptive time-length optimization system that automatically determines the optimal input time length and sampling rate based on user-defined output time length. The system provides a flexible approach to time-series data pre-processing, dynamically adjusting these parameters to enhance predictive performance. ATLO-ML is validated using air quality datasets, including both GAMS-dataset and proprietary data collected from a data center, both in time series format. Results demonstrate that utilizing the optimized time length and sampling rate significantly improves the accuracy of machine learning models compared to fixed time lengths. ATLO-ML shows potential for generalization across various time-sensitive applications, offering a robust solution for optimizing temporal input parameters in machine learning workflows.", "link": "https://arxiv.org/abs/2510.06503", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030335Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06505v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "A Median Perspective on Unlabeled Data for Out-of-Distribution Detection", "summary": "arXiv:2510.06505v1 Announce Type: new Abstract: Out-of-distribution (OOD) detection plays a crucial role in ensuring the robustness and reliability of machine learning systems deployed in real-world applications. Recent approaches have explored the use of unlabeled data, showing potential for enhancing OOD detection capabilities. However, effectively utilizing unlabeled in-the-wild data remains challenging due to the mixed nature of both in-distribution (InD) and OOD samples. The lack of a distinct set of OOD samples complicates the task of training an optimal OOD classifier. In this work, we introduce Medix, a novel framework designed to identify potential outliers from unlabeled data using the median operation. We use the median because it provides a stable estimate of the central tendency, as an OOD detection mechanism, due to its robustness against noise and outliers. Using these identified outliers, along with labeled InD data, we train a robust OOD classifier. From a theoretical perspective, we derive error bounds that demonstrate Medix achieves a low error rate. Empirical results further substantiate our claims, as Medix outperforms existing methods across the board in open-world settings, confirming the validity of our theoretical insights.", "link": "https://arxiv.org/abs/2510.06505", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030377Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06525v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security", "summary": "arXiv:2510.06525v1 Announce Type: new Abstract: Generative AI leaderboards are central to evaluating model capabilities, but remain vulnerable to manipulation. Among key adversarial objectives is rank manipulation, where an attacker must first deanonymize the models behind displayed outputs -- a threat previously demonstrated and explored for large language models (LLMs). We show that this problem can be even more severe for text-to-image leaderboards, where deanonymization is markedly easier. Using over 150,000 generated images from 280 prompts and 19 diverse models spanning multiple organizations, architectures, and sizes, we demonstrate that simple real-time classification in CLIP embedding space identifies the generating model with high accuracy, even without prompt control or historical data. We further introduce a prompt-level separability metric and identify prompts that enable near-perfect deanonymization. Our results indicate that rank manipulation in text-to-image leaderboards is easier than previously recognized, underscoring the need for stronger defenses.", "link": "https://arxiv.org/abs/2510.06525", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030430Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06527v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture", "summary": "arXiv:2510.06527v1 Announce Type: new Abstract: We establish that randomly initialized neural networks, with large width and a natural choice of hyperparameters, have nearly independent outputs exactly when their activation function is nonlinear with zero mean under the Gaussian measure: $\\mathbb{E}_{z \\sim \\mathcal{N}(0,1)}[\\sigma(z)]=0$. For example, this includes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or GeLU by themselves. Because of their nearly independent outputs, we propose neural networks with zero-mean activation functions as a promising candidate for the Alignment Research Center's computational no-coincidence conjecture -- a conjecture that aims to measure the limits of AI interpretability.", "link": "https://arxiv.org/abs/2510.06527", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030463Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06540v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Scalable Policy-Based RL Algorithms for POMDPs", "summary": "arXiv:2510.06540v1 Announce Type: new Abstract: The continuous nature of belief states in POMDPs presents significant computational challenges in learning the optimal policy. In this paper, we consider an approach that solves a Partially Observable Reinforcement Learning (PORL) problem by approximating the corresponding POMDP model into a finite-state Markov Decision Process (MDP) (called Superstate MDP). We first derive theoretical guarantees that improve upon prior work that relate the optimal value function of the transformed Superstate MDP to the optimal value function of the original POMDP. Next, we propose a policy-based learning approach with linear function approximation to learn the optimal policy for the Superstate MDP. Consequently, our approach shows that a POMDP can be approximately solved using TD-learning followed by Policy Optimization by treating it as an MDP, where the MDP state corresponds to a finite history. We show that the approximation error decreases exponentially with the length of this history. To the best of our knowledge, our finite-time bounds are the first to explicitly quantify the error introduced when applying standard TD learning to a setting where the true dynamics are not Markovian.", "link": "https://arxiv.org/abs/2510.06540", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030507Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06545v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Incoherence in goal-conditioned autoregressive models", "summary": "arXiv:2510.06545v1 Announce Type: new Abstract: We investigate mathematically the notion of incoherence: a structural issue with reinforcement learning policies derived by naive goal-conditioning of autoregressive models. We focus on the process of re-training models on their own actions, that is, fine-tuning offline-learned policies with online RL. We prove that it decreases incoherence and leads to an improvement in return, and we aim to characterize the resulting trajectory of policies. By re-framing standard notions of control-as-inference and soft Q learning, we establish a three-way correspondence with two other ways of understanding the iterative re-training process: as folding the posterior into the reward and, in the deterministic case, as decreasing the temperature parameter; the correspondence has computational content via the training-inference trade-off. Through soft-conditioning generative models, we discuss the link between incoherence and the effective horizon.", "link": "https://arxiv.org/abs/2510.06545", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030551Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06557v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "The Markovian Thinker", "summary": "arXiv:2510.06557v1 Announce Type: new Abstract: Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL \"thinking environment\", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.", "link": "https://arxiv.org/abs/2510.06557", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030606Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06567v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "The Framework That Survives Bad Models: Human-AI Collaboration For Clinical Trials", "summary": "arXiv:2510.06567v1 Announce Type: new Abstract: Artificial intelligence (AI) holds great promise for supporting clinical trials, from patient recruitment and endpoint assessment to treatment response prediction. However, deploying AI without safeguards poses significant risks, particularly when evaluating patient endpoints that directly impact trial conclusions. We compared two AI frameworks against human-only assessment for medical image-based disease evaluation, measuring cost, accuracy, robustness, and generalization ability. To stress-test these frameworks, we injected bad models, ranging from random guesses to naive predictions, to ensure that observed treatment effects remain valid even under severe model degradation. We evaluated the frameworks using two randomized controlled trials with endpoints derived from spinal X-ray images. Our findings indicate that using AI as a supporting reader (AI-SR) is the most suitable approach for clinical trials, as it meets all criteria across various model types, even with bad models. This method consistently provides reliable disease estimation, preserves clinical trial treatment effect estimates and conclusions, and retains these advantages when applied to different populations.", "link": "https://arxiv.org/abs/2510.06567", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030649Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06623v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "DPA-Net: A Dual-Path Attention Neural Network for Inferring Glycemic Control Metrics from Self-Monitored Blood Glucose Data", "summary": "arXiv:2510.06623v1 Announce Type: new Abstract: Continuous glucose monitoring (CGM) provides dense and dynamic glucose profiles that enable reliable estimation of Ambulatory Glucose Profile (AGP) metrics, such as Time in Range (TIR), Time Below Range (TBR), and Time Above Range (TAR). However, the high cost and limited accessibility of CGM restrict its widespread adoption, particularly in low- and middle-income regions. In contrast, self-monitoring of blood glucose (SMBG) is inexpensive and widely available but yields sparse and irregular data that are challenging to translate into clinically meaningful glycemic metrics. In this work, we propose a Dual-Path Attention Neural Network (DPA-Net) to estimate AGP metrics directly from SMBG data. DPA-Net integrates two complementary paths: (1) a spatial-channel attention path that reconstructs a CGM-like trajectory from sparse SMBG observations, and (2) a multi-scale ResNet path that directly predicts AGP metrics. An alignment mechanism between the two paths is introduced to reduce bias and mitigate overfitting. In addition, we develop an active point selector to identify realistic and informative SMBG sampling points that reflect patient behavioral patterns. Experimental results on a large, real-world dataset demonstrate that DPA-Net achieves robust accuracy with low errors while reducing systematic bias. To the best of our knowledge, this is the first supervised machine learning framework for estimating AGP metrics from SMBG data, offering a practical and clinically relevant decision-support tool in settings where CGM is not accessible.", "link": "https://arxiv.org/abs/2510.06623", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030701Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06627v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "POME: Post Optimization Model Edit via Muon-style Projection", "summary": "arXiv:2510.06627v1 Announce Type: new Abstract: We introduce Post-Optimization Model Edit (POME), a new algorithm that enhances the performance of fine-tuned large language models using only their pretrained and fine-tuned checkpoints, without requiring extra data or further optimization. The core idea is to apply a muon-style projection to $\\Delta W$, the difference between the fine-tuned and pretrained weights. This projection uses truncated singular value decomposition (SVD) to equalize the influence of dominant update directions and prune small singular values, which often represent noise. As a simple post-processing step, POME is completely decoupled from the training pipeline. It requires zero modifications and imposes no overhead, making it universally compatible with any optimizer or distributed framework. POME delivers consistent gains, boosting average performance by +2.5\\% on GSM8K and +1.0\\% on code generation. Its broad applicability -- from 7B foundation models to 72B RLHF-instructed models -- establishes it as a practical, zero-cost enhancement for any fine-tuning pipeline. Code is available at https://github.com/NUS-HPC-AI-Lab/POME.", "link": "https://arxiv.org/abs/2510.06627", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030740Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06631v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "AI-Driven Forecasting and Monitoring of Urban Water System", "summary": "arXiv:2510.06631v1 Announce Type: new Abstract: Underground water and wastewater pipelines are vital for city operations but plagued by anomalies like leaks and infiltrations, causing substantial water loss, environmental damage, and high repair costs. Conventional manual inspections lack efficiency, while dense sensor deployments are prohibitively expensive. In recent years, artificial intelligence has advanced rapidly and is increasingly applied to urban infrastructure. In this research, we propose an integrated AI and remote-sensor framework to address the challenge of leak detection in underground water pipelines, through deploying a sparse set of remote sensors to capture real-time flow and depth data, paired with HydroNet - a dedicated model utilizing pipeline attributes (e.g., material, diameter, slope) in a directed graph for higher-precision modeling. Evaluations on a real-world campus wastewater network dataset demonstrate that our system collects effective spatio-temporal hydraulic data, enabling HydroNet to outperform advanced baselines. This integration of edge-aware message passing with hydraulic simulations enables accurate network-wide predictions from limited sensor deployments. We envision that this approach can be effectively extended to a wide range of underground water pipeline networks.", "link": "https://arxiv.org/abs/2510.06631", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030783Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06637v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Control-Augmented Autoregressive Diffusion for Data Assimilation", "summary": "arXiv:2510.06637v1 Announce Type: new Abstract: Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments pretrained ARDMs with a lightweight controller network, trained offline by previewing future ARDM rollouts and learning stepwise controls that anticipate upcoming observations under a terminal cost objective. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), a setting where existing methods are often computationally prohibitive and prone to forecast drift under sparse observations. Our approach reduces DA inference to a single forward rollout with on-the-fly corrections, avoiding expensive adjoint computations and/or optimizations during inference. We demonstrate that our method consistently outperforms four state-of-the-art baselines in stability, accuracy, and physical fidelity across two canonical PDEs and six observation regimes. We will release code and checkpoints publicly.", "link": "https://arxiv.org/abs/2510.06637", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.030989Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06646v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators", "summary": "arXiv:2510.06646v1 Announce Type: new Abstract: A core challenge in scientific machine learning, and scientific computing more generally, is modeling continuous phenomena which (in practice) are represented discretely. Machine-learned operators (MLOs) have been introduced as a means to achieve this modeling goal, as this class of architecture can perform inference at arbitrary resolution. In this work, we evaluate whether this architectural innovation is sufficient to perform \"zero-shot super-resolution,\" namely to enable a model to serve inference on higher-resolution data than that on which it was originally trained. We comprehensively evaluate both zero-shot sub-resolution and super-resolution (i.e., multi-resolution) inference in MLOs. We decouple multi-resolution inference into two key behaviors: 1) extrapolation to varying frequency information; and 2) interpolating across varying resolutions. We empirically demonstrate that MLOs fail to do both of these tasks in a zero-shot manner. Consequently, we find MLOs are not able to perform accurate inference at resolutions different from those on which they were trained, and instead they are brittle and susceptible to aliasing. To address these failure modes, we propose a simple, computationally-efficient, and data-driven multi-resolution training protocol that overcomes aliasing and that provides robust multi-resolution generalization.", "link": "https://arxiv.org/abs/2510.06646", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.031032Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06660v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Rethinking Nonlinearity: Trainable Gaussian Mixture Modules for Modern Neural Architectures", "summary": "arXiv:2510.06660v1 Announce Type: new Abstract: Neural networks in general, from MLPs and CNNs to attention-based Transformers, are constructed from layers of linear combinations followed by nonlinear operations such as ReLU, Sigmoid, or Softmax. Despite their strength, these conventional designs are often limited in introducing non-linearity by the choice of activation functions. In this work, we introduce Gaussian Mixture-Inspired Nonlinear Modules (GMNM), a new class of differentiable modules that draw on the universal density approximation Gaussian mixture models (GMMs) and distance properties (metric space) of Gaussian kernal. By relaxing probabilistic constraints and adopting a flexible parameterization of Gaussian projections, GMNM can be seamlessly integrated into diverse neural architectures and trained end-to-end with gradient-based methods. Our experiments demonstrate that incorporating GMNM into architectures such as MLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance over standard baselines. These results highlight GMNM's potential as a powerful and flexible module for enhancing efficiency and accuracy across a wide range of machine learning applications.", "link": "https://arxiv.org/abs/2510.06660", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.031112Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06672v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation", "summary": "arXiv:2510.06672v1 Announce Type: new Abstract: Reinforcement learning algorithms such as GRPO have driven recent advances in large language model (LLM) reasoning. While scaling the number of rollouts stabilizes training, existing approaches suffer from limited exploration on challenging prompts and leave informative feedback signals underexploited, due to context-independent rollout allocation across prompts (e.g., generating 16 rollouts per prompt) and relying heavily on sparse rewards. This paper presents XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy optimization through the principled lens of rollout exploration-exploitation. To enhance exploration, XRPO introduces a mathematically grounded rollout allocator that adaptively prioritizes prompts with higher potential for uncertainty reduction. It further addresses stagnation on zero-reward prompts through an in-context seeding strategy that injects curated exemplars, steering the model into more difficult reasoning trajectories. To strengthen exploitation, XRPO develops a group-relative, novelty-aware advantage sharpening mechanism that leverages sequence likelihoods to amplify low-probability yet correct responses, thereby extending the policy's reach beyond sparse rewards. Experiments across diverse math and coding benchmarks on both reasoning and non-reasoning models demonstrate that XRPO outperforms existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while accelerating training convergence by up to 2.7X.", "link": "https://arxiv.org/abs/2510.06672", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.031214Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06684v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "AutoBalance: An Automatic Balancing Framework for Training Physics-Informed Neural Networks", "summary": "arXiv:2510.06684v1 Announce Type: new Abstract: Physics-Informed Neural Networks (PINNs) provide a powerful and general framework for solving Partial Differential Equations (PDEs) by embedding physical laws into loss functions. However, training PINNs is notoriously difficult due to the need to balance multiple loss terms, such as PDE residuals and boundary conditions, which often have conflicting objectives and vastly different curvatures. Existing methods address this issue by manipulating gradients before optimization (a \"pre-combine\" strategy). We argue that this approach is fundamentally limited, as forcing a single optimizer to process gradients from spectrally heterogeneous loss landscapes disrupts its internal preconditioning. In this work, we introduce AutoBalance, a novel \"post-combine\" training paradigm. AutoBalance assigns an independent adaptive optimizer to each loss component and aggregates the resulting preconditioned updates afterwards. Extensive experiments on challenging PDE benchmarks show that AutoBalance consistently outperforms existing frameworks, achieving significant reductions in solution error, as measured by both the MSE and $L^{\\infty}$ norms. Moreover, AutoBalance is orthogonal to and complementary with other popular PINN methodologies, amplifying their effectiveness on demanding benchmarks.", "link": "https://arxiv.org/abs/2510.06684", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.031354Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06692v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?", "summary": "arXiv:2510.06692v1 Announce Type: new Abstract: Deep Neural Networks (DNNs) have attracted significant attention, and their internal models are now considered valuable intellectual assets. Extracting these internal models through access to a DNN is conceptually similar to extracting a secret key via oracle access to a block cipher. Consequently, cryptanalytic techniques, particularly differential-like attacks, have been actively explored recently. ReLU-based DNNs are the most commonly and widely deployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024) assume access to exact output logits, which are usually invisible, more recent works (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting, where only the final classification result (e.g., \"dog\" or \"car\") is available to the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that model extraction is feasible in polynomial time even under this restricted setting. In this paper, we first show that the assumptions underlying their attack become increasingly unrealistic as the attack-target depth grows. In practice, satisfying these assumptions requires an exponential number of queries with respect to the attack depth, implying that the attack does not always run in polynomial time. To address this critical limitation, we propose a novel attack method called CrossLayer Extraction. Instead of directly extracting the secret parameters (e.g., weights and biases) of a specific neuron, which incurs exponential cost, we exploit neuron interactions across layers to extract this information from deeper layers. This technique significantly reduces query complexity and mitigates the limitations of existing model extraction approaches.", "link": "https://arxiv.org/abs/2510.06692", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.031401Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06714v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Dual Goal Representations", "summary": "arXiv:2510.06714v1 Announce Type: new Abstract: In this work, we introduce dual goal representations for goal-conditioned reinforcement learning (GCRL). A dual goal representation characterizes a state by \"the set of temporal distances from all other states\"; in other words, it encodes a state through its relations to every other state, measured by temporal distance. This representation provides several appealing theoretical properties. First, it depends only on the intrinsic dynamics of the environment and is invariant to the original state representation. Second, it contains provably sufficient information to recover an optimal goal-reaching policy, while being able to filter out exogenous noise. Based on this concept, we develop a practical goal representation learning method that can be combined with any existing GCRL algorithm. Through diverse experiments on the OGBench task suite, we empirically show that dual goal representations consistently improve offline goal-reaching performance across 20 state- and pixel-based tasks.", "link": "https://arxiv.org/abs/2510.06714", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.031490Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/cs.LG|oai:arXiv.org:2510.06735v1", "feed": "https://arxiv.org/rss/cs.LG", "feed_name": "arXiv cs.LG", "title": "Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs", "summary": "arXiv:2510.06735v1 Announce Type: new Abstract: Bayesian causal discovery benefits from prior information elicited from domain experts, and in heterogeneous domains any prior knowledge would be badly needed. However, so far prior elicitation approaches have assumed a single causal graph and hence are not suited to heterogeneous domains. We propose a causal elicitation strategy for heterogeneous settings, based on Bayesian experimental design (BED) principles, and a variational mixture structure learning (VaMSL) method -- extending the earlier differentiable Bayesian structure learning (DiBS) method -- to iteratively infer mixtures of causal Bayesian networks (CBNs). We construct an informative graph prior incorporating elicited expert feedback in the inference of mixtures of CBNs. Our proposed method successfully produces a set of alternative causal models (mixture components or clusters), and achieves an improved structure learning performance on heterogeneous synthetic data when informed by a simulated expert. Finally, we demonstrate that our approach is capable of capturing complex distributions in a breast cancer database.", "link": "https://arxiv.org/abs/2510.06735", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "ml", "aprendizaje automático", "deep learning"], "ingested_at": "2025-10-09T05:07:29.031526Z", "source_priority": "high"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06515v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Online Matching via Reinforcement Learning: An Expert Policy Orchestration Strategy", "summary": "arXiv:2510.06515v1 Announce Type: new Abstract: Online matching problems arise in many complex systems, from cloud services and online marketplaces to organ exchange networks, where timely, principled decisions are critical for maintaining high system performance. Traditional heuristics in these settings are simple and interpretable but typically tailored to specific operating regimes, which can lead to inefficiencies when conditions change. We propose a reinforcement learning (RL) approach that learns to orchestrate a set of such expert policies, leveraging their complementary strengths in a data-driven, adaptive manner. Building on the Adv2 framework (Jonckheere et al., 2024), our method combines expert decisions through advantage-based weight updates and extends naturally to settings where only estimated value functions are available. We establish both expectation and high-probability regret guarantees and derive a novel finite-time bias bound for temporal-difference learning, enabling reliable advantage estimation even under constant step size and non-stationary dynamics. To support scalability, we introduce a neural actor-critic architecture that generalizes across large state spaces while preserving interpretability. Simulations on stochastic matching models, including an organ exchange scenario, show that the orchestrated policy converges faster and yields higher system level efficiency than both individual experts and conventional RL baselines. Our results highlight how structured, adaptive learning can improve the modeling and management of complex resource allocation and decision-making processes.", "link": "https://arxiv.org/abs/2510.06515", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058122Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06685v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Gaussian Equivalence for Self-Attention: Asymptotic Spectral Analysis of Attention Matrix", "summary": "arXiv:2510.06685v1 Announce Type: new Abstract: Self-attention layers have become fundamental building blocks of modern deep neural networks, yet their theoretical understanding remains limited, particularly from the perspective of random matrix theory. In this work, we provide a rigorous analysis of the singular value spectrum of the attention matrix and establish the first Gaussian equivalence result for attention. In a natural regime where the inverse temperature remains of constant order, we show that the singular value distribution of the attention matrix is asymptotically characterized by a tractable linear model. We further demonstrate that the distribution of squared singular values deviates from the Marchenko-Pastur law, which has been believed in previous work. Our proof relies on two key ingredients: precise control of fluctuations in the normalization term and a refined linearization that leverages favorable Taylor expansions of the exponential. This analysis also identifies a threshold for linearization and elucidates why attention, despite not being an entrywise operation, admits a rigorous Gaussian equivalence in this regime.", "link": "https://arxiv.org/abs/2510.06685", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058226Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06919v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Bayesian Nonparametric Dynamical Clustering of Time Series", "summary": "arXiv:2510.06919v1 Announce Type: new Abstract: We present a method that models the evolution of an unbounded number of time series clusters by switching among an unknown number of regimes with linear dynamics. We develop a Bayesian non-parametric approach using a hierarchical Dirichlet process as a prior on the parameters of a Switching Linear Dynamical System and a Gaussian process prior to model the statistical variations in amplitude and temporal alignment within each cluster. By modeling the evolution of time series patterns, the method avoids unnecessary proliferation of clusters in a principled manner. We perform inference by formulating a variational lower bound for off-line and on-line scenarios, enabling efficient learning through optimization. We illustrate the versatility and effectiveness of the approach through several case studies of electrocardiogram analysis using publicly available databases.", "link": "https://arxiv.org/abs/2510.06919", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058260Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06935v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "PyCFRL: A Python library for counterfactually fair offline reinforcement learning via sequential data preprocessing", "summary": "arXiv:2510.06935v1 Announce Type: new Abstract: Reinforcement learning (RL) aims to learn and evaluate a sequential decision rule, often referred to as a \"policy\", that maximizes the population-level benefit in an environment across possibly infinitely many time steps. However, the sequential decisions made by an RL algorithm, while optimized to maximize overall population benefits, may disadvantage certain individuals who are in minority or socioeconomically disadvantaged groups. To address this problem, we introduce PyCFRL, a Python library for ensuring counterfactual fairness in offline RL. PyCFRL implements a novel data preprocessing algorithm for learning counterfactually fair RL policies from offline datasets and provides tools to evaluate the values and counterfactual unfairness levels of RL policies. We describe the high-level functionalities of PyCFRL and demonstrate one of its major use cases through a data example. The library is publicly available on PyPI and Github (https://github.com/JianhanZhang/PyCFRL), and detailed tutorials can be found in the PyCFRL documentation (https://pycfrl-documentation.netlify.app).", "link": "https://arxiv.org/abs/2510.06935", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058300Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06995v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Root Cause Analysis of Outliers in Unknown Cyclic Graphs", "summary": "arXiv:2510.06995v1 Announce Type: new Abstract: We study the propagation of outliers in cyclic causal graphs with linear structural equations, tracing them back to one or several \"root cause\" nodes. We show that it is possible to identify a short list of potential root causes provided that the perturbation is sufficiently strong and propagates according to the same structural equations as in the normal mode. This shortlist consists of the true root causes together with those of its parents lying on a cycle with the root cause. Notably, our method does not require prior knowledge of the causal graph.", "link": "https://arxiv.org/abs/2510.06995", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058334Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.07088v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Explaining Models under Multivariate Bernoulli Distribution via Hoeffding Decomposition", "summary": "arXiv:2510.07088v1 Announce Type: new Abstract: Explaining the behavior of predictive models with random inputs can be achieved through sub-models decomposition, where such sub-models have easier interpretable features. Arising from the uncertainty quantification community, recent results have demonstrated the existence and uniqueness of a generalized Hoeffding decomposition for such predictive models when the stochastic input variables are correlated, based on concepts of oblique projection onto L 2 subspaces. This article focuses on the case where the input variables have Bernoulli distributions and provides a complete description of this decomposition. We show that in this case the underlying L 2 subspaces are one-dimensional and that the functional decomposition is explicit. This leads to a complete interpretability framework and theoretically allows reverse engineering. Explicit indicators of the influence of inputs on the output prediction (exemplified by Sobol' indices and Shapley effects) can be explicitly derived. Illustrated by numerical experiments, this type of analysis proves useful for addressing decision-support problems, based on binary decision diagrams, Boolean networks or binary neural networks. The article outlines perspectives for exploring high-dimensional settings and, beyond the case of binary inputs, extending these findings to models with finite countable inputs.", "link": "https://arxiv.org/abs/2510.07088", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058385Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.07099v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Diffusion-Augmented Reinforcement Learning for Robust Portfolio Optimization under Stress Scenarios", "summary": "arXiv:2510.07099v1 Announce Type: new Abstract: In the ever-changing and intricate landscape of financial markets, portfolio optimisation remains a formidable challenge for investors and asset managers. Conventional methods often struggle to capture the complex dynamics of market behaviour and align with diverse investor preferences. To address this, we propose an innovative framework, termed Diffusion-Augmented Reinforcement Learning (DARL), which synergistically integrates Denoising Diffusion Probabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) for portfolio management. By leveraging DDPMs to generate synthetic market crash scenarios conditioned on varying stress intensities, our approach significantly enhances the robustness of training data. Empirical evaluations demonstrate that DARL outperforms traditional baselines, delivering superior risk-adjusted returns and resilience against unforeseen crises, such as the 2025 Tariff Crisis. This work offers a robust and practical methodology to bolster stress resilience in DRL-driven financial applications.", "link": "https://arxiv.org/abs/2510.07099", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058424Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06264v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "A Mixed-Methods Analysis of Repression and Mobilization in Bangladesh's July Revolution Using Machine Learning and Statistical Modeling", "summary": "arXiv:2510.06264v1 Announce Type: cross Abstract: The 2024 July Revolution in Bangladesh represents a landmark event in the study of civil resistance. This study investigates the central paradox of the success of this student-led civilian uprising: how state violence, intended to quell dissent, ultimately fueled the movement's victory. We employ a mixed-methods approach. First, we develop a qualitative narrative of the conflict's timeline to generate specific, testable hypotheses. Then, using a disaggregated, event-level dataset, we employ a multi-method quantitative analysis to dissect the complex relationship between repression and mobilisation. We provide a framework to analyse explosive modern uprisings like the July Revolution. Initial pooled regression models highlight the crucial role of protest momentum in sustaining the movement. To isolate causal effects, we specify a Two-Way Fixed Effects panel model, which provides robust evidence for a direct and statistically significant local suppression backfire effect. Our Vector Autoregression (VAR) analysis provides clear visual evidence of an immediate, nationwide mobilisation in response to increased lethal violence. We further demonstrate that this effect was non-linear. A structural break analysis reveals that the backfire dynamic was statistically insignificant in the conflict's early phase but was triggered by the catalytic moral shock of the first wave of lethal violence, and its visuals circulated around July 16th. A complementary machine learning analysis (XGBoost, out-of-sample R$^{2}$=0.65) corroborates this from a predictive standpoint, identifying \"excessive force against protesters\" as the single most dominant predictor of nationwide escalation. We conclude that the July Revolution was driven by a contingent, non-linear backfire, triggered by specific catalytic moral shocks and accelerated by the viral reaction to the visual spectacle of state brutality.", "link": "https://arxiv.org/abs/2510.06264", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058540Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06286v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Mass Conservation on Rails -- Rethinking Physics-Informed Learning of Ice Flow Vector Fields", "summary": "arXiv:2510.06286v1 Announce Type: cross Abstract: To reliably project future sea level rise, ice sheet models require inputs that respect physics. Embedding physical principles like mass conservation into models that interpolate Antarctic ice flow vector fields from sparse & noisy measurements not only promotes physical adherence but can also improve accuracy and robustness. While physics-informed neural networks (PINNs) impose physics as soft penalties, offering flexibility but no physical guarantees, we instead propose divergence-free neural networks (dfNNs), which enforce local mass conservation exactly via a vector calculus trick. Our comparison of dfNNs, PINNs, and unconstrained NNs on ice flux interpolation over Byrd Glacier suggests that \"mass conservation on rails\" yields more reliable estimates, and that directional guidance, a learning strategy leveraging continent-wide satellite velocity data, boosts performance across models.", "link": "https://arxiv.org/abs/2510.06286", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058578Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06388v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Making and Evaluating Calibrated Forecasts", "summary": "arXiv:2510.06388v1 Announce Type: cross Abstract: Calibrated predictions can be reliably interpreted as probabilities. An important step towards achieving better calibration is to design an appropriate calibration measure to meaningfully assess the miscalibration level of a predictor. A recent line of work initiated by Haghtalab et al. [2024] studies the design of truthful calibration measures: a truthful measure is minimized when a predictor outputs the true probabilities, whereas a non-truthful measure incentivizes the predictor to lie so as to appear more calibrated. All previous calibration measures were non-truthful until Hartline et al. [2025] introduced the first perfectly truthful calibration measures for binary prediction tasks in the batch setting. We introduce a perfectly truthful calibration measure for multi-class prediction tasks, generalizing the work of Hartline et al. [2025] beyond binary prediction. We study common methods of extending calibration measures from binary to multi-class prediction and identify ones that do or do not preserve truthfulness. In addition to truthfulness, we mathematically prove and empirically verify that our calibration measure exhibits superior robustness: it robustly preserves the ordering between dominant and dominated predictors, regardless of the choice of hyperparameters (bin sizes). This result addresses the non-robustness issue of binned ECE, which has been observed repeatedly in prior work.", "link": "https://arxiv.org/abs/2510.06388", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058622Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06434v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization", "summary": "arXiv:2510.06434v1 Announce Type: cross Abstract: Learning from temporally-correlated data is a core facet of modern machine learning. Yet our understanding of sequential learning remains incomplete, particularly in the multi-trajectory setting where data consists of many independent realizations of a time-indexed stochastic process. This important regime both reflects modern training pipelines such as for large foundation models, and offers the potential for learning without the typical mixing assumptions made in the single-trajectory case. However, instance-optimal bounds are known only for least-squares regression with dependent covariates; for more general models or loss functions, the only broadly applicable guarantees result from a reduction to either i.i.d. learning, with effective sample size scaling only in the number of trajectories, or an existing single-trajectory result when each individual trajectory mixes, with effective sample size scaling as the full data budget deflated by the mixing-time. In this work, we significantly broaden the scope of instance-optimal rates in multi-trajectory settings via the Hellinger localization framework, a general approach for maximum likelihood estimation. Our method proceeds by first controlling the squared Hellinger distance at the path-measure level via a reduction to i.i.d. learning, followed by localization as a quadratic form in parameter space weighted by the trajectory Fisher information. This yields instance-optimal bounds that scale with the full data budget under a broad set of conditions. We instantiate our framework across four diverse case studies: a simple mixture of Markov chains, dependent linear regression under non-Gaussian noise, generalized linear models with non-monotonic activations, and linear-attention sequence models. In all cases, our bounds nearly match the instance-optimal rates from asymptotic normality, substantially improving over standard reductions.", "link": "https://arxiv.org/abs/2510.06434", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058757Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06439v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models", "summary": "arXiv:2510.06439v1 Announce Type: cross Abstract: Hyperparameter tuning is a challenging problem especially when the system itself involves uncertainty. Due to noisy function evaluations, optimization under uncertainty can be computationally expensive. In this paper, we present a novel Bayesian optimization framework tailored for hyperparameter tuning under uncertainty, with a focus on optimizing a scale- or precision-type parameter in stochastic models. The proposed method employs a statistical surrogate for the underlying random variable, enabling analytical evaluation of the expectation operator. Moreover, we derive a closed-form expression for the optimizer of the random acquisition function, which significantly reduces computational cost per iteration. Compared with a conventional one-dimensional Monte Carlo-based optimization scheme, the proposed approach requires 40 times fewer data points, resulting in up to a 40-fold reduction in computational cost. We demonstrate the effectiveness of the proposed method through two numerical examples in computational engineering.", "link": "https://arxiv.org/abs/2510.06439", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058795Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06505v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "A Median Perspective on Unlabeled Data for Out-of-Distribution Detection", "summary": "arXiv:2510.06505v1 Announce Type: cross Abstract: Out-of-distribution (OOD) detection plays a crucial role in ensuring the robustness and reliability of machine learning systems deployed in real-world applications. Recent approaches have explored the use of unlabeled data, showing potential for enhancing OOD detection capabilities. However, effectively utilizing unlabeled in-the-wild data remains challenging due to the mixed nature of both in-distribution (InD) and OOD samples. The lack of a distinct set of OOD samples complicates the task of training an optimal OOD classifier. In this work, we introduce Medix, a novel framework designed to identify potential outliers from unlabeled data using the median operation. We use the median because it provides a stable estimate of the central tendency, as an OOD detection mechanism, due to its robustness against noise and outliers. Using these identified outliers, along with labeled InD data, we train a robust OOD classifier. From a theoretical perspective, we derive error bounds that demonstrate Medix achieves a low error rate. Empirical results further substantiate our claims, as Medix outperforms existing methods across the board in open-world settings, confirming the validity of our theoretical insights.", "link": "https://arxiv.org/abs/2510.06505", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058837Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06527v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture", "summary": "arXiv:2510.06527v1 Announce Type: cross Abstract: We establish that randomly initialized neural networks, with large width and a natural choice of hyperparameters, have nearly independent outputs exactly when their activation function is nonlinear with zero mean under the Gaussian measure: $\\mathbb{E}_{z \\sim \\mathcal{N}(0,1)}[\\sigma(z)]=0$. For example, this includes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or GeLU by themselves. Because of their nearly independent outputs, we propose neural networks with zero-mean activation functions as a promising candidate for the Alignment Research Center's computational no-coincidence conjecture -- a conjecture that aims to measure the limits of AI interpretability.", "link": "https://arxiv.org/abs/2510.06527", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058871Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.06540v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Scalable Policy-Based RL Algorithms for POMDPs", "summary": "arXiv:2510.06540v1 Announce Type: cross Abstract: The continuous nature of belief states in POMDPs presents significant computational challenges in learning the optimal policy. In this paper, we consider an approach that solves a Partially Observable Reinforcement Learning (PORL) problem by approximating the corresponding POMDP model into a finite-state Markov Decision Process (MDP) (called Superstate MDP). We first derive theoretical guarantees that improve upon prior work that relate the optimal value function of the transformed Superstate MDP to the optimal value function of the original POMDP. Next, we propose a policy-based learning approach with linear function approximation to learn the optimal policy for the Superstate MDP. Consequently, our approach shows that a POMDP can be approximately solved using TD-learning followed by Policy Optimization by treating it as an MDP, where the MDP state corresponds to a finite history. We show that the approximation error decreases exponentially with the length of this history. To the best of our knowledge, our finite-time bounds are the first to explicitly quantify the error introduced when applying standard TD learning to a setting where the true dynamics are not Markovian.", "link": "https://arxiv.org/abs/2510.06540", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.058913Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.07093v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Non-Asymptotic Analysis of Efficiency in Conformalized Regression", "summary": "arXiv:2510.07093v1 Announce Type: cross Abstract: Conformal prediction provides prediction sets with coverage guarantees. The informativeness of conformal prediction depends on its efficiency, typically quantified by the expected size of the prediction set. Prior work on the efficiency of conformalized regression commonly treats the miscoverage level $\\alpha$ as a fixed constant. In this work, we establish non-asymptotic bounds on the deviation of the prediction set length from the oracle interval length for conformalized quantile and median regression trained via SGD, under mild assumptions on the data distribution. Our bounds of order $\\mathcal{O}(1/\\sqrt{n} + 1/(\\alpha^2 n) + 1/\\sqrt{m} + \\exp(-\\alpha^2 m))$ capture the joint dependence of efficiency on the proper training set size $n$, the calibration set size $m$, and the miscoverage level $\\alpha$. The results identify phase transitions in convergence rates across different regimes of $\\alpha$, offering guidance for allocating data to control excess prediction set length. Empirical results are consistent with our theoretical findings.", "link": "https://arxiv.org/abs/2510.07093", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059010Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.07128v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "jmstate, a Flexible Python Package for Multi-State Joint Modeling", "summary": "arXiv:2510.07128v1 Announce Type: cross Abstract: Classical joint modeling approaches often rely on competing risks or recurrent event formulations to account for complex real-world processes involving evolving longitudinal markers and discrete event occurrences. However, these frameworks typically capture only limited aspects of the underlying event dynamics. Multi-state joint models offer a more flexible alternative by representing full event histories through a network of possible transitions, including recurrent cycles and terminal absorptions, all potentially influenced by longitudinal covariates. In this paper, we propose a general framework that unifies longitudinal biomarker modeling with multi-state event processes defined on arbitrary directed graphs. Our approach accommodates both Markovian and semi-Markovian transition structures, and extends classical joint models by coupling nonlinear mixed-effects longitudinal submodels with multi-state survival processes via shared latent structures. We derive the full likelihood and develop scalable inference procedures based on stochastic gradient descent. Furthermore, we introduce a dynamic prediction framework, enabling individualized risk assessments along complex state-transition trajectories. To facilitate reproducibility and dissemination, we provide an open-source Python library \\texttt{jmstate} implementing the proposed methodology, available on \\href{https://pypi.org/project/jmstate/}{PyPI}. Simulation experiments and a biomedical case study demonstrate the flexibility and performance of the framework in representing complex longitudinal and multi-state event dynamics. The full Python notebooks used to reproduce the experiments as well as the source code of this paper are available on \\href{https://gitlab.com/felixlaplante0/jmstate-paper/}{GitLab}.", "link": "https://arxiv.org/abs/2510.07128", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059068Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.07132v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "DPMM-CFL: Clustered Federated Learning via Dirichlet Process Mixture Model Nonparametric Clustering", "summary": "arXiv:2510.07132v1 Announce Type: cross Abstract: Clustered Federated Learning (CFL) improves performance under non-IID client heterogeneity by clustering clients and training one model per cluster, thereby balancing between a global model and fully personalized models. However, most CFL methods require the number of clusters K to be fixed a priori, which is impractical when the latent structure is unknown. We propose DPMM-CFL, a CFL algorithm that places a Dirichlet Process (DP) prior over the distribution of cluster parameters. This enables nonparametric Bayesian inference to jointly infer both the number of clusters and client assignments, while optimizing per-cluster federated objectives. This results in a method where, at each round, federated updates and cluster inferences are coupled, as presented in this paper. The algorithm is validated on benchmark datasets under Dirichlet and class-split non-IID partitions.", "link": "https://arxiv.org/abs/2510.07132", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059102Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.07250v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Efficient reductions from a Gaussian source with applications to statistical-computational tradeoffs", "summary": "arXiv:2510.07250v1 Announce Type: cross Abstract: Given a single observation from a Gaussian distribution with unknown mean $\\theta$, we design computationally efficient procedures that can approximately generate an observation from a different target distribution $Q_{\\theta}$ uniformly for all $\\theta$ in a parameter set. We leverage our technique to establish reduction-based computational lower bounds for several canonical high-dimensional statistical models under widely-believed conjectures in average-case complexity. In particular, we cover cases in which: 1. $Q_{\\theta}$ is a general location model with non-Gaussian distribution, including both light-tailed examples (e.g., generalized normal distributions) and heavy-tailed ones (e.g., Student's $t$-distributions). As a consequence, we show that computational lower bounds proved for spiked tensor PCA with Gaussian noise are universal, in that they extend to other non-Gaussian noise distributions within our class. 2. $Q_{\\theta}$ is a normal distribution with mean $f(\\theta)$ for a general, smooth, and nonlinear link function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$. Using this reduction, we construct a reduction from symmetric mixtures of linear regressions to generalized linear models with link function $f$, and establish computational lower bounds for solving the $k$-sparse generalized linear model when $f$ is an even function. This result constitutes the first reduction-based confirmation of a $k$-to-$k^2$ statistical-to-computational gap in $k$-sparse phase retrieval, resolving a conjecture posed by Cai et al. (2016). As a second application, we construct a reduction from the sparse rank-1 submatrix model to the planted submatrix model, establishing a pointwise correspondence between the phase diagrams of the two models that faithfully preserves regions of computational hardness and tractability.", "link": "https://arxiv.org/abs/2510.07250", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059154Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.07314v1", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations", "summary": "arXiv:2510.07314v1 Announce Type: cross Abstract: Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblock to viable fusion power is understanding plasma turbulence, which significantly impairs plasma confinement, and is vital for next-generation reactor design. Plasma turbulence is governed by the nonlinear gyrokinetic equation, which evolves a 5D distribution function over time. Due to its high computational cost, reduced-order models are often employed in practice to approximate turbulent transport of energy. However, they omit nonlinear effects unique to the full 5D dynamics. To tackle this, we introduce GyroSwin, the first scalable 5D neural surrogate that can model 5D nonlinear gyrokinetic simulations, thereby capturing the physical phenomena neglected by reduced models, while providing accurate estimates of turbulent heat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D, (ii) introduces cross-attention and integration modules for latent 3D$\\leftrightarrow$5D interactions between electrostatic potential fields and the distribution function, and (iii) performs channelwise mode separation inspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely used reduced numerics on heat flux prediction, captures the turbulent energy cascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three orders of magnitude while remaining physically verifiable. GyroSwin shows promising scaling laws, tested up to one billion parameters, paving the way for scalable neural surrogates for gyrokinetic simulations of plasma turbulence.", "link": "https://arxiv.org/abs/2510.07314", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059204Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2208.03761v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "An Empirical Analysis of the Laplace and Neural Tangent Kernels", "summary": "arXiv:2208.03761v2 Announce Type: replace Abstract: The neural tangent kernel is a kernel function defined over the parameter distribution of an infinite width neural network. Despite the impracticality of this limit, the neural tangent kernel has allowed for a more direct study of neural networks and a gaze through the veil of their black box. More recently, it has been shown theoretically that the Laplace kernel and neural tangent kernel share the same reproducing kernel Hilbert space in the space of $\\mathbb{S}^{d-1}$ alluding to their equivalence. In this work, we analyze the practical equivalence of the two kernels. We first do so by matching the kernels exactly and then by matching posteriors of a Gaussian process. Moreover, we analyze the kernels in $\\mathbb{R}^d$ and experiment with them in the task of regression.", "link": "https://arxiv.org/abs/2208.03761", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059245Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2502.07939v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Bit-Level Discrete Diffusion with Markov Probabilistic Models: An Improved Framework with Sharp Convergence Bounds under Minimal Assumptions", "summary": "arXiv:2502.07939v2 Announce Type: replace Abstract: This paper introduces Discrete Markov Probabilistic Models (DMPMs), a novel discrete diffusion algorithm for discrete data generation. The algorithm operates in discrete bit space, where the noising process is a continuous-time Markov chain that flips labels uniformly at random. The time-reversal process, like the forward noise process, is a jump process with its intensity governed by a discrete analogue of the classical score function. Crucially, this intensity is proven to be the conditional expectation of a function of the forward process, underlining theoretical alignment with score-based generative models. We establish convergence bounds for the algorithm under minimal assumptions, ensuring robustness and efficiency, which we demonstrate through experiments on low-dimensional Bernoulli-distributed datasets and high-dimensional binary MNIST data. The results highlight competitive performance in generating discrete structures compared to the state-of-the-art. This work bridges theoretical foundations and practical applications, advancing the development of effective and theoretically grounded discrete generative modeling.", "link": "https://arxiv.org/abs/2502.07939", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059286Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2505.22554v4", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "A Copula Based Supervised Filter for Feature Selection in Diabetes Risk Prediction Using Machine Learning", "summary": "arXiv:2505.22554v4 Announce Type: replace Abstract: Effective feature selection is vital for robust and interpretable medical prediction, especially for identifying risk factors concentrated in extreme patient strata. Standard methods emphasize average associations and may miss predictors whose importance lies in the tails of the distribution. We propose a computationally efficient supervised filter that ranks features using the Gumbel copula upper tail dependence coefficient ($\\lambda_U$), prioritizing variables that are simultaneously extreme with the positive class. We benchmarked against Mutual Information, mRMR, ReliefF, and $L_1$ Elastic Net across four classifiers on two diabetes datasets: a large public health survey (CDC, N=253,680) and a clinical benchmark (PIMA, N=768). Evaluation included paired statistical tests, permutation importance, and robustness checks with label flips, feature noise, and missingness. On CDC, our method was the fastest selector and reduced the feature space by about 52% while retaining strong discrimination. Although using all 21 features yielded the highest AUC, our filter significantly outperformed Mutual Information and mRMR and was statistically indistinguishable from ReliefF. On PIMA, with only eight predictors, our ranking produced the numerically highest ROC AUC, and no significant differences were found versus strong baselines. Across both datasets, the upper tail criterion consistently identified clinically coherent, impactful predictors. We conclude that copula based feature selection via upper tail dependence is a powerful, efficient, and interpretable approach for building risk models in public health and clinical medicine.", "link": "https://arxiv.org/abs/2505.22554", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059375Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2507.05306v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Enjoying Non-linearity in Multinomial Logistic Bandits", "summary": "arXiv:2507.05306v2 Announce Type: replace Abstract: We consider the multinomial logistic bandit problem, a variant of where a learner interacts with an environment by selecting actions to maximize expected rewards based on probabilistic feedback from multiple possible outcomes. In the binary setting, recent work has focused on understanding the impact of the non-linearity of the logistic model (Faury et al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant $\\kappa_* \\geq 1$, that may be exponentially large in some problem parameters and which is captured by the derivative of the sigmoid function. It encapsulates the non-linearity and improves existing regret guarantees over $T$ rounds from $\\smash{O(d\\sqrt{T})}$ to $\\smash{O(d\\sqrt{T/\\kappa_*})}$, where $d$ is the dimension of the parameter space. We extend their analysis to the multinomial logistic bandit framework, making it suitable for complex applications with more than two choices, such as reinforcement learning or recommender systems. To achieve this, we extend the definition of \\( \\kappa_* \\) to the multinomial setting and propose an efficient algorithm that leverages the problem's non-linearity. Our method yields a problem-dependent regret bound of order $ \\smash{\\widetilde{\\mathcal{O}}( R d \\sqrt{{KT}/{\\kappa_*}})} $, where $R$ is the norm of the vector of rewards and $K$ is the number of outcomes. This improves upon the best existing guarantees of order $ \\smash{\\widetilde{\\mathcal{O}}( RdK \\sqrt{T} )} $. Moreover, we provide a $\\smash{ \\Omega(Rd\\sqrt{KT/\\kappa_*})}$ lower-bound, showing that our algorithm is minimax-optimal and that our definition of $\\kappa_*$ is optimal.", "link": "https://arxiv.org/abs/2507.05306", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059459Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2509.23385v3", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Flow Matching for Robust Simulation-Based Inference under Model Misspecification", "summary": "arXiv:2509.23385v3 Announce Type: replace Abstract: Simulation-based inference (SBI) is transforming experimental sciences by enabling parameter estimation in complex non-linear models from simulated data. A persistent challenge, however, is model misspecification: simulators are only approximations of reality, and mismatches between simulated and real data can yield biased or overconfident posteriors. We address this issue by introducing Flow Matching Corrected Posterior Estimation (FMCPE), a framework that leverages the flow matching paradigm to refine simulation-trained posterior estimators using a small set of real calibration samples. Our approach proceeds in two stages: first, a posterior approximator is trained on abundant simulated data; second, flow matching transports its predictions toward the true posterior supported by real observations, without requiring explicit knowledge of the misspecification. This design enables FMCPE to combine the scalability of SBI with robustness to distributional shift. Across synthetic benchmarks and real-world datasets, we show that our proposal consistently mitigates the effects of misspecification, delivering improved inference accuracy and uncertainty calibration compared to standard SBI baselines, while remaining computationally efficient.", "link": "https://arxiv.org/abs/2509.23385", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059525Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04042v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Simulation-based inference via telescoping ratio estimation for trawl processes", "summary": "arXiv:2510.04042v2 Announce Type: replace Abstract: The growing availability of large and complex datasets has increased interest in temporal stochastic processes that can capture stylized facts such as marginal skewness, non-Gaussian tails, long memory, and even non-Markovian dynamics. While such models are often easy to simulate from, parameter estimation remains challenging. Simulation-based inference (SBI) offers a promising way forward, but existing methods typically require large training datasets or complex architectures and frequently yield confidence (credible) regions that fail to attain their nominal values, raising doubts on the reliability of estimates for the very features that motivate the use of these models. To address these challenges, we propose a fast and accurate, sample-efficient SBI framework for amortized posterior inference applicable to intractable stochastic processes. The proposed approach relies on two main steps: first, we learn the posterior density by decomposing it sequentially across parameter dimensions. Then, we use Chebyshev polynomial approximations to efficiently generate independent posterior samples, enabling accurate inference even when Markov chain Monte Carlo methods mix poorly. We further develop novel diagnostic tools for SBI in this context, as well as post-hoc calibration techniques; the latter not only lead to performance improvements of the learned inferential tool, but also to the ability to reuse it directly with new time series of varying lengths, thus amortizing the training cost. We demonstrate the method's effectiveness on trawl processes, a class of flexible infinitely divisible models that generalize univariate Gaussian processes, applied to energy demand data.", "link": "https://arxiv.org/abs/2510.04042", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059582Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2412.04781v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "DPGIIL: Dirichlet Process-Deep Generative Model-Integrated Incremental Learning for Clustering in Transmissibility-based Online Structural Anomaly Detection", "summary": "arXiv:2412.04781v2 Announce Type: replace-cross Abstract: Clustering based on vibration responses, such as transmissibility functions (TFs), is promising in structural anomaly detection. However, most existing methods struggle to determine the optimal cluster number, handle high-dimensional streaming data, and rely heavily on manually engineered features due to their shallow structures. To address these issues, this work proposes a novel clustering framework, referred to as Dirichlet process-deep generative model-integrated incremental learning (DPGIIL), for online structural anomaly detection, which combines the advantages of deep generative models (DGMs) in representation learning and the Dirichlet process mixture model (DPMM) in identifying distinct patterns in observed data. Within the context of variational Bayesian inference, a lower bound on the log marginal likelihood of DPGIIL, tighter than the evidence lower bound, is derived analytically, which enables the joint optimization of DGM and DPMM parameters, thereby allowing the DPMM to regularize the DGM's feature extraction process. Additionally, a greedy split-merge scheme-based coordinate ascent variational inference method is devised to accelerate the optimization. The summary statistics of the DPMM, along with the network parameters, are used to retain information about previous data for incremental learning. For online structural anomaly detection, DPGIIL can not only detect anomalies by dynamically assigning incoming data to new clusters but also indicate different structural states using distinct clusters, thereby providing additional information about the operating conditions of the monitored structure compared to traditional anomaly detectors. Three case studies demonstrate the dynamic adaptability of the proposed method and show that it outperforms some state-of-the-art approaches in both structural anomaly detection and clustering.", "link": "https://arxiv.org/abs/2412.04781", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059652Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2502.00336v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Denoising Score Matching with Random Features: Insights on Diffusion Models from Precise Learning Curves", "summary": "arXiv:2502.00336v2 Announce Type: replace-cross Abstract: We theoretically investigate the phenomena of generalization and memorization in diffusion models. Empirical studies suggest that these phenomena are influenced by model complexity and the size of the training dataset. In our experiments, we further observe that the number of noise samples per data sample ($m$) used during Denoising Score Matching (DSM) plays a significant and non-trivial role. We capture these behaviors and shed insights into their mechanisms by deriving asymptotically precise expressions for test and train errors of DSM under a simple theoretical setting. The score function is parameterized by random features neural networks, with the target distribution being $d$-dimensional Gaussian. We operate in a regime where the dimension $d$, number of data samples $n$, and number of features $p$ tend to infinity while keeping the ratios $\\psi_n=\\frac{n}{d}$ and $\\psi_p=\\frac{p}{d}$ fixed. By characterizing the test and train errors, we identify regimes of generalization and memorization as a function of $\\psi_n,\\psi_p$, and $m$. Our theoretical findings are consistent with the empirical observations.", "link": "https://arxiv.org/abs/2502.00336", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059703Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2502.01588v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "A Differentiable Alignment Framework for Sequence-to-Sequence Modeling via Optimal Transport", "summary": "arXiv:2502.01588v2 Announce Type: replace-cross Abstract: Accurate sequence-to-sequence (seq2seq) alignment is critical for applications like medical speech analysis and language learning tools relying on automatic speech recognition (ASR). State-of-the-art end-to-end (E2E) ASR systems, such as the Connectionist Temporal Classification (CTC) and transducer-based models, suffer from peaky behavior and alignment inaccuracies. In this paper, we propose a novel differentiable alignment framework based on one-dimensional optimal transport, enabling the model to learn a single alignment and perform ASR in an E2E manner. We introduce a pseudo-metric, called Sequence Optimal Transport Distance (SOTD), over the sequence space and discuss its theoretical properties. Based on the SOTD, we propose Optimal Temporal Transport Classification (OTTC) loss for ASR and contrast its behavior with CTC. Experimental results on the TIMIT, AMI, and LibriSpeech datasets show that our method considerably improves alignment performance compared to CTC and the more recently proposed Consistency-Regularized CTC, though with a trade-off in ASR performance. We believe this work opens new avenues for seq2seq alignment research, providing a solid foundation for further exploration and development within the community.", "link": "https://arxiv.org/abs/2502.01588", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059749Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2505.21626v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Learning where to learn: Training data distribution optimization for scientific machine learning", "summary": "arXiv:2505.21626v2 Announce Type: replace-cross Abstract: In scientific machine learning, models are routinely deployed with parameter values or boundary conditions far from those used in training. This paper studies the learning-where-to-learn problem of designing a training data distribution that minimizes average prediction error across a family of deployment regimes. A theoretical analysis shows how the training distribution shapes deployment accuracy. This motivates two adaptive algorithms based on bilevel or alternating optimization in the space of probability measures. Discretized implementations using parametric distribution classes or nonparametric particle-based gradient flows deliver optimized training distributions that outperform nonadaptive designs. Once trained, the resulting models exhibit improved sample complexity and robustness to distribution shift. This framework unlocks the potential of principled data acquisition for learning functions and solution operators of partial differential equations.", "link": "https://arxiv.org/abs/2505.21626", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059787Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2506.09348v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Adversarial Surrogate Risk Bounds for Binary Classification", "summary": "arXiv:2506.09348v2 Announce Type: replace-cross Abstract: A central concern in classification is the vulnerability of machine learning models to adversarial attacks. Adversarial training is one of the most popular techniques for training robust classifiers, which involves minimizing an adversarial surrogate risk. Recent work has characterized the conditions under which any sequence minimizing the adversarial surrogate risk also minimizes the adversarial classification risk in the binary setting, a property known as adversarial consistency. However, these results do not address the rate at which the adversarial classification risk approaches its optimal value along such a sequence. This paper provides surrogate risk bounds that quantify that convergence rate.", "link": "https://arxiv.org/abs/2506.09348", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059821Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2507.15112v3", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Distributional Machine Unlearning via Selective Data Removal", "summary": "arXiv:2507.15112v3 Announce Type: replace-cross Abstract: Machine learning systems increasingly face requirements to remove entire domains of information -- such as toxic language or biases -- rather than individual user data. This task presents a dilemma: full removal of the unwanted domain data is computationally expensive, while random partial removal is statistically inefficient. We find that a domain's statistical influence is often concentrated in a small subset of its data samples, suggesting a path between ineffective partial removal and unnecessary complete removal. We formalize this as distributional unlearning: a framework to select a small subset that balances forgetting an unwanted distribution while preserving a desired one. Using Kullback-Leibler divergence constraints, we derive the exact removal-preservation Pareto frontier for exponential families and prove that models trained on the edited data achieve corresponding log-loss bounds. We propose a distance-based selection algorithm and show it is quadratically more sample-efficient than random removal in the challenging low-divergence regime. Experiments across synthetic, text, and image datasets (Jigsaw, CIFAR-10, SMS spam) show our method requires 15-82% less deletion than full removal for strong unlearning effects, e.g., halving initial forget set accuracy. Ultimately, by showing a small forget set often suffices, our framework lays the foundations for more scalable and rigorous subpopulation unlearning.", "link": "https://arxiv.org/abs/2507.15112", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059868Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2508.06635v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Valid Inference with Imperfect Synthetic Data", "summary": "arXiv:2508.06635v2 Announce Type: replace-cross Abstract: Predictions and generations from large language models are increasingly being explored as an aid in limited data regimes, such as in computational social science and human subjects research. While prior technical work has mainly explored the potential to use model-predicted labels for unlabeled data in a principled manner, there is increasing interest in using large language models to generate entirely new synthetic samples (e.g., synthetic simulations), such as in responses to surveys. However, it remains unclear by what means practitioners can combine such data with real data and yet produce statistically valid conclusions upon them. In this paper, we introduce a new estimator based on generalized method of moments, providing a hyperparameter-free solution with strong theoretical guarantees to address this challenge. Intriguingly, we find that interactions between the moment residuals of synthetic data and those of real data (i.e., when they are predictive of each other) can greatly improve estimates of the target parameter. We validate the finite-sample performance of our estimator across different tasks in computational social science applications, demonstrating large empirical gains.", "link": "https://arxiv.org/abs/2508.06635", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059912Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2509.07030v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "A Minimalist Bayesian Framework for Stochastic Optimization", "summary": "arXiv:2509.07030v2 Announce Type: replace-cross Abstract: The Bayesian paradigm offers principled tools for sequential decision-making under uncertainty, but its reliance on a probabilistic model for all parameters can hinder the incorporation of complex structural constraints. We introduce a minimalist Bayesian framework that places a prior only on the component of interest, such as the location of the optimum. Nuisance parameters are eliminated via profile likelihood, which naturally handles constraints. As a direct instantiation, we develop a MINimalist Thompson Sampling (MINTS) algorithm. Our framework accommodates structured problems, including continuum-armed Lipschitz bandits and dynamic pricing. It also provides a probabilistic lens on classical convex optimization algorithms such as the center of gravity and ellipsoid methods. We further analyze MINTS for multi-armed bandits and establish near-optimal regret guarantees.", "link": "https://arxiv.org/abs/2509.07030", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059947Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2509.21181v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Closed-form $\\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\\ell_p$ bias", "summary": "arXiv:2509.21181v2 Announce Type: replace-cross Abstract: For overparameterized linear regression with isotropic Gaussian design and minimum-$\\ell_p$ interpolator $p\\in(1,2]$, we give a unified, high-probability characterization for the scaling of the family of parameter norms $ \\\\{ \\lVert \\widehat{w_p} \\rVert_r \\\\}_{r \\in [1,p]} $ with sample size. We solve this basic, but unresolved question through a simple dual-ray analysis, which reveals a competition between a signal *spike* and a *bulk* of null coordinates in $X^\\top Y$, yielding closed-form predictions for (i) a data-dependent transition $n_\\star$ (the \"elbow\"), and (ii) a universal threshold $r_\\star=2(p-1)$ that separates $\\lVert \\widehat{w_p} \\rVert_r$'s which plateau from those that continue to grow with an explicit exponent. This unified solution resolves the scaling of *all* $\\ell_r$ norms within the family $r\\in [1,p]$ under $\\ell_p$-biased interpolation, and explains in one picture which norms saturate and which increase as $n$ grows. We then study diagonal linear networks (DLNs) trained by gradient descent. By calibrating the initialization scale $\\alpha$ to an effective $p_{\\mathrm{eff}}(\\alpha)$ via the DLN separable potential, we show empirically that DLNs inherit the same elbow/threshold laws, providing a predictive bridge between explicit and implicit bias. Given that many generalization proxies depend on $\\lVert \\widehat {w_p} \\rVert_r$, our results suggest that their predictive power will depend sensitively on which $l_r$ norm is used.", "link": "https://arxiv.org/abs/2509.21181", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.059996Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2509.24076v4", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "A Family of Kernelized Matrix Costs for Multiple-Output Mixture Neural Networks", "summary": "arXiv:2509.24076v4 Announce Type: replace-cross Abstract: Pairwise distance-based costs are crucial for self-supervised and contrastive feature learning. Mixture Density Networks (MDNs) are a widely used approach for generative models and density approximation, using neural networks to produce multiple centers that define a Gaussian mixture. By combining MDNs with contrastive costs, this paper proposes data density approximation using four types of kernelized matrix costs in the Hilbert space: the scalar cost, the vector-matrix cost, the matrix-matrix cost (the trace of Schur complement), and the SVD cost (the nuclear norm), for learning multiple centers required to define a mixture density.", "link": "https://arxiv.org/abs/2509.24076", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.060028Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03470v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "On residual network depth", "summary": "arXiv:2510.03470v2 Announce Type: replace-cross Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled models of unprecedented depth, yet a formal understanding of why depth is so effective remains an open question. A popular intuition, following Veit et al. (2016), is that these residual networks behave like ensembles of many shallower models. Our key finding is an explicit analytical formula that verifies this ensemble perspective, proving that increasing network depth is mathematically equivalent to expanding the size of this implicit ensemble. Furthermore, our expansion reveals a hierarchical ensemble structure in which the combinatorial growth of computation paths leads to an explosion in the output signal, explaining the historical necessity of normalization layers in training deep models. This insight offers a first principles explanation for the historical dependence on normalization layers and sheds new light on a family of successful normalization-free techniques like SkipInit and Fixup. However, while these previous approaches infer scaling factors through optimizer analysis or a heuristic analogy to Batch Normalization, our work offers the first explanation derived directly from the network's inherent functional structure. Specifically, our Residual Expansion Theorem reveals that scaling each residual module provides a principled solution to taming the combinatorial explosion inherent to these architectures. We further show that this scaling acts as a capacity controls that also implicitly regularizes the model's complexity.", "link": "https://arxiv.org/abs/2510.03470", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.060082Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.03569v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Longitudinal Flow Matching for Trajectory Modeling", "summary": "arXiv:2510.03569v2 Announce Type: replace-cross Abstract: Generative models for sequential data often struggle with sparsely sampled and high-dimensional trajectories, typically reducing the learning of dynamics to pairwise transitions. We propose Interpolative Multi-Marginal Flow Matching (IMMFM), a framework that learns continuous stochastic dynamics jointly consistent with multiple observed time points. IMMFM employs a piecewise-quadratic interpolation path as a smooth target for flow matching and jointly optimizes drift and a data-driven diffusion coefficient, supported by a theoretical condition for stable learning. This design captures intrinsic stochasticity, handles irregular sparse sampling, and yields subject-specific trajectories. Experiments on synthetic benchmarks and real-world longitudinal neuroimaging datasets show that IMMFM outperforms existing methods in both forecasting accuracy and further downstream tasks.", "link": "https://arxiv.org/abs/2510.03569", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.060116Z", "source_priority": "medium"}
{"_id": "https://arxiv.org/rss/stat.ML|oai:arXiv.org:2510.04072v2", "feed": "https://arxiv.org/rss/stat.ML", "feed_name": "arXiv stat.ML", "title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning", "summary": "arXiv:2510.04072v2 Announce Type: replace-cross Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in large language models (LLMs). Yet on-policy algorithms such as Group Relative Policy Optimization (GRPO) often suffer in early training: noisy gradients from low-quality rollouts lead to unstable updates and inefficient exploration. We introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient framework to address these limitations via decomposing each step into three stages: a short fast trajectory of inner steps on the same batch, a reposition mechanism to control off-policy drift, and a final slow correction. This reposition-before-update design preserves the objective and rollout process unchanged, making SFPO plug-compatible with existing policy-gradient pipelines. Extensive experiments demonstrate that SFPO consistently improves stability, reduces rollouts, and accelerates convergence of reasoning RL training. Specifically, it outperforms GRPO by up to 2.80 points in average on math reasoning benchmarks. It also achieves up to 4.93\\texttimes{} fewer rollouts and an up to 4.19\\texttimes{} reduction in wall-clock time to match GRPO's best accuracy.", "link": "https://arxiv.org/abs/2510.04072", "published": "2025-10-09T04:00:00", "language": "en", "topics": ["ia", "inteligencia artificial", "machine learning", "ml", "física cuántica", "neurociencia", "estadística", "machine learning"], "ingested_at": "2025-10-09T05:07:30.060161Z", "source_priority": "medium"}
